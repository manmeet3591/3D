{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "backwarp_tenGrid = {}\n",
        "\n",
        "\n",
        "def warp(tenInput, tenFlow):\n",
        "    k = (str(tenFlow.device), str(tenFlow.size()))\n",
        "    if k not in backwarp_tenGrid:\n",
        "        tenHorizontal = torch.linspace(-1.0, 1.0, tenFlow.shape[3], device=device).view(\n",
        "            1, 1, 1, tenFlow.shape[3]).expand(tenFlow.shape[0], -1, tenFlow.shape[2], -1)\n",
        "        tenVertical = torch.linspace(-1.0, 1.0, tenFlow.shape[2], device=device).view(\n",
        "            1, 1, tenFlow.shape[2], 1).expand(tenFlow.shape[0], -1, -1, tenFlow.shape[3])\n",
        "        backwarp_tenGrid[k] = torch.cat(\n",
        "            [tenHorizontal, tenVertical], 1).to(device)\n",
        "\n",
        "    tenFlow = torch.cat([tenFlow[:, 0:1, :, :] / ((tenInput.shape[3] - 1.0) / 2.0),\n",
        "                         tenFlow[:, 1:2, :, :] / ((tenInput.shape[2] - 1.0) / 2.0)], 1)\n",
        "\n",
        "    g = (backwarp_tenGrid[k] + tenFlow).permute(0, 2, 3, 1)\n",
        "    return torch.nn.functional.grid_sample(input=tenInput, grid=g, mode='bilinear', padding_mode='border', align_corners=True)"
      ],
      "metadata": {
        "id": "NWQjiNLReHtt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "# from model.warplayer import warp\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
        "                  padding=padding, dilation=dilation, bias=True),\n",
        "        nn.PReLU(out_planes)\n",
        "        )\n",
        "\n",
        "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
        "    return nn.Sequential(\n",
        "        torch.nn.ConvTranspose2d(in_channels=in_planes, out_channels=out_planes, kernel_size=4, stride=2, padding=1, bias=True),\n",
        "        nn.PReLU(out_planes)\n",
        "        )\n",
        "\n",
        "class Conv2(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride=2):\n",
        "        super(Conv2, self).__init__()\n",
        "        self.conv1 = conv(in_planes, out_planes, 3, stride, 1)\n",
        "        self.conv2 = conv(out_planes, out_planes, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "c = 16\n",
        "class Contextnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Contextnet, self).__init__()\n",
        "        self.conv1 = Conv2(3, c)\n",
        "        self.conv2 = Conv2(c, 2*c)\n",
        "        self.conv3 = Conv2(2*c, 4*c)\n",
        "        self.conv4 = Conv2(4*c, 8*c)\n",
        "\n",
        "    def forward(self, x, flow):\n",
        "        x = self.conv1(x)\n",
        "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False) * 0.5\n",
        "        f1 = warp(x, flow)\n",
        "        x = self.conv2(x)\n",
        "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False) * 0.5\n",
        "        f2 = warp(x, flow)\n",
        "        x = self.conv3(x)\n",
        "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False) * 0.5\n",
        "        f3 = warp(x, flow)\n",
        "        x = self.conv4(x)\n",
        "        flow = F.interpolate(flow, scale_factor=0.5, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False) * 0.5\n",
        "        f4 = warp(x, flow)\n",
        "        return [f1, f2, f3, f4]\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Unet, self).__init__()\n",
        "        self.down0 = Conv2(17, 2*c)\n",
        "        self.down1 = Conv2(4*c, 4*c)\n",
        "        self.down2 = Conv2(8*c, 8*c)\n",
        "        self.down3 = Conv2(16*c, 16*c)\n",
        "        self.up0 = deconv(32*c, 8*c)\n",
        "        self.up1 = deconv(16*c, 4*c)\n",
        "        self.up2 = deconv(8*c, 2*c)\n",
        "        self.up3 = deconv(4*c, c)\n",
        "        self.conv = nn.Conv2d(c, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, img0, img1, warped_img0, warped_img1, mask, flow, c0, c1):\n",
        "        s0 = self.down0(torch.cat((img0, img1, warped_img0, warped_img1, mask, flow), 1))\n",
        "        s1 = self.down1(torch.cat((s0, c0[0], c1[0]), 1))\n",
        "        s2 = self.down2(torch.cat((s1, c0[1], c1[1]), 1))\n",
        "        s3 = self.down3(torch.cat((s2, c0[2], c1[2]), 1))\n",
        "        x = self.up0(torch.cat((s3, c0[3], c1[3]), 1))\n",
        "        x = self.up1(torch.cat((x, s2), 1))\n",
        "        x = self.up2(torch.cat((x, s1), 1))\n",
        "        x = self.up3(torch.cat((x, s0), 1))\n",
        "        x = self.conv(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "zt-kHDjxeSv1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from model.warplayer import warp\n",
        "# from model.refine import *\n",
        "\n",
        "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
        "    return nn.Sequential(\n",
        "        torch.nn.ConvTranspose2d(in_channels=in_planes, out_channels=out_planes, kernel_size=4, stride=2, padding=1),\n",
        "        nn.PReLU(out_planes)\n",
        "    )\n",
        "\n",
        "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
        "                  padding=padding, dilation=dilation, bias=True),\n",
        "        nn.PReLU(out_planes)\n",
        "    )\n",
        "\n",
        "class IFBlock(nn.Module):\n",
        "    def __init__(self, in_planes, c=64):\n",
        "        super(IFBlock, self).__init__()\n",
        "        self.conv0 = nn.Sequential(\n",
        "            conv(in_planes, c//2, 3, 2, 1),\n",
        "            conv(c//2, c, 3, 2, 1),\n",
        "            )\n",
        "        self.convblock = nn.Sequential(\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "        )\n",
        "        self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x, flow, scale):\n",
        "        if scale != 1:\n",
        "            x = F.interpolate(x, scale_factor = 1. / scale, mode=\"bilinear\", align_corners=False)\n",
        "        if flow != None:\n",
        "            flow = F.interpolate(flow, scale_factor = 1. / scale, mode=\"bilinear\", align_corners=False) * 1. / scale\n",
        "            x = torch.cat((x, flow), 1)\n",
        "        x = self.conv0(x)\n",
        "        x = self.convblock(x) + x\n",
        "        tmp = self.lastconv(x)\n",
        "        tmp = F.interpolate(tmp, scale_factor = scale * 2, mode=\"bilinear\", align_corners=False)\n",
        "        flow = tmp[:, :4] * scale * 2\n",
        "        mask = tmp[:, 4:5]\n",
        "        return flow, mask\n",
        "\n",
        "class IFNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IFNet, self).__init__()\n",
        "        self.block0 = IFBlock(6, c=240)\n",
        "        self.block1 = IFBlock(13+4, c=150)\n",
        "        self.block2 = IFBlock(13+4, c=90)\n",
        "        self.block_tea = IFBlock(16+4, c=90)\n",
        "        self.contextnet = Contextnet()\n",
        "        self.unet = Unet()\n",
        "\n",
        "    def forward(self, x, scale=[4,2,1], timestep=0.5):\n",
        "        img0 = x[:, :3]\n",
        "        img1 = x[:, 3:6]\n",
        "        gt = x[:, 6:] # In inference time, gt is None\n",
        "        flow_list = []\n",
        "        merged = []\n",
        "        mask_list = []\n",
        "        warped_img0 = img0\n",
        "        warped_img1 = img1\n",
        "        flow = None\n",
        "        loss_distill = 0\n",
        "        stu = [self.block0, self.block1, self.block2]\n",
        "        for i in range(3):\n",
        "            if flow != None:\n",
        "                flow_d, mask_d = stu[i](torch.cat((img0, img1, warped_img0, warped_img1, mask), 1), flow, scale=scale[i])\n",
        "                flow = flow + flow_d\n",
        "                mask = mask + mask_d\n",
        "            else:\n",
        "                flow, mask = stu[i](torch.cat((img0, img1), 1), None, scale=scale[i])\n",
        "            mask_list.append(torch.sigmoid(mask))\n",
        "            flow_list.append(flow)\n",
        "            warped_img0 = warp(img0, flow[:, :2])\n",
        "            warped_img1 = warp(img1, flow[:, 2:4])\n",
        "            merged_student = (warped_img0, warped_img1)\n",
        "            merged.append(merged_student)\n",
        "        if gt.shape[1] == 3:\n",
        "            flow_d, mask_d = self.block_tea(torch.cat((img0, img1, warped_img0, warped_img1, mask, gt), 1), flow, scale=1)\n",
        "            flow_teacher = flow + flow_d\n",
        "            warped_img0_teacher = warp(img0, flow_teacher[:, :2])\n",
        "            warped_img1_teacher = warp(img1, flow_teacher[:, 2:4])\n",
        "            mask_teacher = torch.sigmoid(mask + mask_d)\n",
        "            merged_teacher = warped_img0_teacher * mask_teacher + warped_img1_teacher * (1 - mask_teacher)\n",
        "        else:\n",
        "            flow_teacher = None\n",
        "            merged_teacher = None\n",
        "        for i in range(3):\n",
        "            merged[i] = merged[i][0] * mask_list[i] + merged[i][1] * (1 - mask_list[i])\n",
        "            if gt.shape[1] == 3:\n",
        "                loss_mask = ((merged[i] - gt).abs().mean(1, True) > (merged_teacher - gt).abs().mean(1, True) + 0.01).float().detach()\n",
        "                loss_distill += (((flow_teacher.detach() - flow_list[i]) ** 2).mean(1, True) ** 0.5 * loss_mask).mean()\n",
        "        c0 = self.contextnet(img0, flow[:, :2])\n",
        "        c1 = self.contextnet(img1, flow[:, 2:4])\n",
        "        tmp = self.unet(img0, img1, warped_img0, warped_img1, mask, flow, c0, c1)\n",
        "        res = tmp[:, :3] * 2 - 1\n",
        "        merged[2] = torch.clamp(merged[2] + res, 0, 1)\n",
        "        return flow_list, mask_list[2], merged, flow_teacher, merged_teacher, loss_distill"
      ],
      "metadata": {
        "id": "oGnAwl1geNt9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from model.warplayer import warp\n",
        "# from model.refine import *\n",
        "\n",
        "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
        "    return nn.Sequential(\n",
        "        torch.nn.ConvTranspose2d(in_channels=in_planes, out_channels=out_planes, kernel_size=4, stride=2, padding=1),\n",
        "        nn.PReLU(out_planes)\n",
        "    )\n",
        "\n",
        "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
        "                  padding=padding, dilation=dilation, bias=True),\n",
        "        nn.PReLU(out_planes)\n",
        "    )\n",
        "\n",
        "class IFBlock(nn.Module):\n",
        "    def __init__(self, in_planes, c=64):\n",
        "        super(IFBlock, self).__init__()\n",
        "        self.conv0 = nn.Sequential(\n",
        "            conv(in_planes, c//2, 3, 2, 1),\n",
        "            conv(c//2, c, 3, 2, 1),\n",
        "            )\n",
        "        self.convblock = nn.Sequential(\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "            conv(c, c),\n",
        "        )\n",
        "        self.lastconv = nn.ConvTranspose2d(c, 5, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x, flow, scale):\n",
        "        if scale != 1:\n",
        "            x = F.interpolate(x, scale_factor = 1. / scale, mode=\"bilinear\", align_corners=False)\n",
        "        if flow != None:\n",
        "            flow = F.interpolate(flow, scale_factor = 1. / scale, mode=\"bilinear\", align_corners=False) * 1. / scale\n",
        "            x = torch.cat((x, flow), 1)\n",
        "        x = self.conv0(x)\n",
        "        x = self.convblock(x) + x\n",
        "        tmp = self.lastconv(x)\n",
        "        tmp = F.interpolate(tmp, scale_factor = scale * 2, mode=\"bilinear\", align_corners=False)\n",
        "        flow = tmp[:, :4] * scale * 2\n",
        "        mask = tmp[:, 4:5]\n",
        "        return flow, mask\n",
        "\n",
        "class IFNet_m(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IFNet_m, self).__init__()\n",
        "        self.block0 = IFBlock(6+1, c=240)\n",
        "        self.block1 = IFBlock(13+4+1, c=150)\n",
        "        self.block2 = IFBlock(13+4+1, c=90)\n",
        "        self.block_tea = IFBlock(16+4+1, c=90)\n",
        "        self.contextnet = Contextnet()\n",
        "        self.unet = Unet()\n",
        "\n",
        "    def forward(self, x, scale=[4,2,1], timestep=0.5, returnflow=False):\n",
        "        timestep = (x[:, :1].clone() * 0 + 1) * timestep\n",
        "        img0 = x[:, :3]\n",
        "        img1 = x[:, 3:6]\n",
        "        gt = x[:, 6:] # In inference time, gt is None\n",
        "        flow_list = []\n",
        "        merged = []\n",
        "        mask_list = []\n",
        "        warped_img0 = img0\n",
        "        warped_img1 = img1\n",
        "        flow = None\n",
        "        loss_distill = 0\n",
        "        stu = [self.block0, self.block1, self.block2]\n",
        "        for i in range(3):\n",
        "            if flow != None:\n",
        "                flow_d, mask_d = stu[i](torch.cat((img0, img1, timestep, warped_img0, warped_img1, mask), 1), flow, scale=scale[i])\n",
        "                flow = flow + flow_d\n",
        "                mask = mask + mask_d\n",
        "            else:\n",
        "                flow, mask = stu[i](torch.cat((img0, img1, timestep), 1), None, scale=scale[i])\n",
        "            mask_list.append(torch.sigmoid(mask))\n",
        "            flow_list.append(flow)\n",
        "            warped_img0 = warp(img0, flow[:, :2])\n",
        "            warped_img1 = warp(img1, flow[:, 2:4])\n",
        "            merged_student = (warped_img0, warped_img1)\n",
        "            merged.append(merged_student)\n",
        "        if gt.shape[1] == 3:\n",
        "            flow_d, mask_d = self.block_tea(torch.cat((img0, img1, timestep, warped_img0, warped_img1, mask, gt), 1), flow, scale=1)\n",
        "            flow_teacher = flow + flow_d\n",
        "            warped_img0_teacher = warp(img0, flow_teacher[:, :2])\n",
        "            warped_img1_teacher = warp(img1, flow_teacher[:, 2:4])\n",
        "            mask_teacher = torch.sigmoid(mask + mask_d)\n",
        "            merged_teacher = warped_img0_teacher * mask_teacher + warped_img1_teacher * (1 - mask_teacher)\n",
        "        else:\n",
        "            flow_teacher = None\n",
        "            merged_teacher = None\n",
        "        for i in range(3):\n",
        "            merged[i] = merged[i][0] * mask_list[i] + merged[i][1] * (1 - mask_list[i])\n",
        "            if gt.shape[1] == 3:\n",
        "                loss_mask = ((merged[i] - gt).abs().mean(1, True) > (merged_teacher - gt).abs().mean(1, True) + 0.01).float().detach()\n",
        "                loss_distill += (((flow_teacher.detach() - flow_list[i]) ** 2).mean(1, True) ** 0.5 * loss_mask).mean()\n",
        "        if returnflow:\n",
        "            return flow\n",
        "        else:\n",
        "            c0 = self.contextnet(img0, flow[:, :2])\n",
        "            c1 = self.contextnet(img1, flow[:, 2:4])\n",
        "            tmp = self.unet(img0, img1, warped_img0, warped_img1, mask, flow, c0, c1)\n",
        "            res = tmp[:, :3] * 2 - 1\n",
        "            merged[2] = torch.clamp(merged[2] + res, 0, 1)\n",
        "        return flow_list, mask_list[2], merged, flow_teacher, merged_teacher, loss_distill"
      ],
      "metadata": {
        "id": "ZsYTN8txeZUd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class EPE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EPE, self).__init__()\n",
        "\n",
        "    def forward(self, flow, gt, loss_mask):\n",
        "        loss_map = (flow - gt.detach()) ** 2\n",
        "        loss_map = (loss_map.sum(1, True) + 1e-6) ** 0.5\n",
        "        return (loss_map * loss_mask)\n",
        "\n",
        "\n",
        "class Ternary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ternary, self).__init__()\n",
        "        patch_size = 7\n",
        "        out_channels = patch_size * patch_size\n",
        "        self.w = np.eye(out_channels).reshape(\n",
        "            (patch_size, patch_size, 1, out_channels))\n",
        "        self.w = np.transpose(self.w, (3, 2, 0, 1))\n",
        "        self.w = torch.tensor(self.w).float().to(device)\n",
        "\n",
        "    def transform(self, img):\n",
        "        patches = F.conv2d(img, self.w, padding=3, bias=None)\n",
        "        transf = patches - img\n",
        "        transf_norm = transf / torch.sqrt(0.81 + transf**2)\n",
        "        return transf_norm\n",
        "\n",
        "    def rgb2gray(self, rgb):\n",
        "        r, g, b = rgb[:, 0:1, :, :], rgb[:, 1:2, :, :], rgb[:, 2:3, :, :]\n",
        "        gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "        return gray\n",
        "\n",
        "    def hamming(self, t1, t2):\n",
        "        dist = (t1 - t2) ** 2\n",
        "        dist_norm = torch.mean(dist / (0.1 + dist), 1, True)\n",
        "        return dist_norm\n",
        "\n",
        "    def valid_mask(self, t, padding):\n",
        "        n, _, h, w = t.size()\n",
        "        inner = torch.ones(n, 1, h - 2 * padding, w - 2 * padding).type_as(t)\n",
        "        mask = F.pad(inner, [padding] * 4)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, img0, img1):\n",
        "        img0 = self.transform(self.rgb2gray(img0))\n",
        "        img1 = self.transform(self.rgb2gray(img1))\n",
        "        return self.hamming(img0, img1) * self.valid_mask(img0, 1)\n",
        "\n",
        "\n",
        "class SOBEL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SOBEL, self).__init__()\n",
        "        self.kernelX = torch.tensor([\n",
        "            [1, 0, -1],\n",
        "            [2, 0, -2],\n",
        "            [1, 0, -1],\n",
        "        ]).float()\n",
        "        self.kernelY = self.kernelX.clone().T\n",
        "        self.kernelX = self.kernelX.unsqueeze(0).unsqueeze(0).to(device)\n",
        "        self.kernelY = self.kernelY.unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        N, C, H, W = pred.shape[0], pred.shape[1], pred.shape[2], pred.shape[3]\n",
        "        img_stack = torch.cat(\n",
        "            [pred.reshape(N*C, 1, H, W), gt.reshape(N*C, 1, H, W)], 0)\n",
        "        sobel_stack_x = F.conv2d(img_stack, self.kernelX, padding=1)\n",
        "        sobel_stack_y = F.conv2d(img_stack, self.kernelY, padding=1)\n",
        "        pred_X, gt_X = sobel_stack_x[:N*C], sobel_stack_x[N*C:]\n",
        "        pred_Y, gt_Y = sobel_stack_y[:N*C], sobel_stack_y[N*C:]\n",
        "\n",
        "        L1X, L1Y = torch.abs(pred_X-gt_X), torch.abs(pred_Y-gt_Y)\n",
        "        loss = (L1X+L1Y)\n",
        "        return loss\n",
        "\n",
        "class MeanShift(nn.Conv2d):\n",
        "    def __init__(self, data_mean, data_std, data_range=1, norm=True):\n",
        "        c = len(data_mean)\n",
        "        super(MeanShift, self).__init__(c, c, kernel_size=1)\n",
        "        std = torch.Tensor(data_std)\n",
        "        self.weight.data = torch.eye(c).view(c, c, 1, 1)\n",
        "        if norm:\n",
        "            self.weight.data.div_(std.view(c, 1, 1, 1))\n",
        "            self.bias.data = -1 * data_range * torch.Tensor(data_mean)\n",
        "            self.bias.data.div_(std)\n",
        "        else:\n",
        "            self.weight.data.mul_(std.view(c, 1, 1, 1))\n",
        "            self.bias.data = data_range * torch.Tensor(data_mean)\n",
        "        self.requires_grad = False\n",
        "\n",
        "class VGGPerceptualLoss(torch.nn.Module):\n",
        "    def __init__(self, rank=0):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        blocks = []\n",
        "        pretrained = True\n",
        "        self.vgg_pretrained_features = models.vgg19(pretrained=pretrained).features\n",
        "        self.normalize = MeanShift([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], norm=True).cuda()\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, X, Y, indices=None):\n",
        "        X = self.normalize(X)\n",
        "        Y = self.normalize(Y)\n",
        "        indices = [2, 7, 12, 21, 30]\n",
        "        weights = [1.0/2.6, 1.0/4.8, 1.0/3.7, 1.0/5.6, 10/1.5]\n",
        "        k = 0\n",
        "        loss = 0\n",
        "        for i in range(indices[-1]):\n",
        "            X = self.vgg_pretrained_features[i](X)\n",
        "            Y = self.vgg_pretrained_features[i](Y)\n",
        "            if (i+1) in indices:\n",
        "                loss += weights[k] * (X - Y.detach()).abs().mean() * 0.1\n",
        "                k += 1\n",
        "        return loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    img0 = torch.zeros(3, 3, 256, 256).float().to(device)\n",
        "    img1 = torch.tensor(np.random.normal(\n",
        "        0, 1, (3, 3, 256, 256))).float().to(device)\n",
        "    ternary_loss = Ternary()\n",
        "    print(ternary_loss(img0, img1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofJCfWpkefEV",
        "outputId": "6d1981c3-9305-4977-da0a-440fec9ed200"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import torch\n",
        "\n",
        "def gauss_kernel(size=5, channels=3):\n",
        "    kernel = torch.tensor([[1., 4., 6., 4., 1],\n",
        "                           [4., 16., 24., 16., 4.],\n",
        "                           [6., 24., 36., 24., 6.],\n",
        "                           [4., 16., 24., 16., 4.],\n",
        "                           [1., 4., 6., 4., 1.]])\n",
        "    kernel /= 256.\n",
        "    kernel = kernel.repeat(channels, 1, 1, 1)\n",
        "    kernel = kernel.to(device)\n",
        "    return kernel\n",
        "\n",
        "def downsample(x):\n",
        "    return x[:, :, ::2, ::2]\n",
        "\n",
        "def upsample(x):\n",
        "    cc = torch.cat([x, torch.zeros(x.shape[0], x.shape[1], x.shape[2], x.shape[3]).to(device)], dim=3)\n",
        "    cc = cc.view(x.shape[0], x.shape[1], x.shape[2]*2, x.shape[3])\n",
        "    cc = cc.permute(0,1,3,2)\n",
        "    cc = torch.cat([cc, torch.zeros(x.shape[0], x.shape[1], x.shape[3], x.shape[2]*2).to(device)], dim=3)\n",
        "    cc = cc.view(x.shape[0], x.shape[1], x.shape[3]*2, x.shape[2]*2)\n",
        "    x_up = cc.permute(0,1,3,2)\n",
        "    return conv_gauss(x_up, 4*gauss_kernel(channels=x.shape[1]))\n",
        "\n",
        "def conv_gauss(img, kernel):\n",
        "    img = torch.nn.functional.pad(img, (2, 2, 2, 2), mode='reflect')\n",
        "    out = torch.nn.functional.conv2d(img, kernel, groups=img.shape[1])\n",
        "    return out\n",
        "\n",
        "def laplacian_pyramid(img, kernel, max_levels=3):\n",
        "    current = img\n",
        "    pyr = []\n",
        "    for level in range(max_levels):\n",
        "        filtered = conv_gauss(current, kernel)\n",
        "        down = downsample(filtered)\n",
        "        up = upsample(down)\n",
        "        diff = current-up\n",
        "        pyr.append(diff)\n",
        "        current = down\n",
        "    return pyr\n",
        "\n",
        "class LapLoss(torch.nn.Module):\n",
        "    def __init__(self, max_levels=5, channels=3):\n",
        "        super(LapLoss, self).__init__()\n",
        "        self.max_levels = max_levels\n",
        "        self.gauss_kernel = gauss_kernel(channels=channels)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        pyr_input  = laplacian_pyramid(img=input, kernel=self.gauss_kernel, max_levels=self.max_levels)\n",
        "        pyr_target = laplacian_pyramid(img=target, kernel=self.gauss_kernel, max_levels=self.max_levels)\n",
        "        return sum(torch.nn.functional.l1_loss(a, b) for a, b in zip(pyr_input, pyr_target))"
      ],
      "metadata": {
        "id": "mELmrTPoejZt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "# from model.warplayer import warp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "# from model.IFNet import *\n",
        "# from model.IFNet_m import *\n",
        "import torch.nn.functional as F\n",
        "# from model.loss import *\n",
        "# from model.laplacian import *\n",
        "# from model.refine import *\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, local_rank=-1, arbitrary=False):\n",
        "        if arbitrary == True:\n",
        "            self.flownet = IFNet_m()\n",
        "        else:\n",
        "            self.flownet = IFNet()\n",
        "        self.device()\n",
        "        self.optimG = AdamW(self.flownet.parameters(), lr=1e-6, weight_decay=1e-3) # use large weight decay may avoid NaN loss\n",
        "        self.epe = EPE()\n",
        "        self.lap = LapLoss()\n",
        "        self.sobel = SOBEL()\n",
        "        if local_rank != -1:\n",
        "            self.flownet = DDP(self.flownet, device_ids=[local_rank], output_device=local_rank)\n",
        "\n",
        "    def train(self):\n",
        "        self.flownet.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.flownet.eval()\n",
        "\n",
        "    def device(self):\n",
        "        self.flownet.to(device)\n",
        "\n",
        "    def load_model(self, path, rank=0):\n",
        "        def convert(param):\n",
        "            return {\n",
        "            k.replace(\"module.\", \"\"): v\n",
        "                for k, v in param.items()\n",
        "                if \"module.\" in k\n",
        "            }\n",
        "\n",
        "        if rank <= 0:\n",
        "            self.flownet.load_state_dict(convert(torch.load('{}/flownet.pkl'.format(path))))\n",
        "\n",
        "    def save_model(self, path, rank=0):\n",
        "        if rank == 0:\n",
        "            torch.save(self.flownet.state_dict(),'{}/flownet.pkl'.format(path))\n",
        "\n",
        "    def inference(self, img0, img1, scale=1, scale_list=[4, 2, 1], TTA=False, timestep=0.5):\n",
        "        for i in range(3):\n",
        "            scale_list[i] = scale_list[i] * 1.0 / scale\n",
        "        imgs = torch.cat((img0, img1), 1)\n",
        "        flow, mask, merged, flow_teacher, merged_teacher, loss_distill = self.flownet(imgs, scale_list, timestep=timestep)\n",
        "        if TTA == False:\n",
        "            return merged[2]\n",
        "        else:\n",
        "            flow2, mask2, merged2, flow_teacher2, merged_teacher2, loss_distill2 = self.flownet(imgs.flip(2).flip(3), scale_list, timestep=timestep)\n",
        "            return (merged[2] + merged2[2].flip(2).flip(3)) / 2\n",
        "\n",
        "    def update(self, imgs, gt, learning_rate=0, mul=1, training=True, flow_gt=None):\n",
        "        for param_group in self.optimG.param_groups:\n",
        "            param_group['lr'] = learning_rate\n",
        "        img0 = imgs[:, :3]\n",
        "        img1 = imgs[:, 3:]\n",
        "        if training:\n",
        "            self.train()\n",
        "        else:\n",
        "            self.eval()\n",
        "        flow, mask, merged, flow_teacher, merged_teacher, loss_distill = self.flownet(torch.cat((imgs, gt), 1), scale=[4, 2, 1])\n",
        "        loss_l1 = (self.lap(merged[2], gt)).mean()\n",
        "        loss_tea = (self.lap(merged_teacher, gt)).mean()\n",
        "        if training:\n",
        "            self.optimG.zero_grad()\n",
        "            loss_G = loss_l1 + loss_tea + loss_distill * 0.01 # when training RIFEm, the weight of loss_distill should be 0.005 or 0.002\n",
        "            loss_G.backward()\n",
        "            self.optimG.step()\n",
        "        else:\n",
        "            flow_teacher = flow[2]\n",
        "        return merged[2], {\n",
        "            'merged_tea': merged_teacher,\n",
        "            'mask': mask,\n",
        "            'mask_tea': mask,\n",
        "            'flow': flow[2][:, :2],\n",
        "            'flow_tea': flow_teacher,\n",
        "            'loss_l1': loss_l1,\n",
        "            'loss_tea': loss_tea,\n",
        "            'loss_distill': loss_distill,\n",
        "            }"
      ],
      "metadata": {
        "id": "QPrYRVuTeDA9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model(arbitrary=False)\n",
        "model.eval()\n",
        "\n",
        "# Generate dummy inputs (batch size = 1, 3 channels (RGB), height = 128, width = 128)\n",
        "img0 = torch.randn(1, 6, 128, 128).to(device) # Input\n",
        "img1 = torch.randn(1, 6, 128, 128).to(device) # Target\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model.inference(img0, img1, scale=1.0, TTA=False, timestep=0.5)\n",
        "\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXRxOq2AeuM7",
        "outputId": "0ed73b30-813c-4115-dd52-e894ba099d40"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = torch.randn(1, 6, 128, 128).to(device)  # Concatenated img0 and img1\n",
        "gt = torch.randn(1, 3, 128, 128).to(device)    # Ground truth middle frame\n",
        "\n",
        "# Forward pass directly on the flownet\n",
        "flow, mask, merged, flow_teacher, merged_teacher, loss_distill = model.flownet(\n",
        "    torch.cat((imgs, gt), 1), scale=[4, 2, 1]\n",
        ")\n",
        "\n",
        "# # Compute loss manually\n",
        "# loss = loss_fn(merged[2], gt)\n",
        "# loss.backward()"
      ],
      "metadata": {
        "id": "UU8Mh_DTfDxQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flow[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOuXO54XgBdo",
        "outputId": "af0cbafb-0720-4f6a-cabf-dd35a816eec2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tMuF_8wgCmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}