{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln8FWPIklYHx",
        "outputId": "63be700e-de49-41da-ea05-c13a01758fe1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=dfe5879c3909207b06ed120ac1cef81e99888898d9d6fc48eff33ceeb2d09326\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=22523ad1010c82cb8452d9eefb0e16550bdabb6c27d4e200e454edf09a303ab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "from timm.models.layers import to_2tuple, trunc_normal_\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeHqjZVTkd1B",
        "outputId": "4ed701ed-a4e9-44fc-e659-dd5cfae4f466"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "efoipRaBkIKt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "import os\n",
        "# from openstl.utils import measure_throughput\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "# from openstl.modules import Attention, PreNorm, FeedForward\n",
        "import math\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_features,\n",
        "            hidden_features=None,\n",
        "            out_features=None,\n",
        "            act_layer=nn.SiLU,\n",
        "            norm_layer=None,\n",
        "            bias=True,\n",
        "            drop=0.,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        bias = to_2tuple(bias)\n",
        "        drop_probs = to_2tuple(drop)\n",
        "\n",
        "        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])\n",
        "        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop_probs[0])\n",
        "        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n",
        "        self.drop2 = nn.Dropout(drop_probs[1])\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.ones_(self.fc1_g.bias)\n",
        "        nn.init.normal_(self.fc1_g.weight, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_gate = self.fc1_g(x)\n",
        "        x = self.fc1_x(x)\n",
        "        x = self.act(x_gate) * x\n",
        "        x = self.drop1(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "class GatedTransformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0., attn_dropout=0., drop_path=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=attn_dropout)),\n",
        "                PreNorm(dim, SwiGLU(dim, mlp_dim, drop=dropout)),\n",
        "                DropPath(drop_path) if drop_path > 0. else nn.Identity(),\n",
        "                DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "            ]))\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff, drop_path1,drop_path2 in self.layers:\n",
        "            x = x + drop_path1(attn(x))\n",
        "            x = x + drop_path2(ff(x))\n",
        "        return self.norm(x)\n",
        "\n",
        "class PredFormerLayer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0., attn_dropout=0., drop_path=0.1):\n",
        "        super(PredFormerLayer, self).__init__()\n",
        "\n",
        "        self.ts_temporal_transformer = GatedTransformer(dim, depth, heads, dim_head,\n",
        "                                                   mlp_dim, dropout, attn_dropout, drop_path)\n",
        "        self.ts_space_transformer = GatedTransformer(dim, depth, heads, dim_head,\n",
        "                                                mlp_dim, dropout, attn_dropout, drop_path)\n",
        "        self.st_space_transformer = GatedTransformer(dim, depth, heads, dim_head,\n",
        "                                                mlp_dim, dropout, attn_dropout, drop_path)\n",
        "        self.st_temporal_transformer = GatedTransformer(dim, depth, heads, dim_head,\n",
        "                                                   mlp_dim, dropout, attn_dropout, drop_path)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, n, _ = x.shape\n",
        "        x_ts, x_ori = x, x\n",
        "\n",
        "\n",
        "        # ts-t branch\n",
        "        x_ts = rearrange(x_ts, 'b t n d -> b n t d')\n",
        "        x_ts = rearrange(x_ts, 'b n t d -> (b n) t d')\n",
        "        x_ts = self.ts_temporal_transformer(x_ts)\n",
        "\n",
        "        # ts-s branch\n",
        "        x_ts = rearrange(x_ts, '(b n) t d -> b n t d', b=b)\n",
        "        x_ts = rearrange(x_ts, 'b n t d -> b t n d')\n",
        "        x_ts = rearrange(x_ts, 'b t n d -> (b t) n d')\n",
        "        x_ts = self.ts_space_transformer(x_ts)\n",
        "\n",
        "\n",
        "        # ts output branch\n",
        "        x_ts = rearrange(x_ts, '(b t) n d -> b t n d', b=b)\n",
        "\n",
        "        # add\n",
        "        # x_ts += x_ori\n",
        "\n",
        "        x_st, x_ori = x_ts, x_ts\n",
        "\n",
        "        # st-s branch\n",
        "        x_st = rearrange(x_st, 'b t n d -> (b t) n d')\n",
        "        x_st = self.st_space_transformer(x_st)\n",
        "\n",
        "        # st-t branch\n",
        "        x_st = rearrange(x_st, '(b t) ... -> b t ...', b=b)\n",
        "        x_st = x_st.permute(0, 2, 1, 3) # b n T d\n",
        "        x_st = rearrange(x_st, 'b n t d -> (b n) t d')\n",
        "        x_st = self.st_temporal_transformer(x_st)\n",
        "\n",
        "        # st output branch\n",
        "        x_st = rearrange(x_st, '(b n) t d -> b n t d', b=b)\n",
        "        x_st = rearrange(x_st, 'b n t d -> b t n d', b=b)\n",
        "\n",
        "        return x_st\n",
        "\n",
        "def sinusoidal_embedding(n_channels, dim):\n",
        "    pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                            for p in range(n_channels)])\n",
        "    pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "    pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "    return rearrange(pe, '... -> 1 ...')\n",
        "\n",
        "class PredFormer_Model(nn.Module):\n",
        "    def __init__(self, model_config, **kwargs):\n",
        "        super().__init__()\n",
        "        self.image_height = model_config['height']\n",
        "        self.image_width = model_config['width']\n",
        "        self.patch_size = model_config['patch_size']\n",
        "        self.num_patches_h = self.image_height // self.patch_size\n",
        "        self.num_patches_w = self.image_width // self.patch_size\n",
        "        self.num_patches = self.num_patches_h * self.num_patches_w\n",
        "        self.num_frames_in = model_config['pre_seq']\n",
        "        self.dim = model_config['dim']\n",
        "        self.num_channels = model_config['num_channels']\n",
        "        self.num_classes = self.num_channels\n",
        "        self.heads = model_config['heads']\n",
        "        self.dim_head = model_config['dim_head']\n",
        "        self.dropout = model_config['dropout']\n",
        "        self.attn_dropout = model_config['attn_dropout']\n",
        "        self.drop_path = model_config['drop_path']\n",
        "        self.scale_dim = model_config['scale_dim']\n",
        "        self.Ndepth = model_config['Ndepth']  # Ensure this is defined\n",
        "        self.depth = model_config['depth']  # Ensure this is defined\n",
        "\n",
        "        assert self.image_height % self.patch_size == 0, 'Image height must be divisible by the patch size.'\n",
        "        assert self.image_width % self.patch_size == 0, 'Image width must be divisible by the patch size.'\n",
        "        self.patch_dim = self.num_channels * self.patch_size ** 2\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size),\n",
        "            nn.Linear(self.patch_dim, self.dim),\n",
        "            )\n",
        "        self.pos_embedding = nn.Parameter(sinusoidal_embedding(self.num_frames_in * self.num_patches, self.dim),\n",
        "                                               requires_grad=False).view(1, self.num_frames_in, self.num_patches, self.dim)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            PredFormerLayer(self.dim, self.depth, self.heads, self.dim_head, self.dim * self.scale_dim, self.dropout, self.attn_dropout, self.drop_path)\n",
        "            for i in range(self.Ndepth)\n",
        "        ])\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(self.dim),\n",
        "            nn.Linear(self.dim, self.num_channels * self.patch_size ** 2)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Patch Embedding\n",
        "        x = self.to_patch_embedding(x)\n",
        "\n",
        "        # Posion Embedding\n",
        "        x += self.pos_embedding.to(x.device)\n",
        "\n",
        "        # PredFormer Encoder\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # MLP head\n",
        "        x = self.mlp_head(x.reshape(-1, self.dim))\n",
        "        x = x.view(B, T, self.num_patches_h, self.num_patches_w, C, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 1, 4, 2, 5, 3, 6).reshape(B, T, C, H, W)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    # image h w c\n",
        "    'height': 64,\n",
        "    'width': 64,\n",
        "    'num_channels': 1,\n",
        "    # video length in and out\n",
        "    'pre_seq': 10,\n",
        "    'after_seq': 10,\n",
        "    # patch size\n",
        "    'patch_size': 8,\n",
        "    'dim': 256,\n",
        "    'heads': 8,\n",
        "    'dim_head': 32,\n",
        "    # dropout\n",
        "    'dropout': 0.0,\n",
        "    'attn_dropout': 0.0,\n",
        "    'drop_path': 0.0,\n",
        "    'scale_dim': 4,\n",
        "    # depth\n",
        "    'depth': 1,\n",
        "    'Ndepth': 6\n",
        "}\n",
        "\n",
        "model = PredFormer_Model(model_config)\n",
        "x = torch.rand(1, 10, 1, 64, 64)\n",
        "output = model(x)\n",
        "print(output.shape)  # [B, T, C, H, W]\n",
        "# # Calculate FLOPs\n",
        "flops = FlopCountAnalysis(model, x)\n",
        "print(f'Number of flops: {flop_count_table(flops)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfnGVkPflNPA",
        "outputId": "5b14dbba-29e6-41bf-9aeb-f58387be8e1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 1, 64, 64])\n",
            "Number of flops: | module                              | #parameters or shape   | #flops    |\n",
            "|:------------------------------------|:-----------------------|:----------|\n",
            "| model                               | 25.298M                | 16.478G   |\n",
            "|  to_patch_embedding.1               |  16.64K                |  10.486M  |\n",
            "|   to_patch_embedding.1.weight       |   (256, 64)            |           |\n",
            "|   to_patch_embedding.1.bias         |   (256,)               |           |\n",
            "|  blocks                             |  25.264M               |  16.456G  |\n",
            "|   blocks.0                          |   4.211M               |   2.743G  |\n",
            "|    blocks.0.ts_temporal_transformer |    1.053M              |    0.677G |\n",
            "|    blocks.0.ts_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.0.st_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.0.st_temporal_transformer |    1.053M              |    0.677G |\n",
            "|   blocks.1                          |   4.211M               |   2.743G  |\n",
            "|    blocks.1.ts_temporal_transformer |    1.053M              |    0.677G |\n",
            "|    blocks.1.ts_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.1.st_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.1.st_temporal_transformer |    1.053M              |    0.677G |\n",
            "|   blocks.2                          |   4.211M               |   2.743G  |\n",
            "|    blocks.2.ts_temporal_transformer |    1.053M              |    0.677G |\n",
            "|    blocks.2.ts_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.2.st_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.2.st_temporal_transformer |    1.053M              |    0.677G |\n",
            "|   blocks.3                          |   4.211M               |   2.743G  |\n",
            "|    blocks.3.ts_temporal_transformer |    1.053M              |    0.677G |\n",
            "|    blocks.3.ts_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.3.st_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.3.st_temporal_transformer |    1.053M              |    0.677G |\n",
            "|   blocks.4                          |   4.211M               |   2.743G  |\n",
            "|    blocks.4.ts_temporal_transformer |    1.053M              |    0.677G |\n",
            "|    blocks.4.ts_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.4.st_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.4.st_temporal_transformer |    1.053M              |    0.677G |\n",
            "|   blocks.5                          |   4.211M               |   2.743G  |\n",
            "|    blocks.5.ts_temporal_transformer |    1.053M              |    0.677G |\n",
            "|    blocks.5.ts_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.5.st_space_transformer    |    1.053M              |    0.695G |\n",
            "|    blocks.5.st_temporal_transformer |    1.053M              |    0.677G |\n",
            "|  mlp_head                           |  16.96K                |  11.305M  |\n",
            "|   mlp_head.0                        |   0.512K               |   0.819M  |\n",
            "|    mlp_head.0.weight                |    (256,)              |           |\n",
            "|    mlp_head.0.bias                  |    (256,)              |           |\n",
            "|   mlp_head.1                        |   16.448K              |   10.486M |\n",
            "|    mlp_head.1.weight                |    (64, 256)           |           |\n",
            "|    mlp_head.1.bias                  |    (64,)               |           |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BfDDiuV5ll02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}