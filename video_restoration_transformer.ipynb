{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nKxJq4Osfh95"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under the BSD license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from distutils.version import LooseVersion\n",
        "from torch.nn.modules.utils import _pair, _single\n",
        "import numpy as np\n",
        "from functools import reduce, lru_cache\n",
        "from operator import mul\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "\n",
        "class ModulatedDeformConv(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 groups=1,\n",
        "                 deformable_groups=1,\n",
        "                 bias=True):\n",
        "        super(ModulatedDeformConv, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = _pair(kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.groups = groups\n",
        "        self.deformable_groups = deformable_groups\n",
        "        self.with_bias = bias\n",
        "        # enable compatibility with nn.Conv2d\n",
        "        self.transposed = False\n",
        "        self.output_padding = _single(0)\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        n = self.in_channels\n",
        "        for k in self.kernel_size:\n",
        "            n *= k\n",
        "        stdv = 1. / math.sqrt(n)\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "    # def forward(self, x, offset, mask):\n",
        "    #     return modulated_deform_conv(x, offset, mask, self.weight, self.bias, self.stride, self.padding, self.dilation,\n",
        "    #                                  self.groups, self.deformable_groups)\n",
        "\n",
        "\n",
        "class ModulatedDeformConvPack(ModulatedDeformConv):\n",
        "    \"\"\"A ModulatedDeformable Conv Encapsulation that acts as normal Conv layers.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Same as nn.Conv2d.\n",
        "        out_channels (int): Same as nn.Conv2d.\n",
        "        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n",
        "        stride (int or tuple[int]): Same as nn.Conv2d.\n",
        "        padding (int or tuple[int]): Same as nn.Conv2d.\n",
        "        dilation (int or tuple[int]): Same as nn.Conv2d.\n",
        "        groups (int): Same as nn.Conv2d.\n",
        "        bias (bool or str): If specified as `auto`, it will be decided by the\n",
        "            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n",
        "            False.\n",
        "    \"\"\"\n",
        "\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ModulatedDeformConvPack, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.conv_offset = nn.Conv2d(\n",
        "            self.in_channels,\n",
        "            self.deformable_groups * 3 * self.kernel_size[0] * self.kernel_size[1],\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=_pair(self.stride),\n",
        "            padding=_pair(self.padding),\n",
        "            dilation=_pair(self.dilation),\n",
        "            bias=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        super(ModulatedDeformConvPack, self).init_weights()\n",
        "        if hasattr(self, 'conv_offset'):\n",
        "            self.conv_offset.weight.data.zero_()\n",
        "            self.conv_offset.bias.data.zero_()\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     out = self.conv_offset(x)\n",
        "    #     o1, o2, mask = torch.chunk(out, 3, dim=1)\n",
        "    #     offset = torch.cat((o1, o2), dim=1)\n",
        "    #     mask = torch.sigmoid(mask)\n",
        "    #     return modulated_deform_conv(x, offset, mask, self.weight, self.bias, self.stride, self.padding, self.dilation,\n",
        "    #                                  self.groups, self.deformable_groups)\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\n",
        "            'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '\n",
        "            'The distribution of values may be incorrect.',\n",
        "            stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        low = norm_cdf((a - mean) / std)\n",
        "        up = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [low, up], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * low - 1, 2 * up - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution.\n",
        "\n",
        "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py\n",
        "\n",
        "    The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "def flow_warp(x, flow, interp_mode='bilinear', padding_mode='zeros', align_corners=True, use_pad_mask=False):\n",
        "    \"\"\"Warp an image or feature map with optical flow.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Tensor with size (n, c, h, w).\n",
        "        flow (Tensor): Tensor with size (n, h, w, 2), normal value.\n",
        "        interp_mode (str): 'nearest' or 'bilinear' or 'nearest4'. Default: 'bilinear'.\n",
        "        padding_mode (str): 'zeros' or 'border' or 'reflection'.\n",
        "            Default: 'zeros'.\n",
        "        align_corners (bool): Before pytorch 1.3, the default value is\n",
        "            align_corners=True. After pytorch 1.3, the default value is\n",
        "            align_corners=False. Here, we use the True as default.\n",
        "        use_pad_mask (bool): only used for PWCNet, x is first padded with ones along the channel dimension.\n",
        "            The mask is generated according to the grid_sample results of the padded dimension.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Warped image or feature map.\n",
        "    \"\"\"\n",
        "    # assert x.size()[-2:] == flow.size()[1:3] # temporaily turned off for image-wise shift\n",
        "    n, _, h, w = x.size()\n",
        "    # create mesh grid\n",
        "    # grid_y, grid_x = torch.meshgrid(torch.arange(0, h).type_as(x), torch.arange(0, w).type_as(x)) # an illegal memory access on TITAN RTX + PyTorch1.9.1\n",
        "    grid_y, grid_x = torch.meshgrid(torch.arange(0, h, dtype=x.dtype, device=x.device), torch.arange(0, w, dtype=x.dtype, device=x.device))\n",
        "    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n",
        "    grid.requires_grad = False\n",
        "\n",
        "    vgrid = grid + flow\n",
        "\n",
        "    # if use_pad_mask: # for PWCNet\n",
        "    #     x = F.pad(x, (0,0,0,0,0,1), mode='constant', value=1)\n",
        "\n",
        "    # scale grid to [-1,1]\n",
        "    if interp_mode == 'nearest4': # todo: bug, no gradient for flow model in this case!!! but the result is good\n",
        "        vgrid_x_floor = 2.0 * torch.floor(vgrid[:, :, :, 0]) / max(w - 1, 1) - 1.0\n",
        "        vgrid_x_ceil = 2.0 * torch.ceil(vgrid[:, :, :, 0]) / max(w - 1, 1) - 1.0\n",
        "        vgrid_y_floor = 2.0 * torch.floor(vgrid[:, :, :, 1]) / max(h - 1, 1) - 1.0\n",
        "        vgrid_y_ceil = 2.0 * torch.ceil(vgrid[:, :, :, 1]) / max(h - 1, 1) - 1.0\n",
        "\n",
        "        output00 = F.grid_sample(x, torch.stack((vgrid_x_floor, vgrid_y_floor), dim=3), mode='nearest', padding_mode=padding_mode, align_corners=align_corners)\n",
        "        output01 = F.grid_sample(x, torch.stack((vgrid_x_floor, vgrid_y_ceil), dim=3), mode='nearest', padding_mode=padding_mode, align_corners=align_corners)\n",
        "        output10 = F.grid_sample(x, torch.stack((vgrid_x_ceil, vgrid_y_floor), dim=3), mode='nearest', padding_mode=padding_mode, align_corners=align_corners)\n",
        "        output11 = F.grid_sample(x, torch.stack((vgrid_x_ceil, vgrid_y_ceil), dim=3), mode='nearest', padding_mode=padding_mode, align_corners=align_corners)\n",
        "\n",
        "        return torch.cat([output00, output01, output10, output11], 1)\n",
        "\n",
        "    else:\n",
        "        vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(w - 1, 1) - 1.0\n",
        "        vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(h - 1, 1) - 1.0\n",
        "        vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n",
        "        output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode, align_corners=align_corners)\n",
        "\n",
        "        # if use_pad_mask: # for PWCNet\n",
        "        #     output = _flow_warp_masking(output)\n",
        "\n",
        "        # TODO, what if align_corners=False\n",
        "        return output\n",
        "\n",
        "\n",
        "class DCNv2PackFlowGuided(ModulatedDeformConvPack):\n",
        "    \"\"\"Flow-guided deformable alignment module.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Same as nn.Conv2d.\n",
        "        out_channels (int): Same as nn.Conv2d.\n",
        "        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n",
        "        stride (int or tuple[int]): Same as nn.Conv2d.\n",
        "        padding (int or tuple[int]): Same as nn.Conv2d.\n",
        "        dilation (int or tuple[int]): Same as nn.Conv2d.\n",
        "        groups (int): Same as nn.Conv2d.\n",
        "        bias (bool or str): If specified as `auto`, it will be decided by the\n",
        "            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n",
        "            False.\n",
        "        max_residue_magnitude (int): The maximum magnitude of the offset residue. Default: 10.\n",
        "        pa_frames (int): The number of parallel warping frames. Default: 2.\n",
        "\n",
        "    Ref:\n",
        "        BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.max_residue_magnitude = kwargs.pop('max_residue_magnitude', 10)\n",
        "        self.pa_frames = kwargs.pop('pa_frames', 2)\n",
        "\n",
        "        super(DCNv2PackFlowGuided, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.conv_offset = nn.Sequential(\n",
        "            nn.Conv2d((1+self.pa_frames//2) * self.in_channels + self.pa_frames, self.out_channels, 3, 1, 1),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, 3, 1, 1),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, 3, 1, 1),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.Conv2d(self.out_channels, 3 * 9 * self.deformable_groups, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        self.init_offset()\n",
        "\n",
        "    def init_offset(self):\n",
        "        super(ModulatedDeformConvPack, self).init_weights()\n",
        "        if hasattr(self, 'conv_offset'):\n",
        "            self.conv_offset[-1].weight.data.zero_()\n",
        "            self.conv_offset[-1].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, x_flow_warpeds, x_current, flows):\n",
        "        out = self.conv_offset(torch.cat(x_flow_warpeds + [x_current] + flows, dim=1))\n",
        "        o1, o2, mask = torch.chunk(out, 3, dim=1)\n",
        "\n",
        "        # offset\n",
        "        offset = self.max_residue_magnitude * torch.tanh(torch.cat((o1, o2), dim=1))\n",
        "        if self.pa_frames == 2:\n",
        "            offset = offset + flows[0].flip(1).repeat(1, offset.size(1)//2, 1, 1)\n",
        "        elif self.pa_frames == 4:\n",
        "            offset1, offset2 = torch.chunk(offset, 2, dim=1)\n",
        "            offset1 = offset1 + flows[0].flip(1).repeat(1, offset1.size(1) // 2, 1, 1)\n",
        "            offset2 = offset2 + flows[1].flip(1).repeat(1, offset2.size(1) // 2, 1, 1)\n",
        "            offset = torch.cat([offset1, offset2], dim=1)\n",
        "        elif self.pa_frames == 6:\n",
        "            offset = self.max_residue_magnitude * torch.tanh(torch.cat((o1, o2), dim=1))\n",
        "            offset1, offset2, offset3 = torch.chunk(offset, 3, dim=1)\n",
        "            offset1 = offset1 + flows[0].flip(1).repeat(1, offset1.size(1) // 2, 1, 1)\n",
        "            offset2 = offset2 + flows[1].flip(1).repeat(1, offset2.size(1) // 2, 1, 1)\n",
        "            offset3 = offset3 + flows[2].flip(1).repeat(1, offset3.size(1) // 2, 1, 1)\n",
        "            offset = torch.cat([offset1, offset2, offset3], dim=1)\n",
        "\n",
        "        # mask\n",
        "        mask = torch.sigmoid(mask)\n",
        "\n",
        "        return torchvision.ops.deform_conv2d(x, offset, self.weight, self.bias, self.stride, self.padding,\n",
        "                                         self.dilation, mask)\n",
        "\n",
        "\n",
        "class BasicModule(nn.Module):\n",
        "    \"\"\"Basic Module for SpyNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BasicModule, self).__init__()\n",
        "\n",
        "        self.basic_module = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=7, stride=1, padding=3), nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7, stride=1, padding=3), nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=7, stride=1, padding=3), nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=7, stride=1, padding=3), nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels=16, out_channels=2, kernel_size=7, stride=1, padding=3))\n",
        "\n",
        "    def forward(self, tensor_input):\n",
        "        return self.basic_module(tensor_input)\n",
        "\n",
        "\n",
        "class SpyNet(nn.Module):\n",
        "    \"\"\"SpyNet architecture.\n",
        "\n",
        "    Args:\n",
        "        load_path (str): path for pretrained SpyNet. Default: None.\n",
        "        return_levels (list[int]): return flows of different levels. Default: [5].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, load_path=None, return_levels=[5]):\n",
        "        super(SpyNet, self).__init__()\n",
        "        self.return_levels = return_levels\n",
        "        self.basic_module = nn.ModuleList([BasicModule() for _ in range(6)])\n",
        "        if load_path:\n",
        "            if not os.path.exists(load_path):\n",
        "                import requests\n",
        "                url = 'https://github.com/JingyunLiang/VRT/releases/download/v0.0/spynet_sintel_final-3d2a1287.pth'\n",
        "                r = requests.get(url, allow_redirects=True)\n",
        "                print(f'downloading SpyNet pretrained model from {url}')\n",
        "                os.makedirs(os.path.dirname(load_path), exist_ok=True)\n",
        "                open(load_path, 'wb').write(r.content)\n",
        "\n",
        "            self.load_state_dict(torch.load(load_path, map_location=lambda storage, loc: storage)['params'])\n",
        "\n",
        "        self.register_buffer('mean', torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def preprocess(self, tensor_input):\n",
        "        tensor_output = (tensor_input - self.mean) / self.std\n",
        "        return tensor_output\n",
        "\n",
        "    def process(self, ref, supp, w, h, w_floor, h_floor):\n",
        "        flow_list = []\n",
        "\n",
        "        ref = [self.preprocess(ref)]\n",
        "        supp = [self.preprocess(supp)]\n",
        "\n",
        "        for level in range(5):\n",
        "            ref.insert(0, F.avg_pool2d(input=ref[0], kernel_size=2, stride=2, count_include_pad=False))\n",
        "            supp.insert(0, F.avg_pool2d(input=supp[0], kernel_size=2, stride=2, count_include_pad=False))\n",
        "\n",
        "        flow = ref[0].new_zeros(\n",
        "            [ref[0].size(0), 2,\n",
        "             int(math.floor(ref[0].size(2) / 2.0)),\n",
        "             int(math.floor(ref[0].size(3) / 2.0))])\n",
        "\n",
        "        for level in range(len(ref)):\n",
        "            upsampled_flow = F.interpolate(input=flow, scale_factor=2, mode='bilinear', align_corners=True) * 2.0\n",
        "\n",
        "            if upsampled_flow.size(2) != ref[level].size(2):\n",
        "                upsampled_flow = F.pad(input=upsampled_flow, pad=[0, 0, 0, 1], mode='replicate')\n",
        "            if upsampled_flow.size(3) != ref[level].size(3):\n",
        "                upsampled_flow = F.pad(input=upsampled_flow, pad=[0, 1, 0, 0], mode='replicate')\n",
        "\n",
        "            flow = self.basic_module[level](torch.cat([\n",
        "                ref[level],\n",
        "                flow_warp(\n",
        "                    supp[level], upsampled_flow.permute(0, 2, 3, 1), interp_mode='bilinear', padding_mode='border'),\n",
        "                upsampled_flow\n",
        "            ], 1)) + upsampled_flow\n",
        "\n",
        "            if level in self.return_levels:\n",
        "                scale = 2**(5-level) # level=5 (scale=1), level=4 (scale=2), level=3 (scale=4), level=2 (scale=8)\n",
        "                flow_out = F.interpolate(input=flow, size=(h//scale, w//scale), mode='bilinear', align_corners=False)\n",
        "                flow_out[:, 0, :, :] *= float(w//scale) / float(w_floor//scale)\n",
        "                flow_out[:, 1, :, :] *= float(h//scale) / float(h_floor//scale)\n",
        "                flow_list.insert(0, flow_out)\n",
        "\n",
        "        return flow_list\n",
        "\n",
        "    def forward(self, ref, supp):\n",
        "        assert ref.size() == supp.size()\n",
        "\n",
        "        h, w = ref.size(2), ref.size(3)\n",
        "        w_floor = math.floor(math.ceil(w / 32.0) * 32.0)\n",
        "        h_floor = math.floor(math.ceil(h / 32.0) * 32.0)\n",
        "\n",
        "        ref = F.interpolate(input=ref, size=(h_floor, w_floor), mode='bilinear', align_corners=False)\n",
        "        supp = F.interpolate(input=supp, size=(h_floor, w_floor), mode='bilinear', align_corners=False)\n",
        "\n",
        "        flow_list = self.process(ref, supp, w, h, w_floor, h_floor)\n",
        "\n",
        "        return flow_list[0] if len(flow_list) == 1 else flow_list\n",
        "\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\" Partition the input into windows. Attention will be conducted within the windows.\n",
        "\n",
        "    Args:\n",
        "        x: (B, D, H, W, C)\n",
        "        window_size (tuple[int]): window size\n",
        "\n",
        "    Returns:\n",
        "        windows: (B*num_windows, window_size*window_size, C)\n",
        "    \"\"\"\n",
        "    B, D, H, W, C = x.shape\n",
        "    x = x.view(B, D // window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2],\n",
        "               window_size[2], C)\n",
        "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, reduce(mul, window_size), C)\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, B, D, H, W):\n",
        "    \"\"\" Reverse windows back to the original input. Attention was conducted within the windows.\n",
        "\n",
        "    Args:\n",
        "        windows: (B*num_windows, window_size, window_size, C)\n",
        "        window_size (tuple[int]): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, D, H, W, C)\n",
        "    \"\"\"\n",
        "    x = windows.view(B, D // window_size[0], H // window_size[1], W // window_size[2], window_size[0], window_size[1],\n",
        "                     window_size[2], -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_window_size(x_size, window_size, shift_size=None):\n",
        "    \"\"\" Get the window size and the shift size \"\"\"\n",
        "\n",
        "    use_window_size = list(window_size)\n",
        "    if shift_size is not None:\n",
        "        use_shift_size = list(shift_size)\n",
        "    for i in range(len(x_size)):\n",
        "        if x_size[i] <= window_size[i]:\n",
        "            use_window_size[i] = x_size[i]\n",
        "            if shift_size is not None:\n",
        "                use_shift_size[i] = 0\n",
        "\n",
        "    if shift_size is None:\n",
        "        return tuple(use_window_size)\n",
        "    else:\n",
        "        return tuple(use_window_size), tuple(use_shift_size)\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def compute_mask(D, H, W, window_size, shift_size, device):\n",
        "    \"\"\" Compute attnetion mask for input of size (D, H, W). @lru_cache caches each stage results. \"\"\"\n",
        "\n",
        "    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n",
        "    cnt = 0\n",
        "    for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
        "        for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
        "            for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
        "                img_mask[:, d, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n",
        "    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n",
        "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "\n",
        "    return attn_mask\n",
        "\n",
        "\n",
        "class Upsample(nn.Sequential):\n",
        "    \"\"\"Upsample module for video SR.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat):\n",
        "        assert LooseVersion(torch.__version__) >= LooseVersion('1.8.1'), \\\n",
        "            'PyTorch version >= 1.8.1 to support 5D PixelShuffle.'\n",
        "\n",
        "        class Transpose_Dim12(nn.Module):\n",
        "            \"\"\" Transpose Dim1 and Dim2 of a tensor.\"\"\"\n",
        "\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "\n",
        "            def forward(self, x):\n",
        "                return x.transpose(1, 2)\n",
        "\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                m.append(nn.Conv3d(num_feat, 4 * num_feat, kernel_size=(1, 3, 3), padding=(0, 1, 1)))\n",
        "                m.append(Transpose_Dim12())\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "                m.append(Transpose_Dim12())\n",
        "                m.append(nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "            m.append(nn.Conv3d(num_feat, num_feat, kernel_size=(1, 3, 3), padding=(0, 1, 1)))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv3d(num_feat, 9 * num_feat, kernel_size=(1, 3, 3), padding=(0, 1, 1)))\n",
        "            m.append(Transpose_Dim12())\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "            m.append(Transpose_Dim12())\n",
        "            m.append(nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
        "            m.append(nn.Conv3d(num_feat, num_feat, kernel_size=(1, 3, 3), padding=(0, 1, 1)))\n",
        "        else:\n",
        "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
        "        super(Upsample, self).__init__(*m)\n",
        "\n",
        "\n",
        "class Mlp_GEGLU(nn.Module):\n",
        "    \"\"\" Multilayer perceptron with gated linear unit (GEGLU). Ref. \"GLU Variants Improve Transformer\".\n",
        "\n",
        "    Args:\n",
        "        x: (B, D, H, W, C)\n",
        "\n",
        "    Returns:\n",
        "        x: (B, D, H, W, C)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc11 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc12 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc11(x)) * self.fc12(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\" Window based multi-head mutual attention and self attention.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The temporal length, height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        mut_attn (bool): If True, add mutual attention to the module. Default: True\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=False, qk_scale=None, mut_attn=True):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.mut_attn = mut_attn\n",
        "\n",
        "        # self attention with relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1),\n",
        "                        num_heads))  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n",
        "        self.register_buffer(\"relative_position_index\", self.get_position_index(window_size))\n",
        "        self.qkv_self = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "        # mutual attention with sine position encoding\n",
        "        if self.mut_attn:\n",
        "            self.register_buffer(\"position_bias\",\n",
        "                                 self.get_sine_position_encoding(window_size[1:], dim // 2, normalize=True))\n",
        "            self.qkv_mut = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "            self.proj = nn.Linear(2 * dim, dim)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\" Forward function.\n",
        "\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n",
        "        \"\"\"\n",
        "\n",
        "        # self attention\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv_self(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n",
        "        x_out = self.attention(q, k, v, mask, (B_, N, C), relative_position_encoding=True)\n",
        "\n",
        "        # mutual attention\n",
        "        if self.mut_attn:\n",
        "            qkv = self.qkv_mut(x + self.position_bias.repeat(1, 2, 1)).reshape(B_, N, 3, self.num_heads,\n",
        "                                                                               C // self.num_heads).permute(2, 0, 3, 1,\n",
        "                                                                                                            4)\n",
        "            (q1, q2), (k1, k2), (v1, v2) = torch.chunk(qkv[0], 2, dim=2), torch.chunk(qkv[1], 2, dim=2), torch.chunk(\n",
        "                qkv[2], 2, dim=2)  # B_, nH, N/2, C\n",
        "            x1_aligned = self.attention(q2, k1, v1, mask, (B_, N // 2, C), relative_position_encoding=False)\n",
        "            x2_aligned = self.attention(q1, k2, v2, mask, (B_, N // 2, C), relative_position_encoding=False)\n",
        "            x_out = torch.cat([torch.cat([x1_aligned, x2_aligned], 1), x_out], 2)\n",
        "\n",
        "        # projection\n",
        "        x = self.proj(x_out)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def attention(self, q, k, v, mask, x_shape, relative_position_encoding=True):\n",
        "        B_, N, C = x_shape\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "\n",
        "        if relative_position_encoding:\n",
        "            relative_position_bias = self.relative_position_bias_table[\n",
        "                self.relative_position_index[:N, :N].reshape(-1)].reshape(N, N, -1)  # Wd*Wh*Ww, Wd*Wh*Ww,nH\n",
        "            attn = attn + relative_position_bias.permute(2, 0, 1).unsqueeze(0)  # B_, nH, N, N\n",
        "\n",
        "        if mask is None:\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask[:, :N, :N].unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_position_index(self, window_size):\n",
        "        ''' Get pair-wise relative position index for each token inside the window. '''\n",
        "\n",
        "        coords_d = torch.arange(window_size[0])\n",
        "        coords_h = torch.arange(window_size[1])\n",
        "        coords_w = torch.arange(window_size[2])\n",
        "        coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))  # 3, Wd, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n",
        "        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += window_size[1] - 1\n",
        "        relative_coords[:, :, 2] += window_size[2] - 1\n",
        "\n",
        "        relative_coords[:, :, 0] *= (2 * window_size[1] - 1) * (2 * window_size[2] - 1)\n",
        "        relative_coords[:, :, 1] *= (2 * window_size[2] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n",
        "\n",
        "        return relative_position_index\n",
        "\n",
        "    def get_sine_position_encoding(self, HW, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        \"\"\" Get sine position encoding \"\"\"\n",
        "\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "\n",
        "        not_mask = torch.ones([1, HW[0], HW[1]])\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * scale\n",
        "\n",
        "        dim_t = torch.arange(num_pos_feats, dtype=torch.float32)\n",
        "        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n",
        "\n",
        "        # BxCxHxW\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_embed = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "\n",
        "        return pos_embed.flatten(2).permute(0, 2, 1).contiguous()\n",
        "\n",
        "\n",
        "class TMSA(nn.Module):\n",
        "    \"\"\" Temporal Mutual Self Attention (TMSA).\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (tuple[int]): Window size.\n",
        "        shift_size (tuple[int]): Shift size for mutual and self attention.\n",
        "        mut_attn (bool): If True, use mutual and self attention. Default: True.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True.\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0.\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm.\n",
        "        use_checkpoint_attn (bool): If True, use torch.checkpoint for attention modules. Default: False.\n",
        "        use_checkpoint_ffn (bool): If True, use torch.checkpoint for feed-forward modules. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 input_resolution,\n",
        "                 num_heads,\n",
        "                 window_size=(6, 8, 8),\n",
        "                 shift_size=(0, 0, 0),\n",
        "                 mut_attn=True,\n",
        "                 mlp_ratio=2.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_path=0.,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 use_checkpoint_attn=False,\n",
        "                 use_checkpoint_ffn=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.use_checkpoint_attn = use_checkpoint_attn\n",
        "        self.use_checkpoint_ffn = use_checkpoint_ffn\n",
        "\n",
        "        assert 0 <= self.shift_size[0] < self.window_size[0], \"shift_size must in 0-window_size\"\n",
        "        assert 0 <= self.shift_size[1] < self.window_size[1], \"shift_size must in 0-window_size\"\n",
        "        assert 0 <= self.shift_size[2] < self.window_size[2], \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(dim, window_size=self.window_size, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "                                    qk_scale=qk_scale, mut_attn=mut_attn)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = Mlp_GEGLU(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer)\n",
        "\n",
        "    def forward_part1(self, x, mask_matrix):\n",
        "        B, D, H, W, C = x.shape\n",
        "        window_size, shift_size = get_window_size((D, H, W), self.window_size, self.shift_size)\n",
        "\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # pad feature maps to multiples of window size\n",
        "        pad_l = pad_t = pad_d0 = 0\n",
        "        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n",
        "        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n",
        "        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]\n",
        "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1), mode='constant')\n",
        "\n",
        "        _, Dp, Hp, Wp, _ = x.shape\n",
        "        # cyclic shift\n",
        "        if any(i > 0 for i in shift_size):\n",
        "            shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
        "            attn_mask = mask_matrix\n",
        "        else:\n",
        "            shifted_x = x\n",
        "            attn_mask = None\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, window_size)  # B*nW, Wd*Wh*Ww, C\n",
        "\n",
        "        # attention / shifted attention\n",
        "        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, *(window_size + (C,)))\n",
        "        shifted_x = window_reverse(attn_windows, window_size, B, Dp, Hp, Wp)  # B D' H' W' C\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if any(i > 0 for i in shift_size):\n",
        "            x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
        "            x = x[:, :D, :H, :W, :]\n",
        "\n",
        "        x = self.drop_path(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_part2(self, x):\n",
        "        return self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "    def forward(self, x, mask_matrix):\n",
        "        \"\"\" Forward function.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature, tensor size (B, D, H, W, C).\n",
        "            mask_matrix: Attention mask for cyclic shift.\n",
        "        \"\"\"\n",
        "\n",
        "        # attention\n",
        "        if self.use_checkpoint_attn:\n",
        "            x = x + checkpoint.checkpoint(self.forward_part1, x, mask_matrix)\n",
        "        else:\n",
        "            x = x + self.forward_part1(x, mask_matrix)\n",
        "\n",
        "        # feed-forward\n",
        "        if self.use_checkpoint_ffn:\n",
        "            x = x + checkpoint.checkpoint(self.forward_part2, x)\n",
        "        else:\n",
        "            x = x + self.forward_part2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TMSAG(nn.Module):\n",
        "    \"\"\" Temporal Mutual Self Attention Group (TMSAG).\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of feature channels\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Depths of this stage.\n",
        "        num_heads (int): Number of attention head.\n",
        "        window_size (tuple[int]): Local window size. Default: (6,8,8).\n",
        "        shift_size (tuple[int]): Shift size for mutual and self attention. Default: None.\n",
        "        mut_attn (bool): If True, use mutual and self attention. Default: True.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 2.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        use_checkpoint_attn (bool): If True, use torch.checkpoint for attention modules. Default: False.\n",
        "        use_checkpoint_ffn (bool): If True, use torch.checkpoint for feed-forward modules. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 input_resolution,\n",
        "                 depth,\n",
        "                 num_heads,\n",
        "                 window_size=[6, 8, 8],\n",
        "                 shift_size=None,\n",
        "                 mut_attn=True,\n",
        "                 mlp_ratio=2.,\n",
        "                 qkv_bias=False,\n",
        "                 qk_scale=None,\n",
        "                 drop_path=0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 use_checkpoint_attn=False,\n",
        "                 use_checkpoint_ffn=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = list(i // 2 for i in window_size) if shift_size is None else shift_size\n",
        "\n",
        "        # build blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TMSA(\n",
        "                dim=dim,\n",
        "                input_resolution=input_resolution,\n",
        "                num_heads=num_heads,\n",
        "                window_size=window_size,\n",
        "                shift_size=[0, 0, 0] if i % 2 == 0 else self.shift_size,\n",
        "                mut_attn=mut_attn,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer,\n",
        "                use_checkpoint_attn=use_checkpoint_attn,\n",
        "                use_checkpoint_ffn=use_checkpoint_ffn\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward function.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature, tensor size (B, C, D, H, W).\n",
        "        \"\"\"\n",
        "        # calculate attention mask for attention\n",
        "        B, C, D, H, W = x.shape\n",
        "        window_size, shift_size = get_window_size((D, H, W), self.window_size, self.shift_size)\n",
        "        x = rearrange(x, 'b c d h w -> b d h w c')\n",
        "        Dp = int(np.ceil(D / window_size[0])) * window_size[0]\n",
        "        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n",
        "        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n",
        "        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size, x.device)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, attn_mask)\n",
        "\n",
        "        x = x.view(B, D, H, W, -1)\n",
        "        x = rearrange(x, 'b d h w c -> b c d h w')\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class RTMSA(nn.Module):\n",
        "    \"\"\" Residual Temporal Mutual Self Attention (RTMSA). Only used in stage 8.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True.\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm.\n",
        "        use_checkpoint_attn (bool): If True, use torch.checkpoint for attention modules. Default: False.\n",
        "        use_checkpoint_ffn (bool): If True, use torch.checkpoint for feed-forward modules. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 input_resolution,\n",
        "                 depth,\n",
        "                 num_heads,\n",
        "                 window_size,\n",
        "                 mlp_ratio=2.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_path=0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 use_checkpoint_attn=False,\n",
        "                 use_checkpoint_ffn=None\n",
        "                 ):\n",
        "        super(RTMSA, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "\n",
        "        self.residual_group = TMSAG(dim=dim,\n",
        "                                    input_resolution=input_resolution,\n",
        "                                    depth=depth,\n",
        "                                    num_heads=num_heads,\n",
        "                                    window_size=window_size,\n",
        "                                    mut_attn=False,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                    drop_path=drop_path,\n",
        "                                    norm_layer=norm_layer,\n",
        "                                    use_checkpoint_attn=use_checkpoint_attn,\n",
        "                                    use_checkpoint_ffn=use_checkpoint_ffn\n",
        "                                    )\n",
        "\n",
        "        self.linear = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.linear(self.residual_group(x).transpose(1, 4)).transpose(1, 4)\n",
        "\n",
        "\n",
        "class Stage(nn.Module):\n",
        "    \"\"\"Residual Temporal Mutual Self Attention Group and Parallel Warping.\n",
        "\n",
        "    Args:\n",
        "        in_dim (int): Number of input channels.\n",
        "        dim (int): Number of channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        mul_attn_ratio (float): Ratio of mutual attention layers. Default: 0.75.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        pa_frames (float): Number of warpped frames. Default: 2.\n",
        "        deformable_groups (float): Number of deformable groups. Default: 16.\n",
        "        reshape (str): Downscale (down), upscale (up) or keep the size (none).\n",
        "        max_residue_magnitude (float): Maximum magnitude of the residual of optical flow.\n",
        "        use_checkpoint_attn (bool): If True, use torch.checkpoint for attention modules. Default: False.\n",
        "        use_checkpoint_ffn (bool): If True, use torch.checkpoint for feed-forward modules. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 dim,\n",
        "                 input_resolution,\n",
        "                 depth,\n",
        "                 num_heads,\n",
        "                 window_size,\n",
        "                 mul_attn_ratio=0.75,\n",
        "                 mlp_ratio=2.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_path=0.,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 pa_frames=2,\n",
        "                 deformable_groups=16,\n",
        "                 reshape=None,\n",
        "                 max_residue_magnitude=10,\n",
        "                 use_checkpoint_attn=False,\n",
        "                 use_checkpoint_ffn=False\n",
        "                 ):\n",
        "        super(Stage, self).__init__()\n",
        "        self.pa_frames = pa_frames\n",
        "\n",
        "        # reshape the tensor\n",
        "        if reshape == 'none':\n",
        "            self.reshape = nn.Sequential(Rearrange('n c d h w -> n d h w c'),\n",
        "                                         nn.LayerNorm(dim),\n",
        "                                         Rearrange('n d h w c -> n c d h w'))\n",
        "        elif reshape == 'down':\n",
        "            self.reshape = nn.Sequential(Rearrange('n c d (h neih) (w neiw) -> n d h w (neiw neih c)', neih=2, neiw=2),\n",
        "                                         nn.LayerNorm(4 * in_dim), nn.Linear(4 * in_dim, dim),\n",
        "                                         Rearrange('n d h w c -> n c d h w'))\n",
        "        elif reshape == 'up':\n",
        "            self.reshape = nn.Sequential(Rearrange('n (neiw neih c) d h w -> n d (h neih) (w neiw) c', neih=2, neiw=2),\n",
        "                                         nn.LayerNorm(in_dim // 4), nn.Linear(in_dim // 4, dim),\n",
        "                                         Rearrange('n d h w c -> n c d h w'))\n",
        "\n",
        "        # mutual and self attention\n",
        "        self.residual_group1 = TMSAG(dim=dim,\n",
        "                                     input_resolution=input_resolution,\n",
        "                                     depth=int(depth * mul_attn_ratio),\n",
        "                                     num_heads=num_heads,\n",
        "                                     window_size=(2, window_size[1], window_size[2]),\n",
        "                                     mut_attn=True,\n",
        "                                     mlp_ratio=mlp_ratio,\n",
        "                                     qkv_bias=qkv_bias,\n",
        "                                     qk_scale=qk_scale,\n",
        "                                     drop_path=drop_path,\n",
        "                                     norm_layer=norm_layer,\n",
        "                                     use_checkpoint_attn=use_checkpoint_attn,\n",
        "                                     use_checkpoint_ffn=use_checkpoint_ffn\n",
        "                                     )\n",
        "        self.linear1 = nn.Linear(dim, dim)\n",
        "\n",
        "        # only self attention\n",
        "        self.residual_group2 = TMSAG(dim=dim,\n",
        "                                     input_resolution=input_resolution,\n",
        "                                     depth=depth - int(depth * mul_attn_ratio),\n",
        "                                     num_heads=num_heads,\n",
        "                                     window_size=window_size,\n",
        "                                     mut_attn=False,\n",
        "                                     mlp_ratio=mlp_ratio,\n",
        "                                     qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                     drop_path=drop_path,\n",
        "                                     norm_layer=norm_layer,\n",
        "                                     use_checkpoint_attn=True,\n",
        "                                     use_checkpoint_ffn=use_checkpoint_ffn\n",
        "                                     )\n",
        "        self.linear2 = nn.Linear(dim, dim)\n",
        "\n",
        "        # parallel warping\n",
        "        if self.pa_frames:\n",
        "            self.pa_deform = DCNv2PackFlowGuided(dim, dim, 3, padding=1, deformable_groups=deformable_groups,\n",
        "                                                 max_residue_magnitude=max_residue_magnitude, pa_frames=pa_frames)\n",
        "            self.pa_fuse = Mlp_GEGLU(dim * (1 + 2), dim * (1 + 2), dim)\n",
        "\n",
        "    def forward(self, x, flows_backward, flows_forward):\n",
        "        x = self.reshape(x)\n",
        "        x = self.linear1(self.residual_group1(x).transpose(1, 4)).transpose(1, 4) + x\n",
        "        x = self.linear2(self.residual_group2(x).transpose(1, 4)).transpose(1, 4) + x\n",
        "\n",
        "        if self.pa_frames:\n",
        "            x = x.transpose(1, 2)\n",
        "            x_backward, x_forward = getattr(self, f'get_aligned_feature_{self.pa_frames}frames')(x, flows_backward, flows_forward)\n",
        "            x = self.pa_fuse(torch.cat([x, x_backward, x_forward], 2).permute(0, 1, 3, 4, 2)).permute(0, 4, 1, 2, 3)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_aligned_feature_2frames(self, x, flows_backward, flows_forward):\n",
        "        '''Parallel feature warping for 2 frames.'''\n",
        "\n",
        "        # backward\n",
        "        n = x.size(1)\n",
        "        x_backward = [torch.zeros_like(x[:, -1, ...])]\n",
        "        for i in range(n - 1, 0, -1):\n",
        "            x_i = x[:, i, ...]\n",
        "            flow = flows_backward[0][:, i - 1, ...]\n",
        "            x_i_warped = flow_warp(x_i, flow.permute(0, 2, 3, 1), 'bilinear')  # frame i+1 aligned towards i\n",
        "            x_backward.insert(0, self.pa_deform(x_i, [x_i_warped], x[:, i - 1, ...], [flow]))\n",
        "\n",
        "        # forward\n",
        "        x_forward = [torch.zeros_like(x[:, 0, ...])]\n",
        "        for i in range(0, n - 1):\n",
        "            x_i = x[:, i, ...]\n",
        "            flow = flows_forward[0][:, i, ...]\n",
        "            x_i_warped = flow_warp(x_i, flow.permute(0, 2, 3, 1), 'bilinear')  # frame i-1 aligned towards i\n",
        "            x_forward.append(self.pa_deform(x_i, [x_i_warped], x[:, i + 1, ...], [flow]))\n",
        "\n",
        "        return [torch.stack(x_backward, 1), torch.stack(x_forward, 1)]\n",
        "\n",
        "    def get_aligned_feature_4frames(self, x, flows_backward, flows_forward):\n",
        "        '''Parallel feature warping for 4 frames.'''\n",
        "\n",
        "        # backward\n",
        "        n = x.size(1)\n",
        "        x_backward = [torch.zeros_like(x[:, -1, ...])]\n",
        "        for i in range(n, 1, -1):\n",
        "            x_i = x[:, i - 1, ...]\n",
        "            flow1 = flows_backward[0][:, i - 2, ...]\n",
        "            if i == n:\n",
        "                x_ii = torch.zeros_like(x[:, n - 2, ...])\n",
        "                flow2 = torch.zeros_like(flows_backward[1][:, n - 3, ...])\n",
        "            else:\n",
        "                x_ii = x[:, i, ...]\n",
        "                flow2 = flows_backward[1][:, i - 2, ...]\n",
        "\n",
        "            x_i_warped = flow_warp(x_i, flow1.permute(0, 2, 3, 1), 'bilinear')  # frame i+1 aligned towards i\n",
        "            x_ii_warped = flow_warp(x_ii, flow2.permute(0, 2, 3, 1), 'bilinear')  # frame i+2 aligned towards i\n",
        "            x_backward.insert(0,\n",
        "                self.pa_deform(torch.cat([x_i, x_ii], 1), [x_i_warped, x_ii_warped], x[:, i - 2, ...], [flow1, flow2]))\n",
        "\n",
        "        # forward\n",
        "        x_forward = [torch.zeros_like(x[:, 0, ...])]\n",
        "        for i in range(-1, n - 2):\n",
        "            x_i = x[:, i + 1, ...]\n",
        "            flow1 = flows_forward[0][:, i + 1, ...]\n",
        "            if i == -1:\n",
        "                x_ii = torch.zeros_like(x[:, 1, ...])\n",
        "                flow2 = torch.zeros_like(flows_forward[1][:, 0, ...])\n",
        "            else:\n",
        "                x_ii = x[:, i, ...]\n",
        "                flow2 = flows_forward[1][:, i, ...]\n",
        "\n",
        "            x_i_warped = flow_warp(x_i, flow1.permute(0, 2, 3, 1), 'bilinear')  # frame i-1 aligned towards i\n",
        "            x_ii_warped = flow_warp(x_ii, flow2.permute(0, 2, 3, 1), 'bilinear')  # frame i-2 aligned towards i\n",
        "            x_forward.append(\n",
        "                self.pa_deform(torch.cat([x_i, x_ii], 1), [x_i_warped, x_ii_warped], x[:, i + 2, ...], [flow1, flow2]))\n",
        "\n",
        "        return [torch.stack(x_backward, 1), torch.stack(x_forward, 1)]\n",
        "\n",
        "    def get_aligned_feature_6frames(self, x, flows_backward, flows_forward):\n",
        "        '''Parallel feature warping for 6 frames.'''\n",
        "\n",
        "        # backward\n",
        "        n = x.size(1)\n",
        "        x_backward = [torch.zeros_like(x[:, -1, ...])]\n",
        "        for i in range(n + 1, 2, -1):\n",
        "            x_i = x[:, i - 2, ...]\n",
        "            flow1 = flows_backward[0][:, i - 3, ...]\n",
        "            if i == n + 1:\n",
        "                x_ii = torch.zeros_like(x[:, -1, ...])\n",
        "                flow2 = torch.zeros_like(flows_backward[1][:, -1, ...])\n",
        "                x_iii = torch.zeros_like(x[:, -1, ...])\n",
        "                flow3 = torch.zeros_like(flows_backward[2][:, -1, ...])\n",
        "            elif i == n:\n",
        "                x_ii = x[:, i - 1, ...]\n",
        "                flow2 = flows_backward[1][:, i - 3, ...]\n",
        "                x_iii = torch.zeros_like(x[:, -1, ...])\n",
        "                flow3 = torch.zeros_like(flows_backward[2][:, -1, ...])\n",
        "            else:\n",
        "                x_ii = x[:, i - 1, ...]\n",
        "                flow2 = flows_backward[1][:, i - 3, ...]\n",
        "                x_iii = x[:, i, ...]\n",
        "                flow3 = flows_backward[2][:, i - 3, ...]\n",
        "\n",
        "            x_i_warped = flow_warp(x_i, flow1.permute(0, 2, 3, 1), 'bilinear')  # frame i+1 aligned towards i\n",
        "            x_ii_warped = flow_warp(x_ii, flow2.permute(0, 2, 3, 1), 'bilinear')  # frame i+2 aligned towards i\n",
        "            x_iii_warped = flow_warp(x_iii, flow3.permute(0, 2, 3, 1), 'bilinear')  # frame i+3 aligned towards i\n",
        "            x_backward.insert(0,\n",
        "                              self.pa_deform(torch.cat([x_i, x_ii, x_iii], 1), [x_i_warped, x_ii_warped, x_iii_warped],\n",
        "                                             x[:, i - 3, ...], [flow1, flow2, flow3]))\n",
        "\n",
        "        # forward\n",
        "        x_forward = [torch.zeros_like(x[:, 0, ...])]\n",
        "        for i in range(0, n - 1):\n",
        "            x_i = x[:, i, ...]\n",
        "            flow1 = flows_forward[0][:, i, ...]\n",
        "            if i == 0:\n",
        "                x_ii = torch.zeros_like(x[:, 0, ...])\n",
        "                flow2 = torch.zeros_like(flows_forward[1][:, 0, ...])\n",
        "                x_iii = torch.zeros_like(x[:, 0, ...])\n",
        "                flow3 = torch.zeros_like(flows_forward[2][:, 0, ...])\n",
        "            elif i == 1:\n",
        "                x_ii = x[:, i - 1, ...]\n",
        "                flow2 = flows_forward[1][:, i - 1, ...]\n",
        "                x_iii = torch.zeros_like(x[:, 0, ...])\n",
        "                flow3 = torch.zeros_like(flows_forward[2][:, 0, ...])\n",
        "            else:\n",
        "                x_ii = x[:, i - 1, ...]\n",
        "                flow2 = flows_forward[1][:, i - 1, ...]\n",
        "                x_iii = x[:, i - 2, ...]\n",
        "                flow3 = flows_forward[2][:, i - 2, ...]\n",
        "\n",
        "            x_i_warped = flow_warp(x_i, flow1.permute(0, 2, 3, 1), 'bilinear')  # frame i-1 aligned towards i\n",
        "            x_ii_warped = flow_warp(x_ii, flow2.permute(0, 2, 3, 1), 'bilinear')  # frame i-2 aligned towards i\n",
        "            x_iii_warped = flow_warp(x_iii, flow3.permute(0, 2, 3, 1), 'bilinear')  # frame i-3 aligned towards i\n",
        "            x_forward.append(self.pa_deform(torch.cat([x_i, x_ii, x_iii], 1), [x_i_warped, x_ii_warped, x_iii_warped],\n",
        "                                            x[:, i + 1, ...], [flow1, flow2, flow3]))\n",
        "\n",
        "        return [torch.stack(x_backward, 1), torch.stack(x_forward, 1)]\n",
        "\n",
        "\n",
        "class VRT(nn.Module):\n",
        "    \"\"\" Video Restoration Transformer (VRT).\n",
        "        A PyTorch impl of : `VRT: A Video Restoration Transformer`  -\n",
        "          https://arxiv.org/pdf/2201.00000\n",
        "\n",
        "    Args:\n",
        "        upscale (int): Upscaling factor. Set as 1 for video deblurring, etc. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        out_chans (int): Number of output image channels. Default: 3.\n",
        "        img_size (int | tuple(int)): Size of input image. Default: [6, 64, 64].\n",
        "        window_size (int | tuple(int)): Window size. Default: (6,8,8).\n",
        "        depths (list[int]): Depths of each Transformer stage.\n",
        "        indep_reconsts (list[int]): Layers that extract features of different frames independently.\n",
        "        embed_dims (list[int]): Number of linear projection output channels.\n",
        "        num_heads (list[int]): Number of attention head of each stage.\n",
        "        mul_attn_ratio (float): Ratio of mutual attention layers. Default: 0.75.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 2.\n",
        "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True.\n",
        "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
        "        norm_layer (obj): Normalization layer. Default: nn.LayerNorm.\n",
        "        spynet_path (str): Pretrained SpyNet model path.\n",
        "        pa_frames (float): Number of warpped frames. Default: 2.\n",
        "        deformable_groups (float): Number of deformable groups. Default: 16.\n",
        "        recal_all_flows (bool): If True, derive (t,t+2) and (t,t+3) flows from (t,t+1). Default: False.\n",
        "        nonblind_denoising (bool): If True, conduct experiments on non-blind denoising. Default: False.\n",
        "        use_checkpoint_attn (bool): If True, use torch.checkpoint for attention modules. Default: False.\n",
        "        use_checkpoint_ffn (bool): If True, use torch.checkpoint for feed-forward modules. Default: False.\n",
        "        no_checkpoint_attn_blocks (list[int]): Layers without torch.checkpoint for attention modules.\n",
        "        no_checkpoint_ffn_blocks (list[int]): Layers without torch.checkpoint for feed-forward modules.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 upscale=4,\n",
        "                 in_chans=3,\n",
        "                 out_chans=3,\n",
        "                 img_size=[6, 64, 64],\n",
        "                 window_size=[6, 8, 8],\n",
        "                 depths=[8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4],\n",
        "                 indep_reconsts=[11, 12],\n",
        "                 embed_dims=[120, 120, 120, 120, 120, 120, 120, 180, 180, 180, 180, 180, 180],\n",
        "                 num_heads=[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
        "                 mul_attn_ratio=0.75,\n",
        "                 mlp_ratio=2.,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 drop_path_rate=0.2,\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 spynet_path=None,\n",
        "                 pa_frames=2,\n",
        "                 deformable_groups=16,\n",
        "                 recal_all_flows=False,\n",
        "                 nonblind_denoising=False,\n",
        "                 use_checkpoint_attn=False,\n",
        "                 use_checkpoint_ffn=False,\n",
        "                 no_checkpoint_attn_blocks=[],\n",
        "                 no_checkpoint_ffn_blocks=[],\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.in_chans = in_chans\n",
        "        self.out_chans = out_chans\n",
        "        self.upscale = upscale\n",
        "        self.pa_frames = pa_frames\n",
        "        self.recal_all_flows = recal_all_flows\n",
        "        self.nonblind_denoising = nonblind_denoising\n",
        "\n",
        "        # conv_first\n",
        "        if self.pa_frames:\n",
        "            if self.nonblind_denoising:\n",
        "                conv_first_in_chans = in_chans*(1+2*4)+1\n",
        "            else:\n",
        "                conv_first_in_chans = in_chans*(1+2*4)\n",
        "        else:\n",
        "            conv_first_in_chans = in_chans\n",
        "        self.conv_first = nn.Conv3d(conv_first_in_chans, embed_dims[0], kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "\n",
        "        # main body\n",
        "        if self.pa_frames:\n",
        "            self.spynet = SpyNet(spynet_path, [2, 3, 4, 5])\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "        reshapes = ['none', 'down', 'down', 'down', 'up', 'up', 'up']\n",
        "        scales = [1, 2, 4, 8, 4, 2, 1]\n",
        "        use_checkpoint_attns = [False if i in no_checkpoint_attn_blocks else use_checkpoint_attn for i in\n",
        "                                range(len(depths))]\n",
        "        use_checkpoint_ffns = [False if i in no_checkpoint_ffn_blocks else use_checkpoint_ffn for i in\n",
        "                               range(len(depths))]\n",
        "\n",
        "        # stage 1- 7\n",
        "        for i in range(7):\n",
        "            setattr(self, f'stage{i + 1}',\n",
        "                    Stage(\n",
        "                        in_dim=embed_dims[i - 1],\n",
        "                        dim=embed_dims[i],\n",
        "                        input_resolution=(img_size[0], img_size[1] // scales[i], img_size[2] // scales[i]),\n",
        "                        depth=depths[i],\n",
        "                        num_heads=num_heads[i],\n",
        "                        mul_attn_ratio=mul_attn_ratio,\n",
        "                        window_size=window_size,\n",
        "                        mlp_ratio=mlp_ratio,\n",
        "                        qkv_bias=qkv_bias,\n",
        "                        qk_scale=qk_scale,\n",
        "                        drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
        "                        norm_layer=norm_layer,\n",
        "                        pa_frames=pa_frames,\n",
        "                        deformable_groups=deformable_groups,\n",
        "                        reshape=reshapes[i],\n",
        "                        max_residue_magnitude=10 / scales[i],\n",
        "                        use_checkpoint_attn=use_checkpoint_attns[i],\n",
        "                        use_checkpoint_ffn=use_checkpoint_ffns[i],\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "        # stage 8\n",
        "        self.stage8 = nn.ModuleList(\n",
        "            [nn.Sequential(\n",
        "                Rearrange('n c d h w ->  n d h w c'),\n",
        "                nn.LayerNorm(embed_dims[6]),\n",
        "                nn.Linear(embed_dims[6], embed_dims[7]),\n",
        "                Rearrange('n d h w c -> n c d h w')\n",
        "            )]\n",
        "        )\n",
        "        for i in range(7, len(depths)):\n",
        "            self.stage8.append(\n",
        "                RTMSA(dim=embed_dims[i],\n",
        "                      input_resolution=img_size,\n",
        "                      depth=depths[i],\n",
        "                      num_heads=num_heads[i],\n",
        "                      window_size=[1, window_size[1], window_size[2]] if i in indep_reconsts else window_size,\n",
        "                      mlp_ratio=mlp_ratio,\n",
        "                      qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                      drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
        "                      norm_layer=norm_layer,\n",
        "                      use_checkpoint_attn=use_checkpoint_attns[i],\n",
        "                      use_checkpoint_ffn=use_checkpoint_ffns[i]\n",
        "                      )\n",
        "            )\n",
        "\n",
        "        self.norm = norm_layer(embed_dims[-1])\n",
        "        self.conv_after_body = nn.Linear(embed_dims[-1], embed_dims[0])\n",
        "\n",
        "        # reconstruction\n",
        "        if self.pa_frames:\n",
        "            if self.upscale == 1:\n",
        "                # for video deblurring, etc.\n",
        "                self.conv_last = nn.Conv3d(embed_dims[0], out_chans, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "            else:\n",
        "                # for video sr\n",
        "                num_feat = 64\n",
        "                self.conv_before_upsample = nn.Sequential(\n",
        "                    nn.Conv3d(embed_dims[0], num_feat, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
        "                    nn.LeakyReLU(inplace=True))\n",
        "                self.upsample = Upsample(upscale, num_feat)\n",
        "                self.conv_last = nn.Conv3d(num_feat, out_chans, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "        else:\n",
        "            num_feat = 64\n",
        "            self.linear_fuse = nn.Conv2d(embed_dims[0]*img_size[0], num_feat, kernel_size=1 , stride=1)\n",
        "            self.conv_last = nn.Conv2d(num_feat, out_chans , kernel_size=7 , stride=1, padding=0)\n",
        "\n",
        "    def init_weights(self, pretrained=None, strict=True):\n",
        "        \"\"\"Init weights for models.\n",
        "\n",
        "        Args:\n",
        "            pretrained (str, optional): Path for pretrained weights. If given\n",
        "                None, pretrained weights will not be loaded. Defaults: None.\n",
        "            strict (boo, optional): Whether strictly load the pretrained model.\n",
        "                Defaults to True.\n",
        "        \"\"\"\n",
        "        if isinstance(pretrained, str):\n",
        "            logger = get_root_logger()\n",
        "            load_checkpoint(self, pretrained, strict=strict, logger=logger)\n",
        "        elif pretrained is not None:\n",
        "            raise TypeError(f'\"pretrained\" must be a str or None. '\n",
        "                            f'But received {type(pretrained)}.')\n",
        "\n",
        "    def reflection_pad2d(self, x, pad=1):\n",
        "        \"\"\" Reflection padding for any dtypes (torch.bfloat16.\n",
        "\n",
        "        Args:\n",
        "            x: (tensor): BxCxHxW\n",
        "            pad: (int): Default: 1.\n",
        "        \"\"\"\n",
        "\n",
        "        x = torch.cat([torch.flip(x[:, :, 1:pad+1, :], [2]), x, torch.flip(x[:, :, -pad-1:-1, :], [2])], 2)\n",
        "        x = torch.cat([torch.flip(x[:, :, :, 1:pad+1], [3]), x, torch.flip(x[:, :, :, -pad-1:-1], [3])], 3)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, D, C, H, W)\n",
        "\n",
        "        # main network\n",
        "        if self.pa_frames:\n",
        "            # obtain noise level map\n",
        "            if self.nonblind_denoising:\n",
        "                x, noise_level_map = x[:, :, :self.in_chans, :, :], x[:, :, self.in_chans:, :, :]\n",
        "\n",
        "            x_lq = x.clone()\n",
        "\n",
        "            # calculate flows\n",
        "            flows_backward, flows_forward = self.get_flows(x)\n",
        "\n",
        "            # warp input\n",
        "            x_backward, x_forward = self.get_aligned_image_2frames(x,  flows_backward[0], flows_forward[0])\n",
        "            x = torch.cat([x, x_backward, x_forward], 2)\n",
        "\n",
        "            # concatenate noise level map\n",
        "            if self.nonblind_denoising:\n",
        "                x = torch.cat([x, noise_level_map], 2)\n",
        "\n",
        "            if self.upscale == 1:\n",
        "                # video deblurring, etc.\n",
        "                x = self.conv_first(x.transpose(1, 2))\n",
        "                x = x + self.conv_after_body(\n",
        "                    self.forward_features(x, flows_backward, flows_forward).transpose(1, 4)).transpose(1, 4)\n",
        "                x = self.conv_last(x).transpose(1, 2)\n",
        "                return x + x_lq\n",
        "            else:\n",
        "                # video sr\n",
        "                x = self.conv_first(x.transpose(1, 2))\n",
        "                x = x + self.conv_after_body(\n",
        "                    self.forward_features(x, flows_backward, flows_forward).transpose(1, 4)).transpose(1, 4)\n",
        "                x = self.conv_last(self.upsample(self.conv_before_upsample(x))).transpose(1, 2)\n",
        "                _, _, C, H, W = x.shape\n",
        "                return x + torch.nn.functional.interpolate(x_lq, size=(C, H, W), mode='trilinear', align_corners=False)\n",
        "        else:\n",
        "            # video fi\n",
        "            x_mean = x.mean([1,3,4], keepdim=True)\n",
        "            x = x - x_mean\n",
        "\n",
        "            x = self.conv_first(x.transpose(1, 2))\n",
        "            x = x + self.conv_after_body(\n",
        "                self.forward_features(x, [], []).transpose(1, 4)).transpose(1, 4)\n",
        "\n",
        "            x = torch.cat(torch.unbind(x , 2) , 1)\n",
        "            x = self.conv_last(self.reflection_pad2d(F.leaky_relu(self.linear_fuse(x), 0.2), pad=3))\n",
        "            x = torch.stack(torch.split(x, dim=1, split_size_or_sections=3), 1)\n",
        "\n",
        "            return x + x_mean\n",
        "\n",
        "\n",
        "    def get_flows(self, x):\n",
        "        ''' Get flows for 2 frames, 4 frames or 6 frames.'''\n",
        "\n",
        "        if self.pa_frames == 2:\n",
        "            flows_backward, flows_forward = self.get_flow_2frames(x)\n",
        "        elif self.pa_frames == 4:\n",
        "            flows_backward_2frames, flows_forward_2frames = self.get_flow_2frames(x)\n",
        "            flows_backward_4frames, flows_forward_4frames = self.get_flow_4frames(flows_forward_2frames, flows_backward_2frames)\n",
        "            flows_backward = flows_backward_2frames + flows_backward_4frames\n",
        "            flows_forward = flows_forward_2frames + flows_forward_4frames\n",
        "        elif self.pa_frames == 6:\n",
        "            flows_backward_2frames, flows_forward_2frames = self.get_flow_2frames(x)\n",
        "            flows_backward_4frames, flows_forward_4frames = self.get_flow_4frames(flows_forward_2frames, flows_backward_2frames)\n",
        "            flows_backward_6frames, flows_forward_6frames = self.get_flow_6frames(flows_forward_2frames, flows_backward_2frames, flows_forward_4frames, flows_backward_4frames)\n",
        "            flows_backward = flows_backward_2frames + flows_backward_4frames + flows_backward_6frames\n",
        "            flows_forward = flows_forward_2frames + flows_forward_4frames + flows_forward_6frames\n",
        "\n",
        "        return flows_backward, flows_forward\n",
        "\n",
        "    def get_flow_2frames(self, x):\n",
        "        '''Get flow between frames t and t+1 from x.'''\n",
        "\n",
        "        b, n, c, h, w = x.size()\n",
        "        x_1 = x[:, :-1, :, :, :].reshape(-1, c, h, w)\n",
        "        x_2 = x[:, 1:, :, :, :].reshape(-1, c, h, w)\n",
        "\n",
        "        # backward\n",
        "        flows_backward = self.spynet(x_1, x_2)\n",
        "        flows_backward = [flow.view(b, n-1, 2, h // (2 ** i), w // (2 ** i)) for flow, i in\n",
        "                          zip(flows_backward, range(4))]\n",
        "\n",
        "        # forward\n",
        "        flows_forward = self.spynet(x_2, x_1)\n",
        "        flows_forward = [flow.view(b, n-1, 2, h // (2 ** i), w // (2 ** i)) for flow, i in\n",
        "                         zip(flows_forward, range(4))]\n",
        "\n",
        "        return flows_backward, flows_forward\n",
        "\n",
        "    def get_flow_4frames(self, flows_forward, flows_backward):\n",
        "        '''Get flow between t and t+2 from (t,t+1) and (t+1,t+2).'''\n",
        "\n",
        "        # backward\n",
        "        d = flows_forward[0].shape[1]\n",
        "        flows_backward2 = []\n",
        "        for flows in flows_backward:\n",
        "            flow_list = []\n",
        "            for i in range(d - 1, 0, -1):\n",
        "                flow_n1 = flows[:, i - 1, :, :, :]  # flow from i+1 to i\n",
        "                flow_n2 = flows[:, i, :, :, :]  # flow from i+2 to i+1\n",
        "                flow_list.insert(0, flow_n1 + flow_warp(flow_n2, flow_n1.permute(0, 2, 3, 1)))  # flow from i+2 to i\n",
        "            flows_backward2.append(torch.stack(flow_list, 1))\n",
        "\n",
        "        # forward\n",
        "        flows_forward2 = []\n",
        "        for flows in flows_forward:\n",
        "            flow_list = []\n",
        "            for i in range(1, d):\n",
        "                flow_n1 = flows[:, i, :, :, :]  # flow from i-1 to i\n",
        "                flow_n2 = flows[:, i - 1, :, :, :]  # flow from i-2 to i-1\n",
        "                flow_list.append(flow_n1 + flow_warp(flow_n2, flow_n1.permute(0, 2, 3, 1)))  # flow from i-2 to i\n",
        "            flows_forward2.append(torch.stack(flow_list, 1))\n",
        "\n",
        "        return flows_backward2, flows_forward2\n",
        "\n",
        "    def get_flow_6frames(self, flows_forward, flows_backward, flows_forward2, flows_backward2):\n",
        "        '''Get flow between t and t+3 from (t,t+2) and (t+2,t+3).'''\n",
        "\n",
        "        # backward\n",
        "        d = flows_forward2[0].shape[1]\n",
        "        flows_backward3 = []\n",
        "        for flows, flows2 in zip(flows_backward, flows_backward2):\n",
        "            flow_list = []\n",
        "            for i in range(d - 1, 0, -1):\n",
        "                flow_n1 = flows2[:, i - 1, :, :, :]  # flow from i+2 to i\n",
        "                flow_n2 = flows[:, i + 1, :, :, :]  # flow from i+3 to i+2\n",
        "                flow_list.insert(0, flow_n1 + flow_warp(flow_n2, flow_n1.permute(0, 2, 3, 1)))  # flow from i+3 to i\n",
        "            flows_backward3.append(torch.stack(flow_list, 1))\n",
        "\n",
        "        # forward\n",
        "        flows_forward3 = []\n",
        "        for flows, flows2 in zip(flows_forward, flows_forward2):\n",
        "            flow_list = []\n",
        "            for i in range(2, d + 1):\n",
        "                flow_n1 = flows2[:, i - 1, :, :, :]  # flow from i-2 to i\n",
        "                flow_n2 = flows[:, i - 2, :, :, :]  # flow from i-3 to i-2\n",
        "                flow_list.append(flow_n1 + flow_warp(flow_n2, flow_n1.permute(0, 2, 3, 1)))  # flow from i-3 to i\n",
        "            flows_forward3.append(torch.stack(flow_list, 1))\n",
        "\n",
        "        return flows_backward3, flows_forward3\n",
        "\n",
        "    def get_aligned_image_2frames(self, x, flows_backward, flows_forward):\n",
        "        '''Parallel feature warping for 2 frames.'''\n",
        "\n",
        "        # backward\n",
        "        n = x.size(1)\n",
        "        x_backward = [torch.zeros_like(x[:, -1, ...]).repeat(1, 4, 1, 1)]\n",
        "        for i in range(n - 1, 0, -1):\n",
        "            x_i = x[:, i, ...]\n",
        "            flow = flows_backward[:, i - 1, ...]\n",
        "            x_backward.insert(0, flow_warp(x_i, flow.permute(0, 2, 3, 1), 'nearest4')) # frame i+1 aligned towards i\n",
        "\n",
        "        # forward\n",
        "        x_forward = [torch.zeros_like(x[:, 0, ...]).repeat(1, 4, 1, 1)]\n",
        "        for i in range(0, n - 1):\n",
        "            x_i = x[:, i, ...]\n",
        "            flow = flows_forward[:, i, ...]\n",
        "            x_forward.append(flow_warp(x_i, flow.permute(0, 2, 3, 1), 'nearest4')) # frame i-1 aligned towards i\n",
        "\n",
        "        return [torch.stack(x_backward, 1), torch.stack(x_forward, 1)]\n",
        "\n",
        "    def forward_features(self, x, flows_backward, flows_forward):\n",
        "        '''Main network for feature extraction.'''\n",
        "\n",
        "        x1 = self.stage1(x, flows_backward[0::4], flows_forward[0::4])\n",
        "        x2 = self.stage2(x1, flows_backward[1::4], flows_forward[1::4])\n",
        "        x3 = self.stage3(x2, flows_backward[2::4], flows_forward[2::4])\n",
        "        x4 = self.stage4(x3, flows_backward[3::4], flows_forward[3::4])\n",
        "        x = self.stage5(x4, flows_backward[2::4], flows_forward[2::4])\n",
        "        x = self.stage6(x + x3, flows_backward[1::4], flows_forward[1::4])\n",
        "        x = self.stage7(x + x2, flows_backward[0::4], flows_forward[0::4])\n",
        "        x = x + x1\n",
        "\n",
        "        for layer in self.stage8:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = rearrange(x, 'n c d h w -> n d h w c')\n",
        "        x = self.norm(x)\n",
        "        x = rearrange(x, 'n d h w c -> n c d h w')\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from models.network_vrt import VRT  # Adjust the import based on your actual file structure\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Define input dimensions\n",
        "B, T, C, H, W = 1, 6, 3, 64, 64  # Batch, Time, Channels, Height, Width\n",
        "scale = 1  # Upscale factor for SR\n",
        "\n",
        "# Random 3D tensor input and target\n",
        "input_tensor = torch.rand(B, T, C, H, W).to('cuda')\n",
        "target_tensor = torch.rand(B, T, C, H * scale, W * scale).to('cuda')"
      ],
      "metadata": {
        "id": "CkSv13b9fqMU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and move to CUDA\n",
        "model = VRT(\n",
        "    upscale=1,\n",
        "    img_size=[T, H, W],\n",
        "    window_size=[6, 8, 8],\n",
        "    depths=[8] * 7 + [4] * 6,\n",
        "    indep_reconsts=[11, 12],\n",
        "    embed_dims=[120] * 7 + [180] * 6,\n",
        "    num_heads=[6] * 13,\n",
        "    pa_frames=2,\n",
        "    deformable_groups=12,\n",
        ").to('cuda')\n",
        "\n",
        "# Print shape check\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    print(f\"Input shape: {input_tensor.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Target shape: {target_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocwt2L1Whirp",
        "outputId": "ddeabcfd-13d0-4b99-b9e8-01a886086c85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 6, 3, 64, 64])\n",
            "Output shape: torch.Size([1, 6, 3, 64, 64])\n",
            "Target shape: torch.Size([1, 6, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ApqPs7pDhmoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}