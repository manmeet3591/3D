{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Copyright (c) Microsoft Corporation. Licensed under the MIT license.\n",
        "\n",
        "`AdaptiveLayerNorm` was inspired by the following file:\n",
        "\n",
        "    https://github.com/facebookresearch/DiT/blob/ed81ce2229091fd4ecc9a223645f95cf379d582b/models.py#L101\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "__all__ = [\"AdaptiveLayerNorm\"]\n",
        "\n",
        "\n",
        "class AdaptiveLayerNorm(nn.Module):\n",
        "    \"\"\"Adaptive layer normalisation with scale and shift modulation.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, context_dim: int, scale_bias: float = 0) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Input dimension.\n",
        "            context_dim (int): Dimension of the conditioning signal.\n",
        "            scale_bias (float, optional): Scale bias to add to the scaling factor. Defaults to `0`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln = nn.LayerNorm(dim, elementwise_affine=False)\n",
        "        self.ln_modulation = nn.Sequential(nn.SiLU(), nn.Linear(context_dim, dim * 2))\n",
        "        self.scale_bias = scale_bias\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialise the weights.\"\"\"\n",
        "        nn.init.zeros_(self.ln_modulation[-1].weight)\n",
        "        nn.init.zeros_(self.ln_modulation[-1].bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape `(B, L, D)`.\n",
        "            c (torch.Tensor): Conditioning tensor of shape `(B, D)`.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape `(B, L, D)`.\n",
        "        \"\"\"\n",
        "        shift, scale = self.ln_modulation(c).unsqueeze(1).chunk(2, dim=-1)\n",
        "        return self.ln(x) * (self.scale_bias + scale) + shift"
      ],
      "metadata": {
        "id": "HR_87dxPITui"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Copyright (c) Microsoft Corporation. Licensed under the MIT license.\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "__all__ = [\"area\", \"compute_patch_areas\", \"radius_earth\"]\n",
        "\n",
        "\n",
        "radius_earth = 6378137 / 1000\n",
        "\"\"\"float: Radius of the earth in kilometers.\"\"\"\n",
        "\n",
        "\n",
        "def area(polygon: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute the area of a polygon specified by latitudes and longitudes in degrees.\n",
        "\n",
        "    This function is a PyTorch port of the PyPI package `area`. In particular, it is heavily\n",
        "    inspired by the following file:\n",
        "\n",
        "        https://github.com/scisco/area/blob/9d9549d6ebffcbe4bffe11b71efa2d406d1c9fe9/area/__init__.py\n",
        "\n",
        "    Args:\n",
        "        polygon (:class:`torch.Tensor`): Polygon of the shape `(*b, n, 2)` where `b` is an optional\n",
        "            multidimensional batch size, `n` is the number of points of the polygon, and 2\n",
        "            concatenates first latitudes and then longitudes. The polygon does not have be closed.\n",
        "\n",
        "    Returns:\n",
        "        :class:`torch.Tensor`: Area in square kilometers.\n",
        "    \"\"\"\n",
        "    # Be sure to close the loop.\n",
        "    polygon = torch.cat((polygon, polygon[..., -1:, :]), axis=-2)\n",
        "\n",
        "    area = torch.zeros(polygon.shape[:-2], dtype=polygon.dtype, device=polygon.device)\n",
        "    n = polygon.shape[-2]  # Number of points of the polygon\n",
        "\n",
        "    rad = torch.deg2rad  # Convert degrees to radians.\n",
        "\n",
        "    if n > 2:\n",
        "        for i in range(n):\n",
        "            i_lower = i\n",
        "            i_middle = (i + 1) % n\n",
        "            i_upper = (i + 2) % n\n",
        "\n",
        "            lon_lower = polygon[..., i_lower, 1]\n",
        "            lat_middle = polygon[..., i_middle, 0]\n",
        "            lon_upper = polygon[..., i_upper, 1]\n",
        "\n",
        "            area = area + (rad(lon_upper) - rad(lon_lower)) * torch.sin(rad(lat_middle))\n",
        "\n",
        "    area = area * radius_earth * radius_earth / 2\n",
        "\n",
        "    return torch.abs(area)\n",
        "\n",
        "\n",
        "def expand_matrix(matrix: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Expand matrix by adding one row and one column to each side, using\n",
        "    linear interpolation.\n",
        "\n",
        "    Args:\n",
        "        matrix (:class:`torch.Tensor`): Matrix to expand.\n",
        "\n",
        "    Returns:\n",
        "        :class:`torch.Tensor`: `matrix`, but with two extra rows and two extra columns.\n",
        "    \"\"\"\n",
        "    # Add top and bottom rows.\n",
        "    matrix = torch.cat(\n",
        "        (\n",
        "            2 * matrix[0:1] - matrix[1:2],\n",
        "            matrix,\n",
        "            2 * matrix[-1:] - matrix[-2:-1],\n",
        "        ),\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    # Add left and right columns.\n",
        "    matrix = torch.cat(\n",
        "        (\n",
        "            2 * matrix[:, 0:1] - matrix[:, 1:2],\n",
        "            matrix,\n",
        "            2 * matrix[:, -1:] - matrix[:, -2:-1],\n",
        "        ),\n",
        "        dim=1,\n",
        "    )\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def compute_patch_areas(lat: torch.Tensor, lon: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"A pair of latitude and longitude matrices defines a number non-intersecting patches on the\n",
        "    Earth. For a global grid, these patches span the entire surface of the Earth. For a local grid,\n",
        "    the patches might span only a country or a continent. This function computes the area of every\n",
        "    specified patch.\n",
        "\n",
        "    To divide the Earth into patches, the idea is to let a grid point be the _center_ of the\n",
        "    corresponding patch. The vertices of this patch will then sit exactly inbetween the grid\n",
        "    point and the grid points immediately diagonally and non-diagonally above, below, left, and\n",
        "    right. For a grid point at the very top of the grid, for example, there is no immediately above\n",
        "    grid point. In that case, we enlarge the grid by a row at the top by linearly interpolating the\n",
        "    latitudinal progression.\n",
        "\n",
        "    Summary of algorithm:\n",
        "    1. Enlarge the latitude and longitude matrices by adding one row and one column to each side.\n",
        "    2. Calculate the patch vertices by averaging every 2x2 square in the enlarged grid. We also\n",
        "        call these points the midpoints.\n",
        "    3. By using the vertices of the patches, i.e. the midpoints, compute the areas of the patches.\n",
        "\n",
        "    Args:\n",
        "        lat (:class:`torch.Tensor`): Latitude matrix. Must be decreasing along rows.\n",
        "        lon (:class:`torch.Tensor`): Longitude matrix. Must be increasing along columns.\n",
        "\n",
        "    Returns:\n",
        "        :class:`torch.Tensor`: Areas in square kilometer.\n",
        "    \"\"\"\n",
        "    if not (lat.dim() == lon.dim() == 2):\n",
        "        raise ValueError(\"`lat` and `lon` must both be matrices.\")\n",
        "    if lat.shape != lat.shape:\n",
        "        raise ValueError(\"`lat` and `lon` must have the same shape.\")\n",
        "\n",
        "    # Check that the latitude matrix is decreasing in the appropriate way.\n",
        "    if not torch.all(lat[1:] - lat[:-1] <= 0):\n",
        "        raise ValueError(\"`lat` must be decreasing along rows.\")\n",
        "\n",
        "    # Check that the longitude matrix is increasing in the appropriate way.\n",
        "    if not torch.all(lon[:, 1:] - lon[:, :-1] >= 0):\n",
        "        raise ValueError(\"`lon` must be increasing along columns.\")\n",
        "\n",
        "    # Enlarge the latitude and longitude matrices for the midpoint computation.\n",
        "    lat = expand_matrix(lat)\n",
        "    lon = expand_matrix(lon)\n",
        "\n",
        "    # Latitudes cannot expand beyond the poles.\n",
        "    lat = torch.clamp(lat, -90, 90)\n",
        "\n",
        "    # Calculate midpoints between entries in lat/lon. This is very important for symmetry of the\n",
        "    # resulting areas.\n",
        "    lat_midpoints = (lat[:-1, :-1] + lat[:-1, 1:] + lat[1:, :-1] + lat[1:, 1:]) / 4\n",
        "    lon_midpoints = (lon[:-1, :-1] + lon[:-1, 1:] + lon[1:, :-1] + lon[1:, 1:]) / 4\n",
        "\n",
        "    # Determine squares and return the area of those squares.\n",
        "    top_left = torch.stack((lat_midpoints[1:, :-1], lon_midpoints[1:, :-1]), dim=-1)\n",
        "    top_right = torch.stack((lat_midpoints[1:, 1:], lon_midpoints[1:, 1:]), dim=-1)\n",
        "    bottom_left = torch.stack((lat_midpoints[:-1, :-1], lon_midpoints[:-1, :-1]), dim=-1)\n",
        "    bottom_right = torch.stack((lat_midpoints[:-1, 1:], lon_midpoints[:-1, 1:]), dim=-1)\n",
        "    polygon = torch.stack((top_left, top_right, bottom_right, bottom_left), dim=-2)\n",
        "\n",
        "    return area(polygon)"
      ],
      "metadata": {
        "id": "TqgKjDQiIYpJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Copyright (c) Microsoft Corporation. Licensed under the MIT license.\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from aurora.area import area, radius_earth\n",
        "\n",
        "__all__ = [\n",
        "    \"FourierExpansion\",\n",
        "    \"pos_expansion\",\n",
        "    \"scale_expansion\",\n",
        "    \"lead_time_expansion\",\n",
        "    \"levels_expansion\",\n",
        "    \"absolute_time_expansion\",\n",
        "]\n",
        "\n",
        "\n",
        "class FourierExpansion(nn.Module):\n",
        "    \"\"\"A Fourier series-style expansion into a high-dimensional space.\n",
        "\n",
        "    Attributes:\n",
        "        lower (float): Lower wavelength.\n",
        "        upper (float): Upper wavelength.\n",
        "        assert_range (bool): Assert that the encoded tensor is within the specified wavelength\n",
        "            range.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lower: float, upper: float, assert_range: bool = True) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            lower (float): Lower wavelength.\n",
        "            upper (float): Upper wavelength.\n",
        "            assert_range (bool, optional): Assert that the encoded tensor is within the specified\n",
        "                wavelength range. Defaults to `True`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        self.assert_range = assert_range\n",
        "\n",
        "    def forward(self, x: torch.Tensor, d: int) -> torch.Tensor:\n",
        "        \"\"\"Perform the expansion.\n",
        "\n",
        "        Adds a dimension of length `d` to the end of the shape of `x`.\n",
        "\n",
        "        Args:\n",
        "            x (:class:`torch.Tensor`): Input to expand of shape `(..., n)`. All elements of `x` must\n",
        "                lie within `[self.lower, self.upper]` if `self.assert_range` is `True`.\n",
        "            d (int): Dimensionality. Must be a multiple of two.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: If `self.assert_range` is `True` and not all elements of `x` are not\n",
        "                within `[self.lower, self.upper]`.\n",
        "            ValueError: If `d` is not a multiple of two.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Fourier series-style expansion of `x` of shape `(..., n, d)`.\n",
        "        \"\"\"\n",
        "        # If the input is not within the configured range, the embedding might be ambiguous!\n",
        "        in_range = torch.logical_and(self.lower <= x.abs(), torch.all(x.abs() <= self.upper))\n",
        "        in_range_or_zero = torch.all(\n",
        "            torch.logical_or(in_range, x == 0)\n",
        "        )  # Allow zeros to pass through.\n",
        "        if self.assert_range and not in_range_or_zero:\n",
        "            raise AssertionError(\n",
        "                f\"The input tensor is not within the configured range\"\n",
        "                f\" `[{self.lower}, {self.upper}]`.\"\n",
        "            )\n",
        "\n",
        "        # We will use half of the dimensionality for `sin` and the other half for `cos`.\n",
        "        if not (d % 2 == 0):\n",
        "            raise ValueError(\"The dimensionality must be a multiple of two.\")\n",
        "\n",
        "        # Always perform the expansion with `float64`s to avoid numerical accuracy shenanigans.\n",
        "        x = x.double()\n",
        "\n",
        "        wavelengths = torch.logspace(\n",
        "            math.log10(self.lower),\n",
        "            math.log10(self.upper),\n",
        "            d // 2,\n",
        "            base=10,\n",
        "            device=x.device,\n",
        "            dtype=x.dtype,\n",
        "        )\n",
        "        prod = torch.einsum(\"...i,j->...ij\", x, 2 * np.pi / wavelengths)\n",
        "        encoding = torch.cat((torch.sin(prod), torch.cos(prod)), dim=-1)\n",
        "\n",
        "        return encoding.float()  # Cast to `float32` to avoid incompatibilities.\n",
        "\n",
        "\n",
        "# Determine a reasonable smallest value for the scale embedding by assuming a smallest delta in\n",
        "# latitudes and longitudes.\n",
        "_delta = 0.01  # Reasonable smallest delta in latitude and longitude\n",
        "_min_patch_area: float = area(\n",
        "    torch.tensor(\n",
        "        [\n",
        "            # The smallest patches will be at the poles. Just use the north pole.\n",
        "            [90, 0],\n",
        "            [90, _delta],\n",
        "            [90 - _delta, _delta],\n",
        "            [90 - _delta, 0],\n",
        "        ],\n",
        "        dtype=torch.float64,\n",
        "    )\n",
        ").item()\n",
        "_area_earth = 4 * np.pi * radius_earth * radius_earth\n",
        "\n",
        "pos_expansion = FourierExpansion(_delta, 720)\n",
        "\"\"\":class:`.FourierExpansion`: Fourier expansion for the encoding of latitudes and longitudes in\n",
        "degrees.\"\"\"\n",
        "\n",
        "scale_expansion = FourierExpansion(_min_patch_area, _area_earth)\n",
        "\"\"\":class:`.FourierExpansion`: Fourier expansion for the encoding of patch areas in squared\n",
        "kilometers.\"\"\"\n",
        "\n",
        "lead_time_expansion = FourierExpansion(1 / 60, 24 * 7 * 3)\n",
        "\"\"\":class:`.FourierExpansion`: Fourier expansion for the lead time encoding in hours.\"\"\"\n",
        "\n",
        "levels_expansion = FourierExpansion(0.01, 1e5)\n",
        "\"\"\":class:`.FourierExpansion`: Fourier expansion for the pressure level encoding in hPa.\"\"\"\n",
        "\n",
        "absolute_time_expansion = FourierExpansion(1, 24 * 365.25, assert_range=False)\n",
        "\"\"\":class:`.FourierExpansion`: Fourier expansion for the absolute time encoding in hours.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p9bNCtC7IhYG",
        "outputId": "09a387bc-ff16-460d-ff63-0ecd4da39164"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "':class:`.FourierExpansion`: Fourier expansion for the absolute time encoding in hours.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Copyright (c) Microsoft Corporation. Licensed under the MIT license.\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import Literal\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "__all__ = [\"LoRA\", \"LoRARollout\", \"LoRAMode\"]\n",
        "\n",
        "LoRAMode = Literal[\"single\", \"all\"]\n",
        "\n",
        "\n",
        "class LoRA(nn.Module):\n",
        "    \"\"\"LoRA adaptation for a linear layer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        r: int = 4,\n",
        "        alpha: int = 1,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): Number of input features.\n",
        "            out_features (int): Number of output features.\n",
        "            r (int, optional): Rank. Defaults to `4`.\n",
        "            alpha (int, optional): Alpha. Defaults to `1`.\n",
        "            dropout (float, optional): Drop-out rate. Defaults to `0.0`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert r > 0, \"The rank must be strictly positive.\"\n",
        "        self.lora_alpha = alpha\n",
        "        self.r = r\n",
        "\n",
        "        self.lora_dropout = nn.Dropout(dropout)\n",
        "        self.lora_A = nn.Parameter(torch.empty((r, in_features)))\n",
        "        self.lora_B = nn.Parameter(torch.empty((out_features, r)))\n",
        "        self.scaling = self.lora_alpha / self.r\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialise weights.\"\"\"\n",
        "        # Initialise A the same way as the default for `nn.Linear` and set B to zero.\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute the LoRA adaptation.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input to the linear layer.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Additive correction for the output of the linear layer.\n",
        "        \"\"\"\n",
        "        x = self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)\n",
        "        return x * self.scaling\n",
        "\n",
        "\n",
        "class LoRARollout(nn.Module):\n",
        "    \"\"\"Per-roll-out-step LoRA finetuning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        r: int = 8,\n",
        "        alpha: int = 8,\n",
        "        dropout: float = 0.0,\n",
        "        max_steps: int = 40,\n",
        "        mode: LoRAMode = \"single\",\n",
        "    ):\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): Number of input features.\n",
        "            out_features (int): Number of output features.\n",
        "            r (int, optional): Rank. Defaults to `4`.\n",
        "            alpha (int, optional): Alpha. Defaults to `1`.\n",
        "            dropout (float, optional): Drop-out rate. Defaults to `0.0`.\n",
        "            max_steps (int, optional): Maximum number of roll-out steps. Defaults to `40`.\n",
        "            mode (str, optional): Mode. `\"single\"` uses the same LoRA for all roll-out steps,\n",
        "                and `\"all\"` uses a different LoRA for every roll-out step. Defaults to `\"single\"`.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.max_steps = max_steps\n",
        "        lora_layers = max_steps if mode == \"all\" else 1\n",
        "        self.loras = nn.ModuleList(\n",
        "            [\n",
        "                LoRA(in_features, out_features, r=r, alpha=alpha, dropout=dropout)\n",
        "                for _ in range(lora_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, step: int) -> torch.Tensor:\n",
        "        \"\"\"Compute the LoRA adaptation.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input to the linear layer.\n",
        "            step (int): Roll-out step, starting at zero.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Additive correction for the output of the linear layer.\n",
        "        \"\"\"\n",
        "        assert step >= 0, f\"Step must be non-negative, found {step}.\"\n",
        "\n",
        "        if step >= self.max_steps:\n",
        "            return 0\n",
        "\n",
        "        if self.mode == \"single\":\n",
        "            return self.loras[0](x)\n",
        "        elif self.mode == \"all\":\n",
        "            return self.loras[step](x)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {self.mode}\")"
      ],
      "metadata": {
        "id": "owzcnMHWIrOu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Copyright (c) Microsoft Corporation. Licensed under the MIT license.\"\"\"\n",
        "\n",
        "from typing import TypeVar\n",
        "\n",
        "import torch\n",
        "from einops import rearrange\n",
        "from timm.models.vision_transformer import trunc_normal_\n",
        "from torch import nn\n",
        "\n",
        "__all__ = [\n",
        "    \"unpatchify\",\n",
        "    \"check_lat_lon_dtype\",\n",
        "    \"maybe_adjust_windows\",\n",
        "    \"init_weights\",\n",
        "]\n",
        "\n",
        "\n",
        "def unpatchify(x: torch.Tensor, V: int, H: int, W: int, P: int) -> torch.Tensor:\n",
        "    \"\"\"Unpatchify hidden representation.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Patchified input of shape `(B, L, C, V * P^2)` where `P` is the\n",
        "            patch size.\n",
        "        V (int): Number of variables.\n",
        "        H (int): Number of latitudes.\n",
        "        W (int): Number of longitudes.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Unpatchified representation of shape `(B, V, C, H, W)`.\n",
        "    \"\"\"\n",
        "    assert x.dim() == 4, f\"Expected 4D tensor, but got {x.dim()}D.\"\n",
        "    B, C = x.size(0), x.size(2)\n",
        "    H = H // P\n",
        "    W = W // P\n",
        "    assert x.size(1) == H * W\n",
        "    assert x.size(-1) == V * P**2\n",
        "\n",
        "    x = x.reshape(shape=(B, H, W, C, P, P, V))\n",
        "    x = rearrange(x, \"B H W C P1 P2 V -> B V C H P1 W P2\")\n",
        "    x = x.reshape(shape=(B, V, C, H * P, W * P))\n",
        "    return x\n",
        "\n",
        "\n",
        "def check_lat_lon_dtype(lat: torch.Tensor, lon: torch.Tensor) -> None:\n",
        "    \"\"\"Assert that `lat` and `lon` are at least `float32`s.\"\"\"\n",
        "    assert lat.dtype in [torch.float32, torch.float64], f\"Latitude num. unstable: {lat.dtype}.\"\n",
        "    assert lon.dtype in [torch.float32, torch.float64], f\"Longitude num. unstable: {lon.dtype}.\"\n",
        "\n",
        "\n",
        "T = TypeVar(\"T\", tuple[int, int], tuple[int, int, int])\n",
        "\n",
        "\n",
        "def maybe_adjust_windows(window_size: T, shift_size: T, res: T) -> tuple[T, T]:\n",
        "    \"\"\"Adjust the window size and shift size if the input resolution is smaller than the window\n",
        "    size.\"\"\"\n",
        "    err_msg = f\"Expected same length, found {len(window_size)}, {len(shift_size)} and {len(res)}.\"\n",
        "    assert len(window_size) == len(shift_size) == len(res), err_msg\n",
        "\n",
        "    mut_shift_size, mut_window_size = list(shift_size), list(window_size)\n",
        "    for i in range(len(res)):\n",
        "        if res[i] <= window_size[i]:\n",
        "            mut_shift_size[i] = 0\n",
        "            mut_window_size[i] = res[i]\n",
        "\n",
        "    new_window_size: T = tuple(mut_window_size)  # type: ignore[assignment]\n",
        "    new_shift_size: T = tuple(mut_shift_size)  # type: ignore[assignment]\n",
        "\n",
        "    assert min(new_window_size) > 0, f\"Window size must be positive. Found {new_window_size}.\"\n",
        "    assert min(new_shift_size) >= 0, f\"Shift size must be non-negative. Found {new_shift_size}.\"\n",
        "\n",
        "    return new_window_size, new_shift_size\n",
        "\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    \"\"\"Initialise weights of a module with a truncated normal distribution.\n",
        "\n",
        "    `nn.LayerNorm` is initialised with a `weight` of 1 and a `bias` of 0.\n",
        "\n",
        "    Args:\n",
        "        m (torch.nn.Module): Module.\n",
        "    \"\"\"\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d, nn.ConvTranspose3d)):\n",
        "        trunc_normal_(m.weight, std=0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "        if m.weight is not None:\n",
        "            nn.init.constant_(m.weight, 1.0)"
      ],
      "metadata": {
        "id": "O_0ki7t2Ixnu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VokEQnJBHz-W",
        "outputId": "f6f48c79-7650-4926-bf4c-f4ba31e30443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Copyright (c) Microsoft Corporation. Licensed under the MIT license.\n",
        "\n",
        "Code adapted from\n",
        "\n",
        "    https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer_v2.py\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import itertools\n",
        "from datetime import timedelta\n",
        "from functools import lru_cache\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from timm.models.layers import DropPath, to_3tuple\n",
        "\n",
        "# from aurora.model.film import AdaptiveLayerNorm\n",
        "# from aurora.model.fourier import lead_time_expansion\n",
        "# from aurora.model.lora import LoRAMode, LoRARollout\n",
        "# from aurora.model.util import init_weights, maybe_adjust_windows\n",
        "\n",
        "__all__ = [\"Swin3DTransformerBackbone\"]\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"A one-hidden-layer MLP with dropout after the hidden layer and at the end.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        hidden_features: Optional[int] = None,\n",
        "        out_features: Optional[int] = None,\n",
        "        act_layer: type = nn.GELU,\n",
        "        drop: float = 0.0,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): Input dimensionality.\n",
        "            hidden_features (int, optional): Hidden layer dimensionality. Defaults to the input\n",
        "                dimensionality.\n",
        "            out_features (int, optional): Output dimensionality. Defaults to the input\n",
        "                dimensionality.\n",
        "            act_layer (type, optional): Activation function to use. Will be instantiated as\n",
        "                `act_layer()`. Defaults to `torch.nn.GELU`.\n",
        "            drop (float, optional): Drop-out rate. Defaults to no drop-out.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Run the MLP.\"\"\"\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"Window-based multi-head self-attention (W-MSA).\n",
        "\n",
        "    It supports both shifted and non-shifted windows.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        window_size: tuple[int, int, int],\n",
        "        num_heads: int,\n",
        "        qkv_bias: bool = True,\n",
        "        qk_scale: Optional[float] = None,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "        lora_r: int = 8,\n",
        "        lora_alpha: int = 8,\n",
        "        lora_dropout: float = 0.0,\n",
        "        lora_steps: int = 40,\n",
        "        lora_mode: LoRAMode = \"single\",\n",
        "        use_lora: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            window_size (tuple[int, int, int]): The size of the windows.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool, optional): If `True`, add a learnable bias to the query, key, dn value.\n",
        "                Defaults to `True`.\n",
        "            qk_scale (float, optional): If set, overrides the default query-key scale of\n",
        "                `1/sqrt(head_dim)`.\n",
        "            attn_drop (float, optional): Drop-out rate of attention weights. Default to `0.0`.\n",
        "            proj_drop (float, optional): Drop-out rate of the output. Default to `0.0`.\n",
        "            lora_r (int, optional): LoRA rank. Defaults to `8`.\n",
        "            lora_alpha (int, optional): LoRA alpha. Defaults to `8`.\n",
        "            lora_dropout (float, optional): LoRA drop-out rate. Defaults to `0.0`.\n",
        "            lora_steps (int, optional): Maximum number of LoRA roll-out steps. Defaults to `40`.\n",
        "            lora_mode (str, optional): Mode. `\"single\"` uses the same LoRA for all roll-out steps,\n",
        "                and `\"all\"` uses a different LoRA for every roll-out step. Defaults to `\"single\"`.\n",
        "            use_lora (bool, optional): Enable LoRA. By default, LoRA is disabled.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # (Wc, Wh, Ww)\n",
        "        self.num_heads = num_heads\n",
        "        assert dim % num_heads == 0, f\"dim ({dim}) should be divisible by num_heads ({num_heads}).\"\n",
        "        self.head_dim = dim // num_heads\n",
        "\n",
        "        self.attn_drop = attn_drop\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        if use_lora:\n",
        "            self.lora_proj = LoRARollout(\n",
        "                dim, dim, lora_r, lora_alpha, lora_dropout, lora_steps, lora_mode\n",
        "            )\n",
        "            self.lora_qkv = LoRARollout(\n",
        "                dim, dim * 3, lora_r, lora_alpha, lora_dropout, lora_steps, lora_mode\n",
        "            )\n",
        "        else:\n",
        "            self.lora_proj = lambda *args, **kwargs: 0  # type: ignore\n",
        "            self.lora_qkv = lambda *args, **kwargs: 0  # type: ignore\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: torch.Tensor | None = None,\n",
        "        rollout_step: int = 0,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Run the forward pass of the window-based multi-head self-attention layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input features with shape of `(nW*B, N, C)`.\n",
        "            mask (torch.Tensor, optional): Attention mask of floating points in the range\n",
        "                `[-inf, 0)` with shape of `(nW, ws, ws)`, where `nW` is the number of windows,\n",
        "                and `ws` is the window size (i.e. total tokens inside the window).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output of shape `(nW*B, N, C)`.\n",
        "        \"\"\"\n",
        "        qkv = self.qkv(x) + self.lora_qkv(x, rollout_step)\n",
        "        qkv = rearrange(qkv, \"B N (qkv H D) -> qkv B H N D\", H=self.num_heads, qkv=3)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn_dropout = self.attn_drop if self.training else 0.0\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            q, k, v = map(lambda t: rearrange(t, \"(B nW) H N D -> B nW H N D\", nW=nW), (q, k, v))\n",
        "            mask = mask.unsqueeze(1).unsqueeze(0)  # (1, nW, 1, ws, ws)\n",
        "            x = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=attn_dropout)\n",
        "            x = rearrange(x, \"B nW H N D -> (B nW) H N D\")\n",
        "        else:\n",
        "            x = F.scaled_dot_product_attention(q, k, v, dropout_p=attn_dropout)\n",
        "\n",
        "        x = rearrange(x, \"B H N D -> B N (H D)\")\n",
        "        x = self.proj(x) + self.lora_proj(x, rollout_step)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}\"\n",
        "\n",
        "\n",
        "def get_two_sidded_padding(H_padding: int, W_padding: int) -> tuple[int, int, int, int]:\n",
        "    \"\"\"Returns the padding for the left, right, top, and bottom sides, in exactly that order.\"\"\"\n",
        "    assert H_padding >= 0, f\"H_padding ({H_padding}) must be >= 0.\"\n",
        "    assert W_padding >= 0, f\"W_padding ({W_padding}) must be >= 0.\"\n",
        "\n",
        "    if H_padding:\n",
        "        padding_top = H_padding // 2\n",
        "        padding_bottom = H_padding - padding_top\n",
        "    else:\n",
        "        padding_top = padding_bottom = 0\n",
        "\n",
        "    if W_padding:\n",
        "        padding_left = W_padding // 2\n",
        "        padding_right = W_padding - padding_left\n",
        "    else:\n",
        "        padding_left = padding_right = 0\n",
        "\n",
        "    return padding_left, padding_right, padding_top, padding_bottom\n",
        "\n",
        "\n",
        "def window_partition_3d(x: torch.Tensor, ws: tuple[int, int, int]) -> torch.Tensor:\n",
        "    \"\"\"Partition into windows.\n",
        "\n",
        "    Args:\n",
        "        x: (torch.Tensor): Input tensor of shape `(B, C, H, W, D)`.\n",
        "        ws: (tuple[int, int, int]): A 3D window size `(Wc, Wh, Ww)`.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Partitioning of shape `(num_windows*B, Wc, Wh, Ww, D)`.\n",
        "    \"\"\"\n",
        "    B, C, H, W, D = x.shape\n",
        "    assert C % ws[0] == 0, f\"C ({C}) % window_size ({ws[0]}) must be 0.\"\n",
        "    assert H % ws[1] == 0, f\"H ({H}) % window_size ({ws[1]}) must be 0.\"\n",
        "    assert W % ws[2] == 0, f\"W ({W}) % window_size ({ws[2]}) must be 0.\"\n",
        "\n",
        "    x = x.view(B, C // ws[0], ws[0], H // ws[1], ws[1], W // ws[2], ws[2], D)\n",
        "    windows = rearrange(x, \"B C1 Wc H1 Wh W1 Ww D -> (B C1 H1 W1) Wc Wh Ww D\")\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse_3d(windows: torch.Tensor, ws: tuple[int, int, int], C: int, H: int, W: int):\n",
        "    \"\"\"Unpartition a partitioning.\n",
        "\n",
        "    Args:\n",
        "        windows (torch.Tensor): Partitioning of shape `(num_windows*B, Wc, Wh, Ww, D)`.\n",
        "        ws (tuple[int, int, int]): The 3D window size.\n",
        "        C (int): Number of levels.\n",
        "        H (int): Height of image.\n",
        "        W (int): Width of image.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Unpartitioned input of shape `(B, C, H, W, D)`.\n",
        "    \"\"\"\n",
        "    assert C % ws[0] == 0, f\"D ({C}) % window_size ({ws[0]}) must be 0.\"\n",
        "    assert H % ws[1] == 0, f\"H ({H}) % window_size ({ws[1]}) must be 0.\"\n",
        "    assert W % ws[2] == 0, f\"W ({W}) % window_size ({ws[2]}) must be 0.\"\n",
        "\n",
        "    C1, H1, W1 = C // ws[0], H // ws[1], W // ws[2]\n",
        "    B = int(windows.shape[0] / (C1 * H1 * W1))\n",
        "    x = rearrange(\n",
        "        windows,\n",
        "        \"(B C1 H1 W1) Wc Wh Ww D -> B (C1 Wc) (H1 Wh) (W1 Ww) D\",\n",
        "        B=B,\n",
        "        C1=C1,\n",
        "        H1=H1,\n",
        "        W1=W1,\n",
        "        Wc=ws[0],\n",
        "        Wh=ws[1],\n",
        "        Ww=ws[2],  # fmt: skip\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_three_sidded_padding(\n",
        "    C_padding: int,\n",
        "    H_padding: int,\n",
        "    W_padding: int,\n",
        ") -> tuple[int, int, int, int, int, int]:\n",
        "    \"\"\"Returns the padding for the left, right, top, bottom, front, and back sides, in exactly that\n",
        "    order.\"\"\"\n",
        "    assert C_padding >= 0, f\"C_padding ({C_padding}) must be >= 0\"\n",
        "\n",
        "    if C_padding:\n",
        "        pad_front = C_padding // 2\n",
        "        pad_back = C_padding - pad_front\n",
        "    else:\n",
        "        pad_front = pad_back = 0\n",
        "\n",
        "    return (\n",
        "        *get_two_sidded_padding(H_padding, W_padding),\n",
        "        pad_front,\n",
        "        pad_back,\n",
        "    )\n",
        "\n",
        "\n",
        "def pad_3d(x: torch.Tensor, pad_size: tuple[int, int, int], value: float = 0.0) -> torch.Tensor:\n",
        "    \"\"\"Pads the input with value to the specified size.\"\"\"\n",
        "    # Padding is done from the last dimension. We use zero padding for the last dimension.\n",
        "    return F.pad(x, (0, 0, *get_three_sidded_padding(*pad_size)), value=value)\n",
        "\n",
        "\n",
        "def crop_3d(x: torch.Tensor, pad_size: tuple[int, int, int]) -> torch.Tensor:\n",
        "    \"\"\"Undoes the `pad_3d` function by cropping the padded values.\"\"\"\n",
        "    B, C, H, W, D = x.shape\n",
        "    Cp, Hp, Wp = pad_size\n",
        "\n",
        "    pleft, pright, ptop, pbottom, pfront, pback = get_three_sidded_padding(Cp, Hp, Wp)\n",
        "    x = x[:, pfront : C - pback, ptop : H - pbottom, pleft : W - pright, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_3d_merge_groups() -> list[tuple[int, int]]:\n",
        "    \"\"\"Returns the groups to be merged for the 3D case to obtain left-right connectivity.\"\"\"\n",
        "    merge_groups_2d = [(1, 2), (4, 5), (7, 8)]\n",
        "    merge_groups_3d = []\n",
        "    for i_c_slice in range(3):\n",
        "        for grp1_2d, grp2_2d in merge_groups_2d:\n",
        "            # The 2D merge groups show up in each of the `c_slices` with an offset of 9. 9\n",
        "            # correspond to the total number of 2D merge groups. See\n",
        "            # :func:`compute_3d_shifted_window_mask`.\n",
        "            offset = i_c_slice * 9\n",
        "            grp1_3d, grp2_3d = grp1_2d + offset, grp2_2d + offset\n",
        "            merge_groups_3d.append((grp1_3d, grp2_3d))\n",
        "    return merge_groups_3d\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def compute_3d_shifted_window_mask(\n",
        "    C: int,\n",
        "    H: int,\n",
        "    W: int,\n",
        "    ws: tuple[int, int, int],\n",
        "    ss: tuple[int, int, int],\n",
        "    device: torch.device,\n",
        "    dtype: torch.dtype = torch.bfloat16,\n",
        "    warped: bool = True,\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Computes the mask of each window for the shifted-window attention.\n",
        "\n",
        "    Args:\n",
        "        C (int): Number of levels.\n",
        "        H (int): Height of the image.\n",
        "        W (int): Width of the image.\n",
        "        ws (tuple[int, int, int]): Window sizes of the form `(Wc, Wh, Ww)`.\n",
        "        ss (tuple[int, int, int]): Shift sizes of the form `(Sc, Sh, Sw)`\n",
        "        dtype (torch.dtype, optional): Data type of the mask. Defaults to `torch.bfloat16`.\n",
        "        warped (bool): If `True`,assume that the left and right sides of the image are connected.\n",
        "            Defaults to `True`.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Attention mask for each window. Masked entries are -100 and non-masked\n",
        "            entries are 0. This matrix is added to the attention matrix before softmax.\n",
        "        torch.Tensor: Image mask splitting the input patches into groups. Used for debugging\n",
        "            purposes.\n",
        "    \"\"\"\n",
        "    img_mask = torch.zeros((1, C, H, W, 1), device=device, dtype=dtype)\n",
        "    c_slices = (slice(0, -ws[0]), slice(-ws[0], -ss[0]), slice(-ss[0], None))\n",
        "    h_slices = (slice(0, -ws[1]), slice(-ws[1], -ss[1]), slice(-ss[1], None))\n",
        "    w_slices = (slice(0, -ws[2]), slice(-ws[2], -ss[2]), slice(-ss[2], None))\n",
        "\n",
        "    # Assign each patch to a communication group. The iteration order here must be consistent with\n",
        "    # the indices that :func:`get_3d_merge_groups` computes.\n",
        "    cnt = 0\n",
        "    for c, h, w in itertools.product(c_slices, h_slices, w_slices):\n",
        "        img_mask[:, c, h, w, :] = cnt\n",
        "        cnt += 1\n",
        "\n",
        "    if warped:\n",
        "        for grp1, grp2 in get_3d_merge_groups():\n",
        "            img_mask = img_mask.masked_fill(img_mask == grp1, grp2)\n",
        "\n",
        "    # Pad to multiple of window size and assign padded patches to a separate group (`cnt` is still\n",
        "    # unused).\n",
        "    pad_size = (ws[0] - C % ws[0], ws[1] - H % ws[1], ws[2] - W % ws[2])\n",
        "    pad_size = (pad_size[0] % ws[0], pad_size[1] % ws[1], pad_size[2] % ws[2])\n",
        "    img_mask = pad_3d(img_mask, pad_size, value=cnt)\n",
        "\n",
        "    mask_windows = window_partition_3d(img_mask, ws)  # (nW*B, ws[0], ws[1], ws[2], 1)\n",
        "    mask_windows = mask_windows.view(-1, ws[0] * ws[1] * ws[2])  # (nW*B, ws[0] * ws[1] * ws[2])\n",
        "    # Two patches communicate if they are in the same group (i.e. the difference below is 0).\n",
        "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "    attn_mask = attn_mask.masked_fill(attn_mask != 0, -100.0).masked_fill(attn_mask == 0, 0.0)\n",
        "\n",
        "    return attn_mask, img_mask\n",
        "\n",
        "\n",
        "class Swin3DTransformerBlock(nn.Module):\n",
        "    \"\"\"3D Swin Transformer block.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        time_dim: int,\n",
        "        window_size: tuple[int, int, int] = (2, 7, 7),\n",
        "        shift_size: tuple[int, int, int] = (0, 0, 0),\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        drop: float = 0.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        drop_path: float = 0.0,\n",
        "        act_layer: type = nn.GELU,\n",
        "        scale_bias: float = 0.0,\n",
        "        lora_steps: int = 40,\n",
        "        lora_mode: LoRAMode = \"single\",\n",
        "        use_lora: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            input_resolution (tuple[int, int]): Input resolution.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            time_dim (int): Dimension of the lead time embedding.\n",
        "            window_size (tuple[int, int, int]): Window size. Defaults to `(2, 7, 7)`.\n",
        "            shift_size (tuple[int, int, int]): Shift size for SW-MSA. Defaults to `(0, 0, 0)`.\n",
        "            mlp_ratio (float): Hidden layer dimensionality divided by that of the input for all\n",
        "                MLPs. Defaults to `4.0`.\n",
        "            qkv_bias (bool, optional): If `True,` add a learnable bias to each query, key, and\n",
        "                value. Defaults to `True`.\n",
        "            drop (float, optional): Drop-out rate. Defaults to `0.0`.\n",
        "            attn_drop (float, optional): Attention drop-out rate. Defaults to `0.0`.\n",
        "            drop_path (float, optional): Stochastic depth rate. Defaults to `0.0`\n",
        "            act_layer (type, optional): Activation function to use. Will be instantiated as\n",
        "                `act_layer()`. Defaults to `torch.nn.GELU`.\n",
        "            scale_bias (float, optional): Scale bias for\n",
        "                :class:`aurora.model.film.AdaptiveLayerNorm`. Defaults to `0`.\n",
        "            lora_steps (int, optional): Maximum number of LoRA roll-out steps. Defaults to `40`.\n",
        "            lora_mode (str, optional): Mode. `\"single\"` uses the same LoRA for all roll-out steps,\n",
        "                and `\"all\"` uses a different LoRA for every roll-out step. Defaults to `\"single\"`.\n",
        "            use_lora (bool): Enable LoRA. By default, LoRA is disabled.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        self.norm1 = AdaptiveLayerNorm(dim, time_dim, scale_bias=scale_bias)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=self.window_size,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "            lora_steps=lora_steps,\n",
        "            use_lora=use_lora,\n",
        "            lora_mode=lora_mode,\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.norm2 = AdaptiveLayerNorm(dim, time_dim, scale_bias=scale_bias)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(\n",
        "            in_features=dim,\n",
        "            hidden_features=mlp_hidden_dim,\n",
        "            act_layer=act_layer,\n",
        "            drop=drop,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        c: torch.Tensor,\n",
        "        res: tuple[int, int, int],\n",
        "        rollout_step: int,\n",
        "        warped: bool = True,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Run the block.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tokens of shape `(B, L, D)`.\n",
        "            c (torch.Tensor): Conditioning context of shape `(B, D)`.\n",
        "            res (tuple[int, int, int]): Resolution of the input `x`.\n",
        "            rollout_step (int): Roll-out step.\n",
        "            warped (bool, optional): Connect the left and right sides. Defaults to `True`.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tokens.\n",
        "        \"\"\"\n",
        "        C, H, W = res\n",
        "        B, L, D = x.shape\n",
        "        assert L == C * H * W, f\"Wrong feature size: {L} vs {C}x{H}x{W}={C*H*W}\"\n",
        "\n",
        "        # If the window size is larger than the input resolution, we do not partition windows.\n",
        "        ws, ss = maybe_adjust_windows(self.window_size, self.shift_size, res)\n",
        "\n",
        "        shortcut = x\n",
        "        x = x.view(B, C, H, W, D)\n",
        "\n",
        "        # Perform cyclic shift.\n",
        "        if not all(s == 0 for s in ss):\n",
        "            shifted_x = torch.roll(x, shifts=(-ss[0], -ss[1], -ss[2]), dims=(1, 2, 3))\n",
        "            attn_mask, _ = compute_3d_shifted_window_mask(\n",
        "                C, H, W, ws, ss, x.device, x.dtype, warped=warped\n",
        "            )\n",
        "        else:\n",
        "            shifted_x = x\n",
        "            attn_mask = None\n",
        "\n",
        "        # Pad the input to multiple of window size.\n",
        "        pad_size = ((-C) % ws[0], (-H) % ws[1], (-W) % ws[2])\n",
        "        shifted_x = pad_3d(shifted_x, pad_size)\n",
        "\n",
        "        # Partition the patches/tokens into windows.\n",
        "        x_windows = window_partition_3d(shifted_x, ws)  # (nW*B, ws, ws, D)\n",
        "        x_windows = x_windows.view(-1, ws[0] * ws[1] * ws[2], D)  # (nW*B, ws*ws, D)\n",
        "\n",
        "        # W-MSA/SW-MSA. Has shape (nW*B, ws*ws, D).\n",
        "        attn_windows = self.attn(x_windows, mask=attn_mask, rollout_step=rollout_step)\n",
        "\n",
        "        # Merge the windows into the original input (patch) resolution.\n",
        "        attn_windows = attn_windows.view(-1, ws[0], ws[1], ws[2], D)  # (nW*B, Wc, Wh, Ww, D)\n",
        "        _, pad_C, pad_H, pad_W, _ = shifted_x.shape\n",
        "        shifted_x = window_reverse_3d(attn_windows, ws, pad_C, pad_H, pad_W)  # (B C' H' W' D)\n",
        "\n",
        "        # Reverse the padding after the attention computations are done.\n",
        "        shifted_x = crop_3d(shifted_x, pad_size)\n",
        "\n",
        "        # Reverse the cyclic shift.\n",
        "        if not all(s == 0 for s in ss):\n",
        "            x = torch.roll(shifted_x, shifts=(ss[0], ss[1], ss[2]), dims=(1, 2, 3))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        x = x.reshape(B, C * H * W, D)\n",
        "\n",
        "        x = shortcut + self.drop_path(self.norm1(x, c))\n",
        "        x = x + self.drop_path(self.norm2(self.mlp(x), c))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging3D(nn.Module):\n",
        "    \"\"\"Patch merging layer.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = nn.LayerNorm(4 * dim)\n",
        "\n",
        "    def _merge(self, x: torch.Tensor, res: tuple[int, int, int]) -> torch.Tensor:\n",
        "        C, H, W = res\n",
        "        B, L, D = x.shape\n",
        "        assert L == C * H * W, f\"Wrong feature size: {L} vs {C}*{H}*{W}={C*H*W}.\"\n",
        "        assert H > 1, f\"Height ({H}) must be larger than 1.\"\n",
        "        assert W > 1, f\"Width ({W}) must be larger than 1.\"\n",
        "\n",
        "        x = x.view(B, C, H, W, D)\n",
        "        x = pad_3d(x, (0, H % 2, W % 2))  # Pad to multiple of 2.\n",
        "        new_H, new_W = x.shape[2], x.shape[3]\n",
        "        assert x.shape[2] % 2 == 0, f\"({new_H}) % 2 != 0.\"\n",
        "        assert x.shape[3] % 2 == 0, f\"({new_W}) % 2 != 0.\"\n",
        "\n",
        "        x = x.reshape(B, C, new_H // 2, 2, new_W // 2, 2, D)\n",
        "        return rearrange(x, \"B C H h W w D -> B (C H W) (h w D)\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor, input_resolution: tuple[int, int, int]) -> torch.Tensor:\n",
        "        \"\"\"Perform the path merging operation.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tokens of shape `(B, C*H*W, D)`.\n",
        "            input_resolution (tuple[int, int, int]): Resolution of `x` of the form `(C, H, W)`.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Merged tokens of shape `(B, C*H/2*W/2, 2*D)`.\n",
        "        \"\"\"\n",
        "        x = self._merge(x, input_resolution)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchSplitting3D(nn.Module):\n",
        "    \"\"\"Patch splitting layer.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        assert dim % 2 == 0, f\"dim ({dim}) should be divisible by 2.\"\n",
        "        self.lin1 = nn.Linear(dim, dim * 2, bias=False)\n",
        "        self.lin2 = nn.Linear(dim // 2, dim // 2, bias=False)\n",
        "        self.norm = nn.LayerNorm(dim // 2)\n",
        "\n",
        "    def _split(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        res: tuple[int, int, int],\n",
        "        crop: tuple[int, int, int],\n",
        "    ) -> torch.Tensor:\n",
        "        C, H, W = res\n",
        "        B, L, D = x.shape\n",
        "        assert L == C * H * W, f\"Wrong number of tokens: {L} != {C}*{H}*{W}={C*H*W}.\"\n",
        "        assert D % 4 == 0, f\"Number of input features ({D}) is not a multiple of 4.\"\n",
        "\n",
        "        x = x.view(B, C, H, W, 2, 2, D // 4)\n",
        "        x = rearrange(x, \"B C H W h w D -> B C (H h) (W w) D\")  # (B, C, 2*H, 2*W, D/4)\n",
        "        x = crop_3d(x, crop)  # Undo padding from `PatchMerging` (if any).\n",
        "        return x.reshape(B, -1, D // 4)  # (B, C*2H*2W, D/4)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        input_resolution: tuple[int, int, int],\n",
        "        crop: tuple[int, int, int] = (0, 0, 0),\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Perform the patch splitting.\n",
        "\n",
        "        Quadruples the number of patches by doubling in the `H` and `W` dimensions.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tokens of shape `(B, C*H*W, D)`.\n",
        "            input_resolution (tuple[int, int, int]): Resolution of `x` of the form `(C, H, W)`.\n",
        "            crop (tuple[int, int, int], optional): Cropping for every dimension. Defaults to\n",
        "                no cropping.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Splitted tokens of shape `(B, C*(2*H)*(2*W), D/2)`.\n",
        "        \"\"\"\n",
        "        x = self.lin1(x)  # (B, C*H*W, D*2)\n",
        "        x = self._split(x, input_resolution, crop)\n",
        "        x = self.norm(x)\n",
        "        x = self.lin2(x)  # (B, C*(2*H)*(2*W), D/2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicLayer3D(nn.Module):\n",
        "    \"\"\"A basic 3D Swin Transformer layer for one stage.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        depth: int,\n",
        "        num_heads: int,\n",
        "        ws: tuple[int, int, int],\n",
        "        time_dim: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        drop: float = 0.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        drop_path: float | list[float] = 0.0,\n",
        "        downsample: type[PatchMerging3D] | None = None,\n",
        "        upsample: type[PatchSplitting3D] | None = None,\n",
        "        scale_bias: float = 0.0,\n",
        "        lora_steps: int = 40,\n",
        "        lora_mode: LoRAMode = \"single\",\n",
        "        use_lora: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialise.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            depth (int): Number of blocks.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            ws (tuple[int, int, int]): Window size.\n",
        "            time_dim (int): Dimension of the lead time embedding.\n",
        "            mlp_ratio (float): Hidden layer dimensionality divided by that of the input for all\n",
        "                MLPs. Defaults to `4.0`.\n",
        "            qkv_bias (bool): If `True`, add a learnable bias to the query, key, and value. Defaults\n",
        "                to `True`.\n",
        "            drop (float): Drop-out rate. Defaults to `0.0`.\n",
        "            attn_drop (float): Attention drop-out rate. Defaults to `0.0`.\n",
        "            drop_path (float): Stochastic depth rate. Defaults to `0.0`.\n",
        "            downsample (PatchMerging3D, optional): Downsampling layer. Defaults to no downsampling.\n",
        "            upsample (PatchSplitting3D, optional): Upsampling layer. Defaults to no upsampling.\n",
        "            scale_bias (float, optional): Scale bias for\n",
        "                :class:`aurora.model.film.AdaptiveLayerNorm`. Default: 0\n",
        "            lora_steps (int, optional): Maximum number of LoRA roll-out steps. Defaults to `40`.\n",
        "            lora_mode (str, optional): Mode. `\"single\"` uses the same LoRA for all roll-out steps,\n",
        "                and `\"all\"` uses a different LoRA for every roll-out step. Defaults to `\"single\"`.\n",
        "            use_lora (bool): Enable LoRA. By default, LoRA is disabled.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if downsample is not None and upsample is not None:\n",
        "            raise ValueError(\"Cannot set both `downsample` and `upsample`.\")\n",
        "\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Swin3DTransformerBlock(\n",
        "                    dim=dim,\n",
        "                    num_heads=num_heads,\n",
        "                    window_size=ws,\n",
        "                    shift_size=(\n",
        "                        (0, 0, 0) if (i % 2 == 0) else (ws[0] // 2, ws[1] // 2, ws[2] // 2)\n",
        "                    ),\n",
        "                    time_dim=time_dim,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    drop=drop,\n",
        "                    attn_drop=attn_drop,\n",
        "                    drop_path=(drop_path[i] if isinstance(drop_path, list) else drop_path),\n",
        "                    scale_bias=scale_bias,\n",
        "                    use_lora=use_lora,\n",
        "                    lora_steps=lora_steps,\n",
        "                    lora_mode=lora_mode,\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample: PatchMerging3D | None = downsample(dim=dim)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        if upsample is not None:\n",
        "            self.upsample: PatchSplitting3D | None = upsample(dim=dim)\n",
        "        else:\n",
        "            self.upsample = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        c: torch.Tensor,\n",
        "        res: tuple[int, int, int],\n",
        "        crop: tuple[int, int, int] = (0, 0, 0),\n",
        "        rollout_step: int = 0,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
        "        \"\"\"Run the basic layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tokens of shape `(B, L, D)`.\n",
        "            c (torch.Tensor): Conditioning context of shape `(B, D)`.\n",
        "            res (tuple[int, int, int]): Resolution of the input `x`.\n",
        "            crop (tuple[int, int, int]): Cropping for every dimension.\n",
        "            rollout_step (int): Roll-out step.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tokens.\n",
        "        \"\"\"\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, c, res, rollout_step)\n",
        "        if self.downsample is not None:\n",
        "            x_scaled = self.downsample(x, res)\n",
        "            return x_scaled, x\n",
        "        if self.upsample is not None:\n",
        "            x_scaled = self.upsample(x, res, crop)\n",
        "            return x_scaled, x\n",
        "        return x, None\n",
        "\n",
        "    def init_respostnorm(self):\n",
        "        \"\"\"Initialise the post-normalisation layers in the residual connection of the windowed\n",
        "        attention mechanism.\"\"\"\n",
        "        for blk in self.blocks:\n",
        "            blk.norm1.init_weights()\n",
        "            blk.norm2.init_weights()\n",
        "\n",
        "\n",
        "class Basic3DEncoderLayer(BasicLayer3D):\n",
        "    \"\"\"A basic 3D Swin Transformer encoder layer. Used for FSDP, which requires a subclass.\"\"\"\n",
        "\n",
        "\n",
        "class Basic3DDecoderLayer(BasicLayer3D):\n",
        "    \"\"\"A basic 3D Swin Transformer decoder layer. Used for FSDP, which requires a subclass.\"\"\"\n",
        "\n",
        "\n",
        "class Swin3DTransformerBackbone(nn.Module):\n",
        "    \"\"\"Swin 3D Transformer backbone.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 96,\n",
        "        encoder_depths: tuple[int, ...] = (2, 2, 6, 2),\n",
        "        encoder_num_heads: tuple[int, ...] = (3, 6, 12, 24),\n",
        "        decoder_depths: tuple[int, ...] = (2, 6, 2, 2),\n",
        "        decoder_num_heads: tuple[int, ...] = (24, 12, 6, 3),\n",
        "        window_size: int | tuple[int, int, int] = 7,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        drop_rate: float = 0.0,\n",
        "        attn_drop_rate: float = 0.1,\n",
        "        drop_path_rate: float = 0.1,\n",
        "        lora_steps: int = 40,\n",
        "        lora_mode: LoRAMode = \"single\",\n",
        "        use_lora: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim (int): Patch embedding dimension. Default to `96`.\n",
        "            encoder_depths (tuple[int, ...]): Number of blocks in each encoder layer. Defaults to\n",
        "                `(2, 2, 6, 2)`.\n",
        "            encoder_num_heads (tuple[int, ...]): Number of attention heads in each encoder layer.\n",
        "                Default to `(3, 6, 12, 24)`.\n",
        "            decoder_depths (tuple[int, ...]): Number of blocks in each decoder layer. Defaults to\n",
        "                `(2, 6, 2, 2)`.\n",
        "            decoder_num_heads (tuple[int, ...]): Number of attention heads in each decoder layer.\n",
        "                Defaults to `(24, 12, 6, 3)`.\n",
        "            window_size (int | tuple[int, int, int]): Window size. Defaults to `7`.\n",
        "            mlp_ratio (float): Hidden layer dimensionality divided by that of the input for all\n",
        "                MLPs. Defaults to `4.0`.\n",
        "            qkv_bias (bool): If `True`, add a learnable bias to the query, key, and value. Defaults\n",
        "                to `True`.\n",
        "            drop_rate (float): Drop-out rate. Defaults to `0.0`.\n",
        "            attn_drop_rate (float): Attention drop-out rate. Defaults to `0.1`.\n",
        "            drop_path_rate (float): Stochastic depth rate. Defaults to `0.1`.\n",
        "            lora_steps (int, optional): Maximum number of LoRA roll-out steps. Defaults to `40`.\n",
        "            lora_mode (str, optional): Mode. `\"single\"` uses the same LoRA for all roll-out steps,\n",
        "                and `\"all\"` uses a different LoRA for every roll-out step. Defaults to `\"single\"`.\n",
        "            use_lora (bool): Enable LoRA. By default, LoRA is disabled.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.window_size = to_3tuple(window_size)\n",
        "        self.num_encoder_layers = len(encoder_depths)\n",
        "        self.num_decoder_layers = len(decoder_depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # Time embedding MLP\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim, bias=True),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(embed_dim, embed_dim, bias=True),\n",
        "        )\n",
        "\n",
        "        assert sum(encoder_depths) == sum(decoder_depths)\n",
        "        dpr: list[float] = [\n",
        "            x.item() for x in torch.linspace(0, drop_path_rate, sum(encoder_depths))\n",
        "        ]\n",
        "\n",
        "        # Build encoder layers.\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_encoder_layers):\n",
        "            layer = Basic3DEncoderLayer(\n",
        "                dim=int(embed_dim * 2**i_layer),\n",
        "                depth=encoder_depths[i_layer],\n",
        "                num_heads=encoder_num_heads[i_layer],\n",
        "                ws=self.window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                time_dim=embed_dim,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(encoder_depths[:i_layer]) : sum(encoder_depths[: i_layer + 1])],\n",
        "                downsample=(PatchMerging3D if (i_layer < self.num_encoder_layers - 1) else None),\n",
        "                use_lora=use_lora,\n",
        "                lora_steps=lora_steps,\n",
        "                lora_mode=lora_mode,\n",
        "            )\n",
        "            self.encoder_layers.append(layer)\n",
        "\n",
        "        # Build decoder layers.\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_decoder_layers):\n",
        "            exponent = self.num_decoder_layers - i_layer - 1\n",
        "            layer = Basic3DDecoderLayer(\n",
        "                dim=int(embed_dim * 2**exponent),\n",
        "                depth=decoder_depths[i_layer],\n",
        "                num_heads=decoder_num_heads[i_layer],\n",
        "                ws=self.window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                time_dim=embed_dim,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(decoder_depths[:i_layer]) : sum(decoder_depths[: i_layer + 1])],\n",
        "                upsample=(PatchSplitting3D if (i_layer < self.num_decoder_layers - 1) else None),\n",
        "                use_lora=use_lora,\n",
        "                lora_steps=lora_steps,\n",
        "                lora_mode=lora_mode,\n",
        "            )\n",
        "            self.decoder_layers.append(layer)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "        # This must overwrite the initialisation of `AdaptiveLayerNorm` by\n",
        "        # `self.apply(init_weights)` above, so should be called afterwards.\n",
        "        for bly in self.encoder_layers:\n",
        "            bly.init_respostnorm()\n",
        "        for bly in self.decoder_layers:\n",
        "            bly.init_respostnorm()\n",
        "\n",
        "    def get_encoder_specs(\n",
        "        self, patch_res: tuple[int, int, int]\n",
        "    ) -> tuple[list[tuple[int, int, int]], list[tuple[int, int, int]]]:\n",
        "        \"\"\"Gets the input resolution and output padding of each encoder layer.\"\"\"\n",
        "        all_res = [patch_res]\n",
        "        padded_outs = []\n",
        "        for _ in range(1, self.num_encoder_layers):\n",
        "            C, H, W = all_res[-1]\n",
        "            pad_H, pad_W = H % 2, W % 2\n",
        "            # The C dimension is never halved because it's tiny compared to H and W.\n",
        "            padded_outs.append((0, pad_H, pad_W))\n",
        "            all_res.append((C, (H + pad_H) // 2, (W + pad_W) // 2))\n",
        "\n",
        "        padded_outs.append((0, 0, 0))\n",
        "        return all_res, padded_outs\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        lead_time: timedelta,\n",
        "        rollout_step: int,\n",
        "        patch_res: tuple[int, int, int],\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Run the backbone.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tokens of shape `(B, L, D)`.\n",
        "            lead_time (datetime.timedelta): Lead time.\n",
        "            rollout_step (int): Roll-out step.\n",
        "            patch_res (tuple[int, int, int]): Patch resolution of the form `(C, H, W)`.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tokens of shape `(B, L, D)`.\n",
        "        \"\"\"\n",
        "        _msg = \"Input shape does not match patch size.\"\n",
        "        assert x.shape[1] == patch_res[0] * patch_res[1] * patch_res[2], _msg\n",
        "\n",
        "        # It's costly to pad across the level dimension, so we should not even though our model\n",
        "        # supports it.\n",
        "        _msg = f\"Patch height ({patch_res[0]}) must be divisible by ws[0] ({self.window_size[0]})\"\n",
        "        assert patch_res[0] % self.window_size[0] == 0, _msg\n",
        "\n",
        "        all_enc_res, padded_outs = self.get_encoder_specs(patch_res)\n",
        "\n",
        "        lead_hours = lead_time / timedelta(hours=1)\n",
        "        lead_times = lead_hours * torch.ones(x.shape[0], dtype=torch.float32, device=x.device)\n",
        "        c = self.time_mlp(lead_time_expansion(lead_times, self.embed_dim).to(dtype=x.dtype))\n",
        "\n",
        "        skips = []\n",
        "        for i, layer in enumerate(self.encoder_layers):\n",
        "            x, x_unscaled = layer(x, c, all_enc_res[i], rollout_step=rollout_step)\n",
        "            skips.append(x_unscaled)\n",
        "        for i, layer in enumerate(self.decoder_layers):\n",
        "            index = self.num_decoder_layers - i - 1\n",
        "            x, _ = layer(\n",
        "                x,\n",
        "                c,\n",
        "                all_enc_res[index],\n",
        "                padded_outs[index - 1],\n",
        "                rollout_step=rollout_step,\n",
        "            )\n",
        "\n",
        "            if 0 < i < self.num_decoder_layers - 1:\n",
        "                # For the intermediate stages, we use additive skip connections.\n",
        "                x = x + skips[index - 1]\n",
        "            elif i == self.num_decoder_layers - 1:\n",
        "                # For the last stage, we perform concatentation like in Pangu.\n",
        "                x = torch.cat([x, skips[0]], dim=-1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "rDmFb7wTI4Kg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------\n",
        "B = 2           # Batch size\n",
        "C = 4           # Latent vertical levels\n",
        "H = 32          # Height\n",
        "W = 64          # Width\n",
        "D = 192         # Embedding dimension\n",
        "lead_time = timedelta(hours=6)\n",
        "patch_res = (C, H, W)\n",
        "lead_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGsswOpGI9Fu",
        "outputId": "255b255b-13a6-469c-eb73-7cbf144c5f57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.timedelta(seconds=21600)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Swin3DTransformerBackbone(\n",
        "    embed_dim=D,\n",
        "    encoder_depths=(2, 2, 2),\n",
        "    encoder_num_heads=(3, 6, 12),\n",
        "    decoder_depths=(2, 2, 2),\n",
        "    decoder_num_heads=(12, 6, 3),\n",
        "    window_size=(2, 12, 6),\n",
        "    use_lora=False,\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# INPUT\n",
        "# ------------------------------\n",
        "x = torch.randn(B, C, H, W, D)\n",
        "x_flat = x.view(B, C * H * W, D)"
      ],
      "metadata": {
        "id": "WrOAk2yLI_Ti"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# FORWARD PASS\n",
        "# ------------------------------\n",
        "out_flat = model(x_flat, lead_time=lead_time, rollout_step=0, patch_res=patch_res)\n",
        "out = out_flat.view(B, C, H, W, -1)  # output will have last dim = 2D due to concat\n",
        "\n",
        "# ------------------------------\n",
        "# TARGET\n",
        "# ------------------------------\n",
        "target = torch.randn(B, C, H, W, out.shape[-1])  # match shape of output\n",
        "\n",
        "# ------------------------------\n",
        "# LOSS\n",
        "# ------------------------------\n",
        "loss = torch.nn.functional.mse_loss(out, target)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")\n",
        "print(f\"Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M87-7YWCJEyO",
        "outputId": "3f253ef6-d1fc-4a9a-aa95-65bdeb92728a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 32, 64, 192])\n",
            "Output shape: torch.Size([2, 4, 32, 64, 384])\n",
            "Loss: 2.0403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "projector = torch.nn.Linear(384, 192)  # After model output\n",
        "out_flat = projector(out_flat)         # Reduce to 192\n",
        "out = out_flat.view(B, C, H, W, 192)"
      ],
      "metadata": {
        "id": "jow2RQ_UJNrG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1BW-abNJpCZ",
        "outputId": "77defa33-26e2-4d9d-bf4e-e3a2008054c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 32, 64, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dC55ONF0Jqvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}