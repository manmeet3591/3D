{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3D Squeeze and Excitation Modules\n",
        "*****************************\n",
        "3D Extensions of the following 2D squeeze and excitation blocks:\n",
        "    1. `Channel Squeeze and Excitation <https://arxiv.org/abs/1709.01507>`_\n",
        "    2. `Spatial Squeeze and Excitation <https://arxiv.org/abs/1803.02579>`_\n",
        "    3. `Channel and Spatial Squeeze and Excitation <https://arxiv.org/abs/1803.02579>`_\n",
        "New Project & Excite block, designed specifically for 3D inputs\n",
        "    'quote'\n",
        "    Coded by -- Anne-Marie Rickmann (https://github.com/arickm)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ChannelSELayer3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D extension of Squeeze-and-Excitation (SE) block described in:\n",
        "        *Hu et al., Squeeze-and-Excitation Networks, arXiv:1709.01507*\n",
        "        *Zhu et al., AnatomyNet, arXiv:arXiv:1808.05238*\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, reduction_ratio=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_channels (int): No of input channels\n",
        "            reduction_ratio (int): By how much should the num_channels should be reduced\n",
        "        \"\"\"\n",
        "        super(ChannelSELayer3D, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        num_channels_reduced = num_channels // reduction_ratio\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
        "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_channels, D, H, W = x.size()\n",
        "        # Average along each channel\n",
        "        squeeze_tensor = self.avg_pool(x)\n",
        "\n",
        "        # channel excitation\n",
        "        fc_out_1 = self.relu(self.fc1(squeeze_tensor.view(batch_size, num_channels)))\n",
        "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
        "\n",
        "        output_tensor = torch.mul(x, fc_out_2.view(batch_size, num_channels, 1, 1, 1))\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "class SpatialSELayer3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D extension of SE block -- squeezing spatially and exciting channel-wise described in:\n",
        "        *Roy et al., Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks, MICCAI 2018*\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_channels (int): No of input channels\n",
        "        \"\"\"\n",
        "        super(SpatialSELayer3D, self).__init__()\n",
        "        self.conv = nn.Conv3d(num_channels, 1, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, weights=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            weights (torch.Tensor): weights for few shot learning\n",
        "            x: X, shape = (batch_size, num_channels, D, H, W)\n",
        "\n",
        "        Returns:\n",
        "            (torch.Tensor): output_tensor\n",
        "        \"\"\"\n",
        "        # channel squeeze\n",
        "        batch_size, channel, D, H, W = x.size()\n",
        "\n",
        "        if weights:\n",
        "            weights = weights.view(1, channel, 1, 1)\n",
        "            out = F.conv2d(x, weights)\n",
        "        else:\n",
        "            out = self.conv(x)\n",
        "\n",
        "        squeeze_tensor = self.sigmoid(out)\n",
        "\n",
        "        # spatial excitation\n",
        "        output_tensor = torch.mul(x, squeeze_tensor.view(batch_size, 1, D, H, W))\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "class ChannelSpatialSELayer3D(nn.Module):\n",
        "    \"\"\"\n",
        "       3D extension of concurrent spatial and channel squeeze & excitation:\n",
        "           *Roy et al., Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks, arXiv:1803.02579*\n",
        "       \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, reduction_ratio=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_channels (int): No of input channels\n",
        "            reduction_ratio (int): By how much should the num_channels should be reduced\n",
        "        \"\"\"\n",
        "        super(ChannelSpatialSELayer3D, self).__init__()\n",
        "        self.cSE = ChannelSELayer3D(num_channels, reduction_ratio)\n",
        "        self.sSE = SpatialSELayer3D(num_channels)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output_tensor = torch.max(self.cSE(input_tensor), self.sSE(input_tensor))\n",
        "        return output_tensor"
      ],
      "metadata": {
        "id": "YUhxH6nAV3yw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# from pytorch3dunet.unet3d.se import ChannelSELayer3D, ChannelSpatialSELayer3D, SpatialSELayer3D\n",
        "\n",
        "\n",
        "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding,\n",
        "                dropout_prob, is3d):\n",
        "    \"\"\"\n",
        "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
        "    and optional batchnorm/groupnorm.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        kernel_size(int or tuple): size of the convolving kernel\n",
        "        order (string): order of things, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'gcr' -> groupnorm + conv + ReLU\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "            'bcr' -> batchnorm + conv + ReLU\n",
        "            'cbrd' -> conv + batchnorm + ReLU + dropout\n",
        "            'cbrD' -> conv + batchnorm + ReLU + dropout2d\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "        dropout_prob (float): dropout probability\n",
        "        is3d (bool): is3d (bool): if True use Conv3d, otherwise use Conv2d\n",
        "    Return:\n",
        "        list of tuple (name, module)\n",
        "    \"\"\"\n",
        "    assert 'c' in order, \"Conv layer MUST be present\"\n",
        "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
        "\n",
        "    modules = []\n",
        "    for i, char in enumerate(order):\n",
        "        if char == 'r':\n",
        "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
        "        elif char == 'l':\n",
        "            modules.append(('LeakyReLU', nn.LeakyReLU(inplace=True)))\n",
        "        elif char == 'e':\n",
        "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
        "        elif char == 'c':\n",
        "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
        "            bias = not ('g' in order or 'b' in order)\n",
        "            if is3d:\n",
        "                conv = nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
        "            else:\n",
        "                conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
        "\n",
        "            modules.append(('conv', conv))\n",
        "        elif char == 'g':\n",
        "            is_before_conv = i < order.index('c')\n",
        "            if is_before_conv:\n",
        "                num_channels = in_channels\n",
        "            else:\n",
        "                num_channels = out_channels\n",
        "\n",
        "            # use only one group if the given number of groups is greater than the number of channels\n",
        "            if num_channels < num_groups:\n",
        "                num_groups = 1\n",
        "\n",
        "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
        "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
        "        elif char == 'b':\n",
        "            is_before_conv = i < order.index('c')\n",
        "            if is3d:\n",
        "                bn = nn.BatchNorm3d\n",
        "            else:\n",
        "                bn = nn.BatchNorm2d\n",
        "\n",
        "            if is_before_conv:\n",
        "                modules.append(('batchnorm', bn(in_channels)))\n",
        "            else:\n",
        "                modules.append(('batchnorm', bn(out_channels)))\n",
        "        elif char == 'd':\n",
        "            modules.append(('dropout', nn.Dropout(p=dropout_prob)))\n",
        "        elif char == 'D':\n",
        "            modules.append(('dropout2d', nn.Dropout2d(p=dropout_prob)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c', 'd', 'D']\")\n",
        "\n",
        "    return modules\n",
        "\n",
        "\n",
        "class SingleConv(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
        "    of operations can be specified via the `order` parameter\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "        order (string): determines the order of layers, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'crg' -> conv + ReLU + groupnorm\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding\n",
        "        dropout_prob (float): dropout probability, default 0.1\n",
        "        is3d (bool): if True use Conv3d, otherwise use Conv2d\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='gcr', num_groups=8,\n",
        "                 padding=1, dropout_prob=0.1, is3d=True):\n",
        "        super(SingleConv, self).__init__()\n",
        "\n",
        "        for name, module in create_conv(in_channels, out_channels, kernel_size, order,\n",
        "                                        num_groups, padding, dropout_prob, is3d):\n",
        "            self.add_module(name, module)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Sequential):\n",
        "    \"\"\"\n",
        "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
        "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
        "    This can be changed however by providing the 'order' argument, e.g. in order\n",
        "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
        "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
        "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "        order (string): determines the order of layers, e.g.\n",
        "            'cr' -> conv + ReLU\n",
        "            'crg' -> conv + ReLU + groupnorm\n",
        "            'cl' -> conv + LeakyReLU\n",
        "            'ce' -> conv + ELU\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "        upscale (int): number of the convolution to upscale in encoder if DoubleConv, default: 2\n",
        "        dropout_prob (float or tuple): dropout probability for each convolution, default 0.1\n",
        "        is3d (bool): if True use Conv3d instead of Conv2d layers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='gcr',\n",
        "                 num_groups=8, padding=1, upscale=2, dropout_prob=0.1, is3d=True):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        if encoder:\n",
        "            # we're in the encoder path\n",
        "            conv1_in_channels = in_channels\n",
        "            if upscale == 1:\n",
        "                conv1_out_channels = out_channels\n",
        "            else:\n",
        "                conv1_out_channels = out_channels // 2\n",
        "            if conv1_out_channels < in_channels:\n",
        "                conv1_out_channels = in_channels\n",
        "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
        "        else:\n",
        "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
        "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
        "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
        "\n",
        "        # check if dropout_prob is a tuple and if so\n",
        "        # split it for different dropout probabilities for each convolution.\n",
        "        if isinstance(dropout_prob, list) or isinstance(dropout_prob, tuple):\n",
        "            dropout_prob1 = dropout_prob[0]\n",
        "            dropout_prob2 = dropout_prob[1]\n",
        "        else:\n",
        "            dropout_prob1 = dropout_prob2 = dropout_prob\n",
        "\n",
        "        # conv1\n",
        "        self.add_module('SingleConv1',\n",
        "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
        "                                   padding=padding, dropout_prob=dropout_prob1, is3d=is3d))\n",
        "        # conv2\n",
        "        self.add_module('SingleConv2',\n",
        "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
        "                                   padding=padding, dropout_prob=dropout_prob2, is3d=is3d))\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block that can be used instead of standard DoubleConv in the Encoder module.\n",
        "    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n",
        "\n",
        "    Notice we use ELU instead of ReLU (order='cge') and put non-linearity after the groupnorm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, is3d=True, **kwargs):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            # conv1x1 for increasing the number of channels\n",
        "            if is3d:\n",
        "                self.conv1 = nn.Conv3d(in_channels, out_channels, 1)\n",
        "            else:\n",
        "                self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.conv1 = nn.Identity()\n",
        "\n",
        "        # residual block\n",
        "        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups,\n",
        "                                is3d=is3d)\n",
        "        # remove non-linearity from the 3rd convolution since it's going to be applied after adding the residual\n",
        "        n_order = order\n",
        "        for c in 'rel':\n",
        "            n_order = n_order.replace(c, '')\n",
        "        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n",
        "                                num_groups=num_groups, is3d=is3d)\n",
        "\n",
        "        # create non-linearity separately\n",
        "        if 'l' in order:\n",
        "            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        elif 'e' in order:\n",
        "            self.non_linearity = nn.ELU(inplace=True)\n",
        "        else:\n",
        "            self.non_linearity = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply first convolution to bring the number of channels to out_channels\n",
        "        residual = self.conv1(x)\n",
        "\n",
        "        # residual block\n",
        "        out = self.conv2(residual)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.non_linearity(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNetBlockSE(ResNetBlock):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, se_module='scse', **kwargs):\n",
        "        super(ResNetBlockSE, self).__init__(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, order=order,\n",
        "            num_groups=num_groups, **kwargs)\n",
        "        assert se_module in ['scse', 'cse', 'sse']\n",
        "        if se_module == 'scse':\n",
        "            self.se_module = ChannelSpatialSELayer3D(num_channels=out_channels, reduction_ratio=1)\n",
        "        elif se_module == 'cse':\n",
        "            self.se_module = ChannelSELayer3D(num_channels=out_channels, reduction_ratio=1)\n",
        "        elif se_module == 'sse':\n",
        "            self.se_module = SpatialSELayer3D(num_channels=out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = super().forward(x)\n",
        "        out = self.se_module(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A single module from the encoder path consisting of the optional max\n",
        "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
        "    from the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
        "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
        "    a basic module (DoubleConv or ResNetBlock).\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
        "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
        "        pool_kernel_size (int or tuple): the size of the window\n",
        "        pool_type (str): pooling layer: 'max' or 'avg'\n",
        "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
        "        conv_layer_order (string): determines the order of layers\n",
        "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "        upscale (int): number of the convolution to upscale in encoder if DoubleConv, default: 2\n",
        "        dropout_prob (float or tuple): dropout probability, default 0.1\n",
        "        is3d (bool): use 3d or 2d convolutions/pooling operation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
        "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='gcr',\n",
        "                 num_groups=8, padding=1, upscale=2, dropout_prob=0.1, is3d=True):\n",
        "        super(Encoder, self).__init__()\n",
        "        assert pool_type in ['max', 'avg']\n",
        "        if apply_pooling:\n",
        "            if pool_type == 'max':\n",
        "                if is3d:\n",
        "                    self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
        "                else:\n",
        "                    self.pooling = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
        "            else:\n",
        "                if is3d:\n",
        "                    self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
        "                else:\n",
        "                    self.pooling = nn.AvgPool2d(kernel_size=pool_kernel_size)\n",
        "        else:\n",
        "            self.pooling = None\n",
        "\n",
        "        self.basic_module = basic_module(in_channels, out_channels,\n",
        "                                         encoder=True,\n",
        "                                         kernel_size=conv_kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups,\n",
        "                                         padding=padding,\n",
        "                                         upscale=upscale,\n",
        "                                         dropout_prob=dropout_prob,\n",
        "                                         is3d=is3d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pooling is not None:\n",
        "            x = self.pooling(x)\n",
        "        x = self.basic_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A single module for decoder path consisting of the upsampling layer\n",
        "    (either learned ConvTranspose3d or nearest neighbor interpolation)\n",
        "    followed by a basic module (DoubleConv or ResNetBlock).\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
        "        scale_factor (int or tuple): used as the multiplier for the image H/W/D in\n",
        "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
        "            from the corresponding encoder\n",
        "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
        "        conv_layer_order (string): determines the order of layers\n",
        "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "        upsample (str): algorithm used for upsampling:\n",
        "            InterpolateUpsampling:   'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'\n",
        "            TransposeConvUpsampling: 'deconv'\n",
        "            No upsampling:           None\n",
        "            Default: 'default' (chooses automatically)\n",
        "        dropout_prob (float or tuple): dropout probability, default 0.1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=2, basic_module=DoubleConv,\n",
        "                 conv_layer_order='gcr', num_groups=8, padding=1, upsample='default',\n",
        "                 dropout_prob=0.1, is3d=True):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # perform concat joining per default\n",
        "        concat = True\n",
        "\n",
        "        # don't adapt channels after join operation\n",
        "        adapt_channels = False\n",
        "\n",
        "        if upsample is not None and upsample != 'none':\n",
        "            if upsample == 'default':\n",
        "                if basic_module == DoubleConv:\n",
        "                    upsample = 'nearest'  # use nearest neighbor interpolation for upsampling\n",
        "                    concat = True  # use concat joining\n",
        "                    adapt_channels = False  # don't adapt channels\n",
        "                elif basic_module == ResNetBlock or basic_module == ResNetBlockSE:\n",
        "                    upsample = 'deconv'  # use deconvolution upsampling\n",
        "                    concat = False  # use summation joining\n",
        "                    adapt_channels = True  # adapt channels after joining\n",
        "\n",
        "            # perform deconvolution upsampling if mode is deconv\n",
        "            if upsample == 'deconv':\n",
        "                self.upsampling = TransposeConvUpsampling(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                          kernel_size=conv_kernel_size, scale_factor=scale_factor,\n",
        "                                                          is3d=is3d)\n",
        "            else:\n",
        "                self.upsampling = InterpolateUpsampling(mode=upsample)\n",
        "        else:\n",
        "            # no upsampling\n",
        "            self.upsampling = NoUpsampling()\n",
        "            # concat joining\n",
        "            self.joining = partial(self._joining, concat=True)\n",
        "\n",
        "        # perform joining operation\n",
        "        self.joining = partial(self._joining, concat=concat)\n",
        "\n",
        "        # adapt the number of in_channels for the ResNetBlock\n",
        "        if adapt_channels is True:\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.basic_module = basic_module(in_channels, out_channels,\n",
        "                                         encoder=False,\n",
        "                                         kernel_size=conv_kernel_size,\n",
        "                                         order=conv_layer_order,\n",
        "                                         num_groups=num_groups,\n",
        "                                         padding=padding,\n",
        "                                         dropout_prob=dropout_prob,\n",
        "                                         is3d=is3d)\n",
        "\n",
        "    def forward(self, encoder_features, x):\n",
        "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
        "        x = self.joining(encoder_features, x)\n",
        "        x = self.basic_module(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _joining(encoder_features, x, concat):\n",
        "        if concat:\n",
        "            return torch.cat((encoder_features, x), dim=1)\n",
        "        else:\n",
        "            return encoder_features + x\n",
        "\n",
        "\n",
        "def create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding,\n",
        "                    conv_upscale, dropout_prob,\n",
        "                    layer_order, num_groups, pool_kernel_size, is3d):\n",
        "    # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
        "    encoders = []\n",
        "    for i, out_feature_num in enumerate(f_maps):\n",
        "        if i == 0:\n",
        "            # apply conv_coord only in the first encoder if any\n",
        "            encoder = Encoder(in_channels, out_feature_num,\n",
        "                              apply_pooling=False,  # skip pooling in the firs encoder\n",
        "                              basic_module=basic_module,\n",
        "                              conv_layer_order=layer_order,\n",
        "                              conv_kernel_size=conv_kernel_size,\n",
        "                              num_groups=num_groups,\n",
        "                              padding=conv_padding,\n",
        "                              upscale=conv_upscale,\n",
        "                              dropout_prob=dropout_prob,\n",
        "                              is3d=is3d)\n",
        "        else:\n",
        "            encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
        "                              basic_module=basic_module,\n",
        "                              conv_layer_order=layer_order,\n",
        "                              conv_kernel_size=conv_kernel_size,\n",
        "                              num_groups=num_groups,\n",
        "                              pool_kernel_size=pool_kernel_size,\n",
        "                              padding=conv_padding,\n",
        "                              upscale=conv_upscale,\n",
        "                              dropout_prob=dropout_prob,\n",
        "                              is3d=is3d)\n",
        "\n",
        "        encoders.append(encoder)\n",
        "\n",
        "    return nn.ModuleList(encoders)\n",
        "\n",
        "\n",
        "def create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order,\n",
        "                    num_groups, upsample, dropout_prob, is3d):\n",
        "    # create decoder path consisting of the Decoder modules. The length of the decoder list is equal to `len(f_maps) - 1`\n",
        "    decoders = []\n",
        "    reversed_f_maps = list(reversed(f_maps))\n",
        "    for i in range(len(reversed_f_maps) - 1):\n",
        "        if basic_module == DoubleConv and upsample != 'deconv':\n",
        "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
        "        else:\n",
        "            in_feature_num = reversed_f_maps[i]\n",
        "\n",
        "        out_feature_num = reversed_f_maps[i + 1]\n",
        "\n",
        "        decoder = Decoder(in_feature_num, out_feature_num,\n",
        "                          basic_module=basic_module,\n",
        "                          conv_layer_order=layer_order,\n",
        "                          conv_kernel_size=conv_kernel_size,\n",
        "                          num_groups=num_groups,\n",
        "                          padding=conv_padding,\n",
        "                          upsample=upsample,\n",
        "                          dropout_prob=dropout_prob,\n",
        "                          is3d=is3d)\n",
        "        decoders.append(decoder)\n",
        "    return nn.ModuleList(decoders)\n",
        "\n",
        "\n",
        "class AbstractUpsampling(nn.Module):\n",
        "    \"\"\"\n",
        "    Abstract class for upsampling. A given implementation should upsample a given 5D input tensor using either\n",
        "    interpolation or learned transposed convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, upsample):\n",
        "        super(AbstractUpsampling, self).__init__()\n",
        "        self.upsample = upsample\n",
        "\n",
        "    def forward(self, encoder_features, x):\n",
        "        # get the spatial dimensions of the output given the encoder_features\n",
        "        output_size = encoder_features.size()[2:]\n",
        "        # upsample the input and return\n",
        "        return self.upsample(x, output_size)\n",
        "\n",
        "\n",
        "class InterpolateUpsampling(AbstractUpsampling):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        mode (str): algorithm used for upsampling:\n",
        "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
        "            used only if transposed_conv is False\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='nearest'):\n",
        "        upsample = partial(self._interpolate, mode=mode)\n",
        "        super().__init__(upsample)\n",
        "\n",
        "    @staticmethod\n",
        "    def _interpolate(x, size, mode):\n",
        "        return F.interpolate(x, size=size, mode=mode)\n",
        "\n",
        "\n",
        "class TransposeConvUpsampling(AbstractUpsampling):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        in_channels (int): number of input channels for transposed conv\n",
        "            used only if transposed_conv is True\n",
        "        out_channels (int): number of output channels for transpose conv\n",
        "            used only if transposed_conv is True\n",
        "        kernel_size (int or tuple): size of the convolving kernel\n",
        "            used only if transposed_conv is True\n",
        "        scale_factor (int or tuple): stride of the convolution\n",
        "            used only if transposed_conv is True\n",
        "        is3d (bool): if True use ConvTranspose3d, otherwise use ConvTranspose2d\n",
        "    \"\"\"\n",
        "\n",
        "    class Upsample(nn.Module):\n",
        "        \"\"\"\n",
        "        Workaround the 'ValueError: requested an output size...' in the `_output_padding` method in\n",
        "        transposed convolution. It performs transposed conv followed by the interpolation to the correct size if necessary.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, conv_transposed, is3d):\n",
        "            super().__init__()\n",
        "            self.conv_transposed = conv_transposed\n",
        "            self.is3d = is3d\n",
        "\n",
        "        def forward(self, x, size):\n",
        "            x = self.conv_transposed(x)\n",
        "            return F.interpolate(x, size=size)\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, scale_factor=2, is3d=True):\n",
        "        # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
        "        if is3d is True:\n",
        "            conv_transposed = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                                                 stride=scale_factor, padding=1, bias=False)\n",
        "        else:\n",
        "            conv_transposed = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                                                 stride=scale_factor, padding=1, bias=False)\n",
        "        upsample = self.Upsample(conv_transposed, is3d)\n",
        "        super().__init__(upsample)\n",
        "\n",
        "\n",
        "class NoUpsampling(AbstractUpsampling):\n",
        "    def __init__(self):\n",
        "        super().__init__(self._no_upsampling)\n",
        "\n",
        "    @staticmethod\n",
        "    def _no_upsampling(x, size):\n",
        "        return x"
      ],
      "metadata": {
        "id": "QTZ-giAtZuux"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.color import label2rgb\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_dir):\n",
        "    \"\"\"Saves model and training parameters at '{checkpoint_dir}/last_checkpoint.pytorch'.\n",
        "    If is_best==True saves '{checkpoint_dir}/best_checkpoint.pytorch' as well.\n",
        "\n",
        "    Args:\n",
        "        state (dict): contains model's state_dict, optimizer's state_dict, epoch\n",
        "            and best evaluation metric value so far\n",
        "        is_best (bool): if True state contains the best model seen so far\n",
        "        checkpoint_dir (string): directory where the checkpoint are to be saved\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.mkdir(checkpoint_dir)\n",
        "\n",
        "    last_file_path = os.path.join(checkpoint_dir, 'last_checkpoint.pytorch')\n",
        "    torch.save(state, last_file_path)\n",
        "    if is_best:\n",
        "        best_file_path = os.path.join(checkpoint_dir, 'best_checkpoint.pytorch')\n",
        "        shutil.copyfile(last_file_path, best_file_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer=None,\n",
        "                    model_key='model_state_dict', optimizer_key='optimizer_state_dict'):\n",
        "    \"\"\"Loads model and training parameters from a given checkpoint_path\n",
        "    If optimizer is provided, loads optimizer's state_dict of as well.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (string): path to the checkpoint to be loaded\n",
        "        model (torch.nn.Module): model into which the parameters are to be copied\n",
        "        optimizer (torch.optim.Optimizer) optional: optimizer instance into\n",
        "            which the parameters are to be copied\n",
        "\n",
        "    Returns:\n",
        "        state\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise IOError(f\"Checkpoint '{checkpoint_path}' does not exist\")\n",
        "\n",
        "    state = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(state[model_key])\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(state[optimizer_key])\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def save_network_output(output_path, output, logger=None):\n",
        "    if logger is not None:\n",
        "        logger.info(f'Saving network output to: {output_path}...')\n",
        "    output = output.detach().cpu()[0]\n",
        "    with h5py.File(output_path, 'w') as f:\n",
        "        f.create_dataset('predictions', data=output, compression='gzip')\n",
        "\n",
        "\n",
        "loggers = {}\n",
        "\n",
        "\n",
        "def get_logger(name, level=logging.INFO):\n",
        "    global loggers\n",
        "    if loggers.get(name) is not None:\n",
        "        return loggers[name]\n",
        "    else:\n",
        "        logger = logging.getLogger(name)\n",
        "        logger.setLevel(level)\n",
        "        # Logging to console\n",
        "        stream_handler = logging.StreamHandler(sys.stdout)\n",
        "        formatter = logging.Formatter(\n",
        "            '%(asctime)s [%(threadName)s] %(levelname)s %(name)s - %(message)s')\n",
        "        stream_handler.setFormatter(formatter)\n",
        "        logger.addHandler(stream_handler)\n",
        "\n",
        "        loggers[name] = logger\n",
        "\n",
        "        return logger\n",
        "\n",
        "\n",
        "def get_number_of_learnable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "class RunningAverage:\n",
        "    \"\"\"Computes and stores the average\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.sum = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.count += n\n",
        "        self.sum += value * n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def number_of_features_per_level(init_channel_number, num_levels):\n",
        "    return [init_channel_number * 2 ** k for k in range(num_levels)]\n",
        "\n",
        "\n",
        "class TensorboardFormatter:\n",
        "    \"\"\"\n",
        "    Tensorboard formatters converts a given batch of images (be it input/output to the network or the target segmentation\n",
        "    image) to a series of images that can be displayed in tensorboard. This is the parent class for all tensorboard\n",
        "    formatters which ensures that returned images are in the 'CHW' format.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, skip_last_target=False, log_channelwise=False):\n",
        "        self.skip_last_target = skip_last_target\n",
        "        self.log_channelwise = log_channelwise\n",
        "\n",
        "    def __call__(self, name, batch):\n",
        "        \"\"\"\n",
        "        Transform a batch to a series of tuples of the form (tag, img), where `tag` corresponds to the image tag\n",
        "        and `img` is the image itself.\n",
        "\n",
        "        Args:\n",
        "             name (str): one of 'inputs'/'targets'/'predictions'\n",
        "             batch (torch.tensor): 4D or 5D torch tensor\n",
        "\n",
        "        Returns:\n",
        "            list[(str, np.ndarray)]: list of tuples of the form (tag, img)\n",
        "        \"\"\"\n",
        "\n",
        "        def _check_img(tag_img):\n",
        "            tag, img = tag_img\n",
        "\n",
        "            assert img.ndim == 2 or img.ndim == 3, 'Only 2D (HW) and 3D (CHW) images are accepted for display'\n",
        "\n",
        "            if img.ndim == 2:\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "            else:\n",
        "                C = img.shape[0]\n",
        "                assert C == 1 or C == 3, 'Only (1, H, W) or (3, H, W) images are supported'\n",
        "\n",
        "            return tag, img\n",
        "\n",
        "        tagged_images = self._process_batch(name, batch)\n",
        "\n",
        "        return list(map(_check_img, tagged_images))\n",
        "\n",
        "    def _process_batch(self, name, batch):\n",
        "        if name == 'targets' and self.skip_last_target:\n",
        "            batch = batch[:, :-1, ...]\n",
        "\n",
        "        tag_template = '{}/batch_{}/slice_{}'\n",
        "\n",
        "        tagged_images = []\n",
        "\n",
        "        if batch.ndim == 5:\n",
        "            # NCDHW\n",
        "            slice_idx = batch.shape[2] // 2  # get the middle slice\n",
        "            for batch_idx in range(batch.shape[0]):\n",
        "                if self.log_channelwise and name == 'predictions':\n",
        "                    tag_template = '{}/batch_{}/channel_{}/slice_{}'\n",
        "                    for channel_idx in range(batch.shape[1]):\n",
        "                        tag = tag_template.format(name, batch_idx, channel_idx, slice_idx)\n",
        "                        img = batch[batch_idx, channel_idx, slice_idx, ...]\n",
        "                        tagged_images.append((tag, self._normalize_img(img)))\n",
        "                else:\n",
        "                    tag = tag_template.format(name, batch_idx, slice_idx)\n",
        "                    if name in ['predictions', 'targets']:\n",
        "                        # for single channel predictions, just log the image\n",
        "                        if batch.shape[1] == 1:\n",
        "                            img = batch[batch_idx, :, slice_idx, ...]\n",
        "                            tagged_images.append((tag, self._normalize_img(img)))\n",
        "                        else:\n",
        "                            # predictions are probabilities so convert to label image\n",
        "                            img = batch[batch_idx].argmax(axis=0)\n",
        "                            # take the middle slice\n",
        "                            img = img[slice_idx, ...]\n",
        "                            # convert to label image\n",
        "                            img = label2rgb(img)\n",
        "                            img = img.transpose(2, 0, 1)\n",
        "                            tagged_images.append((tag, img))\n",
        "                    else:\n",
        "                        # handle input images\n",
        "                        if batch.shape[1] in [1, 3]:\n",
        "                            # if single channel or RGB image, log directly\n",
        "                            img = batch[batch_idx, :, slice_idx, ...]\n",
        "                            tagged_images.append((tag, self._normalize_img(img)))\n",
        "                        else:\n",
        "                            # log channelwise\n",
        "                            tag_template = '{}/batch_{}/channel_{}/slice_{}'\n",
        "                            for channel_idx in range(batch.shape[1]):\n",
        "                                tag = tag_template.format(name, batch_idx, channel_idx, slice_idx)\n",
        "                                img = batch[batch_idx, channel_idx, slice_idx, ...]\n",
        "                                tagged_images.append((tag, self._normalize_img(img)))\n",
        "\n",
        "        else:\n",
        "            # batch has no channel dim: NDHW\n",
        "            slice_idx = batch.shape[1] // 2  # get the middle slice\n",
        "            for batch_idx in range(batch.shape[0]):\n",
        "                tag = tag_template.format(name, batch_idx, slice_idx)\n",
        "                img = batch[batch_idx, slice_idx, ...]\n",
        "                # this is target segmentation so convert to label image\n",
        "                lbl = label2rgb(img)\n",
        "                lbl = lbl.transpose(2, 0, 1)\n",
        "                tagged_images.append((tag, lbl))\n",
        "\n",
        "        return tagged_images\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_img(img):\n",
        "        return np.nan_to_num((img - np.min(img)) / np.ptp(img))\n",
        "\n",
        "\n",
        "def _find_masks(batch, min_size=10):\n",
        "    \"\"\"Center the z-slice in the 'middle' of a given instance, given a batch of instances\n",
        "\n",
        "    Args:\n",
        "        batch (ndarray): 5d numpy tensor (NCDHW)\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for b in batch:\n",
        "        assert b.shape[0] == 1\n",
        "        patch = b[0]\n",
        "        z_sum = patch.sum(axis=(1, 2))\n",
        "        coords = np.where(z_sum > min_size)[0]\n",
        "        if len(coords) > 0:\n",
        "            ind = coords[len(coords) // 2]\n",
        "            result.append(b[:, ind:ind + 1, ...])\n",
        "        else:\n",
        "            ind = b.shape[1] // 2\n",
        "            result.append(b[:, ind:ind + 1, ...])\n",
        "\n",
        "    return np.stack(result, axis=0)\n",
        "\n",
        "\n",
        "def get_tensorboard_formatter(formatter_config):\n",
        "    if formatter_config is None:\n",
        "        return TensorboardFormatter()\n",
        "    return TensorboardFormatter(**formatter_config)\n",
        "\n",
        "\n",
        "def expand_as_one_hot(input, C, ignore_index=None):\n",
        "    \"\"\"\n",
        "    Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\n",
        "    It is assumed that the batch dimension is present.\n",
        "    Args:\n",
        "        input (torch.Tensor): 3D/4D input image\n",
        "        C (int): number of channels/labels\n",
        "        ignore_index (int): ignore index to be kept during the expansion\n",
        "    Returns:\n",
        "        4D/5D output torch.Tensor (NxCxSPATIAL)\n",
        "    \"\"\"\n",
        "    assert input.dim() == 4\n",
        "\n",
        "    # expand the input tensor to Nx1xSPATIAL before scattering\n",
        "    input = input.unsqueeze(1)\n",
        "    # create output tensor shape (NxCxSPATIAL)\n",
        "    shape = list(input.size())\n",
        "    shape[1] = C\n",
        "\n",
        "    if ignore_index is not None:\n",
        "        # create ignore_index mask for the result\n",
        "        mask = input.expand(shape) == ignore_index\n",
        "        # clone the src tensor and zero out ignore_index in the input\n",
        "        input = input.clone()\n",
        "        input[input == ignore_index] = 0\n",
        "        # scatter to get the one-hot tensor\n",
        "        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
        "        # bring back the ignore_index in the result\n",
        "        result[mask] = ignore_index\n",
        "        return result\n",
        "    else:\n",
        "        # scatter to get the one-hot tensor\n",
        "        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
        "\n",
        "\n",
        "def convert_to_numpy(*inputs):\n",
        "    \"\"\"\n",
        "    Coverts input tensors to numpy ndarrays\n",
        "\n",
        "    Args:\n",
        "        inputs (iteable of torch.Tensor): torch tensor\n",
        "\n",
        "    Returns:\n",
        "        tuple of ndarrays\n",
        "    \"\"\"\n",
        "\n",
        "    def _to_numpy(i):\n",
        "        assert isinstance(i, torch.Tensor), \"Expected input to be torch.Tensor\"\n",
        "        return i.detach().cpu().numpy()\n",
        "\n",
        "    return (_to_numpy(i) for i in inputs)\n",
        "\n",
        "\n",
        "def create_optimizer(optimizer_config, model):\n",
        "    optim_name = optimizer_config.get('name', 'Adam')\n",
        "    # common optimizer settings\n",
        "    learning_rate = optimizer_config.get('learning_rate', 1e-3)\n",
        "    weight_decay = optimizer_config.get('weight_decay', 0)\n",
        "\n",
        "    # grab optimizer specific settings and init\n",
        "    # optimizer\n",
        "    if optim_name == 'Adadelta':\n",
        "        rho = optimizer_config.get('rho', 0.9)\n",
        "        optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, rho=rho,\n",
        "                                   weight_decay=weight_decay)\n",
        "    elif optim_name == 'Adagrad':\n",
        "        lr_decay = optimizer_config.get('lr_decay', 0)\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=lr_decay,\n",
        "                                  weight_decay=weight_decay)\n",
        "    elif optim_name == 'AdamW':\n",
        "        betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, betas=betas,\n",
        "                                weight_decay=weight_decay)\n",
        "    elif optim_name == 'SparseAdam':\n",
        "        betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "        optimizer = optim.SparseAdam(model.parameters(), lr=learning_rate, betas=betas)\n",
        "    elif optim_name == 'Adamax':\n",
        "        betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "        optimizer = optim.Adamax(model.parameters(), lr=learning_rate, betas=betas,\n",
        "                                 weight_decay=weight_decay)\n",
        "    elif optim_name == 'ASGD':\n",
        "        lambd = optimizer_config.get('lambd', 0.0001)\n",
        "        alpha = optimizer_config.get('alpha', 0.75)\n",
        "        t0 = optimizer_config.get('t0', 1e6)\n",
        "        optimizer = optim.Adamax(model.parameters(), lr=learning_rate, lambd=lambd,\n",
        "                                 alpha=alpha, t0=t0, weight_decay=weight_decay)\n",
        "    elif optim_name == 'LBFGS':\n",
        "        max_iter = optimizer_config.get('max_iter', 20)\n",
        "        max_eval = optimizer_config.get('max_eval', None)\n",
        "        tolerance_grad = optimizer_config.get('tolerance_grad', 1e-7)\n",
        "        tolerance_change = optimizer_config.get('tolerance_change', 1e-9)\n",
        "        history_size = optimizer_config.get('history_size', 100)\n",
        "        optimizer = optim.LBFGS(model.parameters(), lr=learning_rate, max_iter=max_iter,\n",
        "                                max_eval=max_eval, tolerance_grad=tolerance_grad,\n",
        "                                tolerance_change=tolerance_change, history_size=history_size)\n",
        "    elif optim_name == 'NAdam':\n",
        "        betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "        momentum_decay = optimizer_config.get('momentum_decay', 4e-3)\n",
        "        optimizer = optim.NAdam(model.parameters(), lr=learning_rate, betas=betas,\n",
        "                                momentum_decay=momentum_decay,\n",
        "                                weight_decay=weight_decay)\n",
        "    elif optim_name == 'RAdam':\n",
        "        betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "        optimizer = optim.RAdam(model.parameters(), lr=learning_rate, betas=betas,\n",
        "                                weight_decay=weight_decay)\n",
        "    elif optim_name == 'RMSprop':\n",
        "        alpha = optimizer_config.get('alpha', 0.99)\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=alpha,\n",
        "                                  weight_decay=weight_decay)\n",
        "    elif optim_name == 'Rprop':\n",
        "        momentum = optimizer_config.get('momentum', 0)\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
        "    elif optim_name == 'SGD':\n",
        "        momentum = optimizer_config.get('momentum', 0)\n",
        "        dampening = optimizer_config.get('dampening', 0)\n",
        "        nesterov = optimizer_config.get('nesterov', False)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum,\n",
        "                              dampening=dampening, nesterov=nesterov,\n",
        "                              weight_decay=weight_decay)\n",
        "    else:  # Adam is default\n",
        "        betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas,\n",
        "                               weight_decay=weight_decay)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def create_lr_scheduler(lr_config, optimizer):\n",
        "    if lr_config is None:\n",
        "        return None\n",
        "    class_name = lr_config.pop('name')\n",
        "    m = importlib.import_module('torch.optim.lr_scheduler')\n",
        "    clazz = getattr(m, class_name)\n",
        "    # add optimizer to the config\n",
        "    lr_config['optimizer'] = optimizer\n",
        "    return clazz(**lr_config)\n",
        "\n",
        "\n",
        "def get_class(class_name, modules):\n",
        "    for module in modules:\n",
        "        m = importlib.import_module(module)\n",
        "        clazz = getattr(m, class_name, None)\n",
        "        if clazz is not None:\n",
        "            return clazz\n",
        "    raise RuntimeError(f'Unsupported dataset class: {class_name}')"
      ],
      "metadata": {
        "id": "aok-3Np7WR3_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# from pytorch3dunet.unet3d.buildingblocks import DoubleConv, ResNetBlock, ResNetBlockSE, \\\n",
        "#     create_decoders, create_encoders\n",
        "# from pytorch3dunet.unet3d.utils import get_class, number_of_features_per_level\n",
        "\n",
        "\n",
        "class AbstractUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for standard and residual UNet.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output segmentation masks;\n",
        "            Note that the of out_channels might correspond to either\n",
        "            different semantic classes or to different binary segmentation mask.\n",
        "            It's up to the user of the class to interpret the out_channels and\n",
        "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
        "            or BCEWithLogitsLoss (two-class) respectively)\n",
        "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
        "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
        "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the final 1x1 convolution,\n",
        "            otherwise apply nn.Softmax. In effect only if `self.training == False`, i.e. during validation/testing\n",
        "        basic_module: basic model for the encoder/decoder (DoubleConv, ResNetBlock, ....)\n",
        "        layer_order (string): determines the order of layers in `SingleConv` module.\n",
        "            E.g. 'crg' stands for GroupNorm3d+Conv3d+ReLU. See `SingleConv` for more info\n",
        "        num_groups (int): number of groups for the GroupNorm\n",
        "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
        "            default: 4\n",
        "        is_segmentation (bool): if True and the model is in eval mode, Sigmoid/Softmax normalization is applied\n",
        "            after the final convolution; if False (regression problem) the normalization layer is skipped\n",
        "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
        "        pool_kernel_size (int or tuple): the size of the window\n",
        "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
        "        conv_upscale (int): number of the convolution to upscale in encoder if DoubleConv, default: 2\n",
        "        upsample (str): algorithm used for decoder upsampling:\n",
        "            InterpolateUpsampling:   'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'\n",
        "            TransposeConvUpsampling: 'deconv'\n",
        "            No upsampling:           None\n",
        "            Default: 'default' (chooses automatically)\n",
        "        dropout_prob (float or tuple): dropout probability, default: 0.1\n",
        "        is3d (bool): if True the model is 3D, otherwise 2D, default: True\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_kernel_size=3, pool_kernel_size=2,\n",
        "                 conv_padding=1, conv_upscale=2, upsample='default', dropout_prob=0.1, is3d=True):\n",
        "        super(AbstractUNet, self).__init__()\n",
        "\n",
        "        if isinstance(f_maps, int):\n",
        "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
        "\n",
        "        assert isinstance(f_maps, list) or isinstance(f_maps, tuple)\n",
        "        assert len(f_maps) > 1, \"Required at least 2 levels in the U-Net\"\n",
        "        if 'g' in layer_order:\n",
        "            assert num_groups is not None, \"num_groups must be specified if GroupNorm is used\"\n",
        "\n",
        "        # create encoder path\n",
        "        self.encoders = create_encoders(in_channels, f_maps, basic_module, conv_kernel_size,\n",
        "                                        conv_padding, conv_upscale, dropout_prob,\n",
        "                                        layer_order, num_groups, pool_kernel_size, is3d)\n",
        "\n",
        "        # create decoder path\n",
        "        self.decoders = create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding,\n",
        "                                        layer_order, num_groups, upsample, dropout_prob,\n",
        "                                        is3d)\n",
        "\n",
        "        # in the last layer a 1×1 convolution reduces the number of output channels to the number of labels\n",
        "        if is3d:\n",
        "            self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
        "        else:\n",
        "            self.final_conv = nn.Conv2d(f_maps[0], out_channels, 1)\n",
        "\n",
        "        if is_segmentation:\n",
        "            # semantic segmentation problem\n",
        "            if final_sigmoid:\n",
        "                self.final_activation = nn.Sigmoid()\n",
        "            else:\n",
        "                self.final_activation = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            # regression problem\n",
        "            self.final_activation = None\n",
        "\n",
        "    def forward(self, x, return_logits=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (N, C, D, H, W) for 3D or (N, C, H, W) for 2D,\n",
        "                              where N is the batch size, C is the number of channels,\n",
        "                              D is the depth, H is the height, and W is the width.\n",
        "            return_logits (bool): If True, returns both the output and the logits.\n",
        "                                  If False, returns only the output. Default is False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after passing through the network.\n",
        "                          If return_logits is True, returns a tuple of (output, logits).\n",
        "        \"\"\"\n",
        "        output, logits = self._forward_logits(x)\n",
        "        if return_logits:\n",
        "            return output, logits\n",
        "        return output\n",
        "\n",
        "    def _forward_logits(self, x):\n",
        "        # encoder part\n",
        "        encoders_features = []\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "            # reverse the encoder outputs to be aligned with the decoder\n",
        "            encoders_features.insert(0, x)\n",
        "\n",
        "        # remove the last encoder's output from the list\n",
        "        # !!remember: it's the 1st in the list\n",
        "        encoders_features = encoders_features[1:]\n",
        "\n",
        "        # decoder part\n",
        "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
        "            # pass the output from the corresponding encoder and the output\n",
        "            # of the previous decoder\n",
        "            x = decoder(encoder_features, x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        if self.final_activation is not None:\n",
        "            # compute final activation\n",
        "            out = self.final_activation(x)\n",
        "            # return both probabilities and logits\n",
        "            return out, x\n",
        "\n",
        "        return x, x\n",
        "\n",
        "\n",
        "class UNet3D(AbstractUNet):\n",
        "    \"\"\"\n",
        "    3DUnet model from\n",
        "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
        "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
        "\n",
        "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1,\n",
        "                 conv_upscale=2, upsample='default', dropout_prob=0.1, **kwargs):\n",
        "        super(UNet3D, self).__init__(in_channels=in_channels,\n",
        "                                     out_channels=out_channels,\n",
        "                                     final_sigmoid=final_sigmoid,\n",
        "                                     basic_module=DoubleConv,\n",
        "                                     f_maps=f_maps,\n",
        "                                     layer_order=layer_order,\n",
        "                                     num_groups=num_groups,\n",
        "                                     num_levels=num_levels,\n",
        "                                     is_segmentation=is_segmentation,\n",
        "                                     conv_padding=conv_padding,\n",
        "                                     conv_upscale=conv_upscale,\n",
        "                                     upsample=upsample,\n",
        "                                     dropout_prob=dropout_prob,\n",
        "                                     is3d=True)\n",
        "\n",
        "\n",
        "class ResidualUNet3D(AbstractUNet):\n",
        "    \"\"\"\n",
        "    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n",
        "    Uses ResNetBlock as a basic building block, summation joining instead\n",
        "    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).\n",
        "    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1,\n",
        "                 conv_upscale=2, upsample='default', dropout_prob=0.1, **kwargs):\n",
        "        super(ResidualUNet3D, self).__init__(in_channels=in_channels,\n",
        "                                             out_channels=out_channels,\n",
        "                                             final_sigmoid=final_sigmoid,\n",
        "                                             basic_module=ResNetBlock,\n",
        "                                             f_maps=f_maps,\n",
        "                                             layer_order=layer_order,\n",
        "                                             num_groups=num_groups,\n",
        "                                             num_levels=num_levels,\n",
        "                                             is_segmentation=is_segmentation,\n",
        "                                             conv_padding=conv_padding,\n",
        "                                             conv_upscale=conv_upscale,\n",
        "                                             upsample=upsample,\n",
        "                                             dropout_prob=dropout_prob,\n",
        "                                             is3d=True)\n",
        "\n",
        "\n",
        "class ResidualUNetSE3D(AbstractUNet):\n",
        "    \"\"\"_summary_\n",
        "    Residual 3DUnet model implementation with squeeze and excitation based on\n",
        "    https://arxiv.org/pdf/1706.00120.pdf.\n",
        "    Uses ResNetBlockSE as a basic building block, summation joining instead\n",
        "    of concatenation joining and transposed convolutions for upsampling (watch\n",
        "    out for block artifacts). Since the model effectively becomes a residual\n",
        "    net, in theory it allows for deeper UNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1,\n",
        "                 conv_upscale=2, upsample='default', dropout_prob=0.1, **kwargs):\n",
        "        super(ResidualUNetSE3D, self).__init__(in_channels=in_channels,\n",
        "                                               out_channels=out_channels,\n",
        "                                               final_sigmoid=final_sigmoid,\n",
        "                                               basic_module=ResNetBlockSE,\n",
        "                                               f_maps=f_maps,\n",
        "                                               layer_order=layer_order,\n",
        "                                               num_groups=num_groups,\n",
        "                                               num_levels=num_levels,\n",
        "                                               is_segmentation=is_segmentation,\n",
        "                                               conv_padding=conv_padding,\n",
        "                                               conv_upscale=conv_upscale,\n",
        "                                               upsample=upsample,\n",
        "                                               dropout_prob=dropout_prob,\n",
        "                                               is3d=True)\n",
        "\n",
        "\n",
        "class UNet2D(AbstractUNet):\n",
        "    \"\"\"\n",
        "    2DUnet model from\n",
        "    `\"U-Net: Convolutional Networks for Biomedical Image Segmentation\" <https://arxiv.org/abs/1505.04597>`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1,\n",
        "                 conv_upscale=2, upsample='default', dropout_prob=0.1, **kwargs):\n",
        "        super(UNet2D, self).__init__(in_channels=in_channels,\n",
        "                                     out_channels=out_channels,\n",
        "                                     final_sigmoid=final_sigmoid,\n",
        "                                     basic_module=DoubleConv,\n",
        "                                     f_maps=f_maps,\n",
        "                                     layer_order=layer_order,\n",
        "                                     num_groups=num_groups,\n",
        "                                     num_levels=num_levels,\n",
        "                                     is_segmentation=is_segmentation,\n",
        "                                     conv_padding=conv_padding,\n",
        "                                     conv_upscale=conv_upscale,\n",
        "                                     upsample=upsample,\n",
        "                                     dropout_prob=dropout_prob,\n",
        "                                     is3d=False)\n",
        "\n",
        "\n",
        "class ResidualUNet2D(AbstractUNet):\n",
        "    \"\"\"\n",
        "    Residual 2DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
        "                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1,\n",
        "                 conv_upscale=2, upsample='default', dropout_prob=0.1, **kwargs):\n",
        "        super(ResidualUNet2D, self).__init__(in_channels=in_channels,\n",
        "                                             out_channels=out_channels,\n",
        "                                             final_sigmoid=final_sigmoid,\n",
        "                                             basic_module=ResNetBlock,\n",
        "                                             f_maps=f_maps,\n",
        "                                             layer_order=layer_order,\n",
        "                                             num_groups=num_groups,\n",
        "                                             num_levels=num_levels,\n",
        "                                             is_segmentation=is_segmentation,\n",
        "                                             conv_padding=conv_padding,\n",
        "                                             conv_upscale=conv_upscale,\n",
        "                                             upsample=upsample,\n",
        "                                             dropout_prob=dropout_prob,\n",
        "                                             is3d=False)\n",
        "\n",
        "\n",
        "def get_model(model_config):\n",
        "    model_class = get_class(model_config['name'], modules=[\n",
        "        'pytorch3dunet.unet3d.model'\n",
        "    ])\n",
        "    return model_class(**model_config)\n",
        "\n",
        "\n",
        "def is_model_2d(model):\n",
        "    if isinstance(model, nn.DataParallel):\n",
        "        model = model.module\n",
        "    return isinstance(model, UNet2D)"
      ],
      "metadata": {
        "id": "TMrtB-Ag-mWD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet3D(\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    is_segmentation=False,\n",
        "    final_sigmoid=False\n",
        ")"
      ],
      "metadata": {
        "id": "o1miFm2NWeBd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a batch of 2 RGB video clips, each with 8 frames of size 64x64\n",
        "x = torch.randn(2, 3, 8, 64, 64)  # (batch, channels, time/depth, height, width)\n",
        "\n",
        "# Run forward pass\n",
        "output = model(x)\n",
        "\n",
        "# Check output\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0UQtZvFXCXf",
        "outputId": "bd6fee15-62d3-45c7-a940-adf4815b5f3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([2, 3, 8, 64, 64])\n",
            "Output shape: torch.Size([2, 3, 8, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRzkaLf8XKf5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}